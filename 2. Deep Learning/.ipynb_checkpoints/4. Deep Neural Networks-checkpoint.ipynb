{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Linear Models are Limited](#Linear Models are Limited)\n",
    "2. [2-Layer Neural Network](#2-Layer Neural Network)\n",
    "    1. [Network of ReLUs](#Network of ReLUs)\n",
    "    2. [TensorFlow ReLUs](#TensorFlow ReLUs)\n",
    "3. [Chain Rule and Backpropagation](#Chain Rule and Backpropagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Linear Models are Limited <a name='Linear Models are Limited'></a>\n",
    "\n",
    "We've trained so far is simple model, and it's also relatively limited. \n",
    "\n",
    "__Quiz:__\n",
    "\n",
    "How many train parameters did it actually have, which input was a 28 by 28 image, and the output was 10 classes. \n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 18.00.25.png' width=400>\n",
    "\n",
    "__Answer:__\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Total number of parameters} &= \\text{size of } \\mathbf{W} + \\text{size of } \\mathbf{b} \\\\\n",
    "&= 28 \\times 28 \\times 10 + 10 \\\\\n",
    "&= 7850\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If we have $N$ inputs, and $K$ outputs, we have $(N+1)K$ parameters to use. We might want to use many, many more parameters in practice, but it's linear. This means that the kind of interactions that we're capable of representing with that model is somewhat limited. For example, if two inputs interacti in an additive way, $y = x_1 + x_2$, the model can represent them well as a matrix multiply. But if two inputs interact in the way that the outcome depends on the product of the two, $y=x_1 \\times x_2$, we won't be able to model that efficiently with a linear model. \n",
    "\n",
    "However, linear operations are really nice. Big matrix multiplies are exactly what GPUs wer designed for, and numerically linear operations are very stable. \n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 18.26.49.png' width=300>\n",
    "\n",
    "We can show mathematically that small changed in the input can never yield big changes in the output. In addition, the derivates are very nice too. The derivative of a linear function is constant. \n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 18.27.18.png' width=300>\n",
    "\n",
    "This means that we can't get more stable numerically than a constant. So, we would like to keep the parameters inside big linear functions, but we would also want the entire model to be nonlinear.\n",
    "\n",
    "We can't just keep multiplying our inputs by linear functions, because that's just equivalent to one big linear function as below.\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 18.52.19.png' width=300>\n",
    "\n",
    "So, we're going to have to introduce non-linearities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 2-Layer Neural Network <a name='2-Layer Neural Network'></a>\n",
    "\n",
    "### 2.1. Network of ReLUs <a name='Network of ReLUs'></a>\n",
    "\n",
    "Rectified Linear Units (ReLUs) are literally the simplest non-linear functions. They're linear if $x$ is greater than $0$, and they're the $0$ everywhere else.\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 18.35.17.png' width = 400>\n",
    "\n",
    "ReLUs have nice derivatives, as well. \n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 18.37.28.png' width = 200>\n",
    "\n",
    "When $x$ is less than zero, the value is 0. So, the derivative is 0 as well. When $x$ is greater than 0, the value is equal to x. So, the derivative is equal to 1. \n",
    "\n",
    "A logistic classifier can be non-linear. Instead of having a single matrix\n",
    "multiplier as our classifier, we're going to insert a ReLUs right in the middle.\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 18.41.05.png' width=500>\n",
    "\n",
    "We now have two matrices. One going from the inputs to the ReLUs, and another one connecting the ReLUs to the classifier. \n",
    "\n",
    "We've solved two of our problems. Our function in now nonlinear thanks\n",
    "to the ReLUs in the middle, and we now have a new knob that we can tune,\n",
    "the number $H$ which corresponds to the number of ReLUs units that we have in the classifier. We can make it as big as we want. \n",
    "\n",
    "__Note__: Depicted above is a \"2-layer\" neural network:\n",
    "\n",
    "<img src='Figures4/RELU.png' width=500>\n",
    "\n",
    "    1. The first layer effectively consists of the set of weights and biases applied to X and passed through ReLUs. The output of this layer is fed to the next one, but is not observable outside the network, hence it is known as a hidden layer.\n",
    "    \n",
    "    2. The second layer consists of the weights and biases applied to these intermediate outputs, followed by the softmax function to generate probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2. TensorFlow ReLUs <a name='TensorFlow ReLUs'></a>\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 18.35.17.png' width = 400>\n",
    "\n",
    "A Rectified linear unit (ReLU) is type of [activation function](https://en.wikipedia.org/wiki/Activation_function) that is defined as $f(x) = max(0, x)$. The function returns 0 if $x$ is negative, otherwise it returns $x$. TensorFlow provides the ReLU function as __tf.nn.relu()__, as shown below.\n",
    "\n",
    "```Python\n",
    "# Hidden Layer with ReLU activation function\n",
    "hidden_layer = tf.add(tf.matmul(features, hidden_weights), hidden_biases)\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "output = tf.add(tf.matmul(hidden_layer, output_weights), output_biases)\n",
    "```\n",
    "\n",
    "<img src='Figures4/insert-relu.png' width=500>\n",
    "\n",
    "The above code applies the __tf.nn.relu()__ function to the __hidden_layer__, effectively turning off any negative weights and acting like an on/off switch. Adding additional layers, like the output layer, after an activation function turns the model into a nonlinear function. This nonlinearity allows the network to solve more complex problems.\n",
    "\n",
    "__Quiz:__\n",
    "\n",
    "Use TensorFlow's ReLU function to turn the linear model below into a nonlinear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.11000013   8.44000053]\n",
      " [  0.           0.        ]\n",
      " [ 24.01000214  38.23999786]]\n"
     ]
    }
   ],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "output = None\n",
    "hidden_layer_weights = [\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.4, 0.6, 0.6],\n",
    "    [0.5, 0.9, 0.1],\n",
    "    [0.8, 0.2, 0.8]]\n",
    "out_weights = [\n",
    "    [0.1, 0.6],\n",
    "    [0.2, 0.1],\n",
    "    [0.7, 0.9]]\n",
    "\n",
    "# Weights and biases\n",
    "weights = [\n",
    "    tf.Variable(hidden_layer_weights),\n",
    "    tf.Variable(out_weights)]\n",
    "biases = [\n",
    "    tf.Variable(tf.zeros(3)),\n",
    "    tf.Variable(tf.zeros(2))]\n",
    "\n",
    "# Input\n",
    "features = tf.Variable([[1.0, 2.0, 3.0, 4.0], [-1.0, -2.0, -3.0, -4.0], [11.0, 12.0, 13.0, 14.0]])\n",
    "\n",
    "# TODO: Create Model\n",
    "linear1 = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "ReLU = tf.nn.relu(linear1)\n",
    "linear2 = tf.add(tf.matmul(ReLU, weights[1]), biases[1])\n",
    "\n",
    "# TODO: Print session results\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output = sess.run(linear2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__\n",
    "\n",
    "```Python\n",
    "# Quiz Solution\n",
    "# Note: You can't run code in this tab\n",
    "import tensorflow as tf\n",
    "\n",
    "output = None\n",
    "hidden_layer_weights = [\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.4, 0.6, 0.6],\n",
    "    [0.5, 0.9, 0.1],\n",
    "    [0.8, 0.2, 0.8]]\n",
    "out_weights = [\n",
    "    [0.1, 0.6],\n",
    "    [0.2, 0.1],\n",
    "    [0.7, 0.9]]\n",
    "\n",
    "# Weights and biases\n",
    "weights = [\n",
    "    tf.Variable(hidden_layer_weights),\n",
    "    tf.Variable(out_weights)]\n",
    "biases = [\n",
    "    tf.Variable(tf.zeros(3)),\n",
    "    tf.Variable(tf.zeros(2))]\n",
    "\n",
    "# Input\n",
    "features = tf.Variable([[1.0, 2.0, 3.0, 4.0], [-1.0, -2.0, -3.0, -4.0], [11.0, 12.0, 13.0, 14.0]])\n",
    "\n",
    "# TODO: Create Model\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "\n",
    "# TODO: Print session results\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(logits))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Chain Rule and Backpropagation <a name='Chain Rule and Backpropagation'><a/>\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 19.50.36.png' width = 400>\n",
    "\n",
    "One reason to build this network by stacking simple operations, like multiplications, and sums, and ReLUs, on top of each other is\n",
    "that it makes the math very simple.\n",
    "\n",
    "The key mathematical insight is the chain rule. \n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 19.52.42.png' width = 300>\n",
    "\n",
    "If we have two functions that get composed, then the chain rule tells the derivatives of that function simply by taking the product of the derivatives of the components. That's very powerful.\n",
    "\n",
    "As long as you know how to write the derivatives of your individual functions, there is a simple graphical way to combine them together and compute the derivative for the whole function.\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 19.54.26.png' width = 500>\n",
    "\n",
    "There is a way to write this chain rule that is very efficient computationally, with lots of data reuse, and that looks like a very simple data pipeline.\n",
    "\n",
    "Imagine the network is a stack of simple operations. Some have parameters like the matrix transforms, some don't like the ReLUs in those blocks.\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 19.59.04.png' width = 400>\n",
    "\n",
    "When we apply data to some input x, we have data flowing through the stack up to your predictions $y$.\n",
    "\n",
    "To compute the derivatives, we create another graph.\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 20.01.01.png' width = 400>\n",
    "\n",
    "The data in the new graph flows backwards through the network, and get's combined using the chain rule that we saw before and produces gradients. That graph can be derived completely automatically from the individual operations in the network. This is called back-propagation, and it's a very powerful concept.\n",
    "\n",
    "It makes computing derivatives of complex function very efficient as long as the function is made up of simple blocks with simple derivatives.\n",
    "\n",
    "16\n",
    "00:00:56,250 --> 00:01:00,910\n",
    "Running the model up to the predictions\n",
    "is often call the forward prop, and\n",
    "\n",
    "17\n",
    "00:01:00,910 --> 00:01:04,190\n",
    "the model that goes backwards\n",
    "is called the back prop.\n",
    "\n",
    "18\n",
    "00:01:04,190 --> 00:01:07,290\n",
    "So, to recap,\n",
    "to run stochastic gradient descent,\n",
    "\n",
    "19\n",
    "00:01:07,290 --> 00:01:11,220\n",
    "for every single little batch of\n",
    "your data in your training set,\n",
    "\n",
    "20\n",
    "00:01:11,220 --> 00:01:14,900\n",
    "you're going to run the forward prop,\n",
    "and then the back prop.\n",
    "\n",
    "21\n",
    "00:01:14,900 --> 00:01:19,270\n",
    "And that will give you gradients for\n",
    "each of your weights in your model.\n",
    "\n",
    "22\n",
    "00:01:19,270 --> 00:01:21,950\n",
    "Then you're going to apply those\n",
    "gradients with the learning weights\n",
    "\n",
    "23\n",
    "00:01:21,950 --> 00:01:24,710\n",
    "to your original weights,\n",
    "and update them.\n",
    "\n",
    "24\n",
    "00:01:24,710 --> 00:01:28,200\n",
    "And you're going to repeat that\n",
    "all over again, many, many times.\n",
    "\n",
    "25\n",
    "00:01:28,200 --> 00:01:31,610\n",
    "This is how your entire\n",
    "model gets optimized.\n",
    "\n",
    "26\n",
    "00:01:31,610 --> 00:01:34,610\n",
    "I am not going to go through more\n",
    "of the maths of what's going on in\n",
    "\n",
    "27\n",
    "00:01:34,610 --> 00:01:35,910\n",
    "each of those blocks.\n",
    "\n",
    "28\n",
    "00:01:35,910 --> 00:01:38,780\n",
    "Because, again, you don't typically\n",
    "have to worry about that, and\n",
    "\n",
    "29\n",
    "00:01:38,780 --> 00:01:42,270\n",
    "it's essentially the chain rule,\n",
    "but keep in mind, this diagram.\n",
    "\n",
    "30\n",
    "00:01:42,270 --> 00:01:45,730\n",
    "In particular each block of\n",
    "the back prop often takes about\n",
    "\n",
    "31\n",
    "00:01:45,730 --> 00:01:50,395\n",
    "twice the memory that's needed for\n",
    "prop and twice the compute.\n",
    "\n",
    "32\n",
    "00:01:50,395 --> 00:01:52,520\n",
    "That's important when you\n",
    "want to size your model and\n",
    "\n",
    "33\n",
    "00:01:52,520 --> 00:01:53,940\n",
    "fit it in memory for example.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
