{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1. [Linear Models are Limited](#Linear Models are Limited)\n",
    "2. [2-Layer Neural Network](#2-Layer Neural Network)\n",
    "    1. [Network of ReLUs](#Network of ReLUs)\n",
    "    2. [TensorFlow ReLUs](#TensorFlow ReLUs)\n",
    "    3. [Chain Rule and Backpropagation](#Chain Rule and Backpropagation)\n",
    "3. [Deep Neural Network in TensorFlow](#Deep Neural Network in TensorFlow)\n",
    "    1. [TensorFlow MNIST](#TensorFlow MNIST)\n",
    "    2. [Learning Parameters](#Learning Parameters)\n",
    "    3. [Hidden Layer Parameters](#Hidden Layer Parameters)\n",
    "    4. [Weights and Biases](#Weights and Biases)\n",
    "    5. [Input](#Input)\n",
    "    6. [Multilayer Perceptron](#Multilayer Perceptron)\n",
    "    7. [Optimizer](#Optimizer)\n",
    "4. [Training a Deep Learning Network](#Training a Deep Learning Network)\n",
    "5. [Save and Restore TensorFlow Models](#Save and Restore TensorFlow Models)\n",
    "    1. [Saving Variables](#Saving Variables)\n",
    "    2. [Save a Trained Model](#Save a Trained Model)\n",
    "    3. [Load a Trained Model](#Load a Trained Model)\n",
    "6. [Loading the Weights and Biases into a New Model](#Loading the Weights and Biases into a New Model)\n",
    "    1. [Naming Error](#Naming Error)\n",
    "    2. [Regularization](#Regularization)\n",
    "7. [Regularization](#Regularization)\n",
    "    1. [Investigation of Performance Under Validation Set](#Investigation of Performance Under Validation Set)\n",
    "    2. [L2 Regularization](#L2 Regularization)\n",
    "8. [Dropout](#Dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. Linear Models are Limited <a name='Linear Models are Limited'></a>\n",
    "\n",
    "We've trained so far is simple model, and it's also relatively limited. \n",
    "\n",
    "__Quiz:__\n",
    "\n",
    "How many train parameters did it actually have, which input was a 28 by 28 image, and the output was 10 classes. \n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 18.00.25.png' width=400>\n",
    "\n",
    "__Answer:__\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Total number of parameters} &= \\text{size of } \\mathbf{W} + \\text{size of } \\mathbf{b} \\\\\n",
    "&= 28 \\times 28 \\times 10 + 10 \\\\\n",
    "&= 7850\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If we have $N$ inputs, and $K$ outputs, we have $(N+1)K$ parameters to use. We might want to use many, many more parameters in practice, but it's linear. This means that the kind of interactions that we're capable of representing with that model is somewhat limited. For example, if two inputs interacti in an additive way, $y = x_1 + x_2$, the model can represent them well as a matrix multiply. But if two inputs interact in the way that the outcome depends on the product of the two, $y=x_1 \\times x_2$, we won't be able to model that efficiently with a linear model. \n",
    "\n",
    "However, linear operations are really nice. Big matrix multiplies are exactly what GPUs wer designed for, and numerically linear operations are very stable. \n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 18.26.49.png' width=300>\n",
    "\n",
    "We can show mathematically that small changed in the input can never yield big changes in the output. In addition, the derivates are very nice too. The derivative of a linear function is constant. \n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 18.27.18.png' width=300>\n",
    "\n",
    "This means that we can't get more stable numerically than a constant. So, we would like to keep the parameters inside big linear functions, but we would also want the entire model to be nonlinear.\n",
    "\n",
    "We can't just keep multiplying our inputs by linear functions, because that's just equivalent to one big linear function as below.\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 18.52.19.png' width=300>\n",
    "\n",
    "So, we're going to have to introduce non-linearities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. 2-Layer Neural Network <a name='2-Layer Neural Network'></a>\n",
    "\n",
    "### 2.1. Network of ReLUs <a name='Network of ReLUs'></a>\n",
    "\n",
    "Rectified Linear Units (ReLUs) are literally the simplest non-linear functions. They're linear if $x$ is greater than $0$, and they're the $0$ everywhere else.\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 18.35.17.png' width = 400>\n",
    "\n",
    "ReLUs have nice derivatives, as well. \n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 18.37.28.png' width = 200>\n",
    "\n",
    "When $x$ is less than zero, the value is 0. So, the derivative is 0 as well. When $x$ is greater than 0, the value is equal to x. So, the derivative is equal to 1. \n",
    "\n",
    "A logistic classifier can be non-linear. Instead of having a single matrix\n",
    "multiplier as our classifier, we're going to insert a ReLUs right in the middle.\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 18.41.05.png' width=500>\n",
    "\n",
    "We now have two matrices. One going from the inputs to the ReLUs, and another one connecting the ReLUs to the classifier. \n",
    "\n",
    "We've solved two of our problems. Our function in now nonlinear thanks\n",
    "to the ReLUs in the middle, and we now have a new knob that we can tune,\n",
    "the number $H$ which corresponds to the number of ReLUs units that we have in the classifier. We can make it as big as we want. \n",
    "\n",
    "__Note__: Depicted above is a \"2-layer\" neural network:\n",
    "\n",
    "<img src='Figures4/RELU.png' width=500>\n",
    "\n",
    "    1. The first layer effectively consists of the set of weights and biases applied to X and passed through ReLUs. The output of this layer is fed to the next one, but is not observable outside the network, hence it is known as a hidden layer.\n",
    "    \n",
    "    2. The second layer consists of the weights and biases applied to these intermediate outputs, followed by the softmax function to generate probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2.2. TensorFlow ReLUs <a name='TensorFlow ReLUs'></a>\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 18.35.17.png' width = 400>\n",
    "\n",
    "A Rectified linear unit (ReLU) is type of [activation function](https://en.wikipedia.org/wiki/Activation_function) that is defined as $f(x) = max(0, x)$. The function returns 0 if $x$ is negative, otherwise it returns $x$. TensorFlow provides the ReLU function as __tf.nn.relu()__, as shown below.\n",
    "\n",
    "```Python\n",
    "# Hidden Layer with ReLU activation function\n",
    "hidden_layer = tf.add(tf.matmul(features, hidden_weights), hidden_biases)\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "output = tf.add(tf.matmul(hidden_layer, output_weights), output_biases)\n",
    "```\n",
    "\n",
    "<img src='Figures4/insert-relu.png' width=500>\n",
    "\n",
    "The above code applies the __tf.nn.relu()__ function to the __hidden_layer__, effectively turning off any negative weights and acting like an on/off switch. Adding additional layers, like the output layer, after an activation function turns the model into a nonlinear function. This nonlinearity allows the network to solve more complex problems.\n",
    "\n",
    "__Quiz:__\n",
    "\n",
    "Use TensorFlow's ReLU function to turn the linear model below into a nonlinear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.11000013   8.44000053]\n",
      " [  0.           0.        ]\n",
      " [ 24.01000214  38.23999786]]\n"
     ]
    }
   ],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "output = None\n",
    "hidden_layer_weights = [\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.4, 0.6, 0.6],\n",
    "    [0.5, 0.9, 0.1],\n",
    "    [0.8, 0.2, 0.8]]\n",
    "out_weights = [\n",
    "    [0.1, 0.6],\n",
    "    [0.2, 0.1],\n",
    "    [0.7, 0.9]]\n",
    "\n",
    "# Weights and biases\n",
    "weights = [\n",
    "    tf.Variable(hidden_layer_weights),\n",
    "    tf.Variable(out_weights)]\n",
    "biases = [\n",
    "    tf.Variable(tf.zeros(3)),\n",
    "    tf.Variable(tf.zeros(2))]\n",
    "\n",
    "# Input\n",
    "features = tf.Variable([[1.0, 2.0, 3.0, 4.0], [-1.0, -2.0, -3.0, -4.0], [11.0, 12.0, 13.0, 14.0]])\n",
    "\n",
    "# TODO: Create Model\n",
    "linear1 = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "ReLU = tf.nn.relu(linear1)\n",
    "linear2 = tf.add(tf.matmul(ReLU, weights[1]), biases[1])\n",
    "\n",
    "# TODO: Print session results\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    output = sess.run(linear2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Answer:__\n",
    "\n",
    "```Python\n",
    "# Quiz Solution\n",
    "# Note: You can't run code in this tab\n",
    "import tensorflow as tf\n",
    "\n",
    "output = None\n",
    "hidden_layer_weights = [\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.4, 0.6, 0.6],\n",
    "    [0.5, 0.9, 0.1],\n",
    "    [0.8, 0.2, 0.8]]\n",
    "out_weights = [\n",
    "    [0.1, 0.6],\n",
    "    [0.2, 0.1],\n",
    "    [0.7, 0.9]]\n",
    "\n",
    "# Weights and biases\n",
    "weights = [\n",
    "    tf.Variable(hidden_layer_weights),\n",
    "    tf.Variable(out_weights)]\n",
    "biases = [\n",
    "    tf.Variable(tf.zeros(3)),\n",
    "    tf.Variable(tf.zeros(2))]\n",
    "\n",
    "# Input\n",
    "features = tf.Variable([[1.0, 2.0, 3.0, 4.0], [-1.0, -2.0, -3.0, -4.0], [11.0, 12.0, 13.0, 14.0]])\n",
    "\n",
    "# TODO: Create Model\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "\n",
    "# TODO: Print session results\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(logits))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2.3. Chain Rule and Backpropagation <a name='Chain Rule and Backpropagation'><a/>\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 19.50.36.png' width = 400>\n",
    "\n",
    "One reason to build this network by stacking simple operations, like multiplications, and sums, and ReLUs, on top of each other is\n",
    "that it makes the math very simple.\n",
    "\n",
    "The key mathematical insight is the chain rule. \n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 19.52.42.png' width = 300>\n",
    "\n",
    "If we have two functions that get composed, then the chain rule tells the derivatives of that function simply by taking the product of the derivatives of the components. That's very powerful.\n",
    "\n",
    "As long as you know how to write the derivatives of your individual functions, there is a simple graphical way to combine them together and compute the derivative for the whole function.\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 19.54.26.png' width = 500>\n",
    "\n",
    "There is a way to write this chain rule that is very efficient computationally, with lots of data reuse, and that looks like a very simple data pipeline. Imagine the network is a stack of simple operations. Some have parameters like the matrix transforms, some don't, like the ReLUs.\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 19.59.04.png' width = 400>\n",
    "\n",
    "When we apply data to some input x, we have data flowing through the stack up to the predictions $y$. To compute the derivatives, we create another graph.\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-26 at 20.01.01.png' width = 400>\n",
    "\n",
    "The data in the new graph flows backwards through the network, and get's combined using the chain rule that we saw before and produces gradients. That graph can be derived completely automatically from the individual operations in the network. This is called back-propagation, and it's a very powerful concept. It makes computing derivatives of complex function very efficient as long as the function is made up of simple blocks with simple derivatives.\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-27 at 11.37.11.png' width = 500>\n",
    "\n",
    "Running the model up to the predictions is often call the forward prop, and the model that goes backwards\n",
    "is called the back prop. To run stochastic gradient descent, for every single little batch of data in training set, we're going to run the forward prop, and then the back prop. And that will give gradients for each of weights in model. Then we're going to apply those gradients with the learning weights to the original weights, and update them. And we're going to repeat that all over again, many, many times. This is how entire model gets optimized.\n",
    "\n",
    "Keep in mind, this diagram. In particular, each block of the back prop often takes about twice the memory that's needed for prop and twice the compute. That's important when we want to size the model and fit it in memory for example.\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-27 at 11.42.46.png' width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3. Deep Neural Network in TensorFlow <a name='Deep Neural Network in TensorFlow'></a>\n",
    "\n",
    "We've seen how to build a logistic classifier using TensorFlow. Now we're going to see how to use the logistic classifier to build a deep neural network. In the following walkthrough, we'll step through TensorFlow code written to classify the letters in the MNIST database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.1. TensorFlow MNIST <a name='TensorFlow MNIST'></a>\n",
    "\n",
    "The MNIST dataset is provided by TensorFlow, which batches and One-Hot encodes the data for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"datasets/mnist/\", one_hot=True, reshape=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.2. Learning Parameters <a name='Learning Parameters'></a>\n",
    "\n",
    "The focus here is on the architecture of multilayer neural networks, not parameter tuning, so here we'll just give the learning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "batch_size = 128  # Decrease batch size if you don't have enough memory\n",
    "display_step = 1\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.3. Hidden Layer Parameters <a name='Hidden Layer Parameters'></a>\n",
    "\n",
    "The variable **n\\_hidden\\_layer** determines the size of the hidden layer in the neural network. This is also known as the width of a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_hidden_layer = 256 # layer number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.4. Weights and Biases <a name='Weights and Biases'></a>\n",
    "\n",
    "Deep neural networks use multiple layers with each layer requiring it's own weight and bias. The **'hidden\\_layer'** weight and bias is for the hidden layer. The __'out'__ weight and bias is for the output layer. If the neural network were deeper, there would be weights and biases for each additional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.5. Input <a name='Input'></a>\n",
    "\n",
    "The MNIST data is made up of 28px by 28px images with a single [channel](https://en.wikipedia.org/wiki/Channel_(digital_image%29). The __tf.reshape()__ function above reshapes the 28px by 28px matrices in _x_ into row vectors of 784px."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "x_flat = tf.reshape(x, [-1, n_input])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.6. Multilayer Perceptron <a name='Multilayer Perceptron'></a>\n",
    "\n",
    "<img src='Figures4/multi-layer.png' width=500>\n",
    "\n",
    "We've seen the linear function __tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer'])__ before, also known as __xw + b__. Combining linear functions together using a ReLU will give you a two layer network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Hidden layer with RELU activation\n",
    "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']),\\\n",
    "    biases['hidden_layer'])\n",
    "layer_1 = tf.nn.relu(layer_1)\n",
    "# Output layer with linear activation\n",
    "logits = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.7. Optimizer <a name='Optimizer'></a>\n",
    "\n",
    "This is the same optimization technique used in the Intro to TensorFLow lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(\\\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n",
    "    .minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.8. Session\n",
    "\n",
    "The MNIST library in TensorFlow provides the ability to receive the dataset in batches. Calling the __mnist.train.next_batch()__ function returns a subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 4. Training a Deep Learning Network <a name='Training a Deep Learning Network'></a>\n",
    "\n",
    "So we you have a small neural network. It's not particularly deep,\n",
    "just two layers. We can make it bigger, more complex, by increasing the size of that hidden layer in the middle. But it turns out that increasing this $H$ is not particularly efficient in general. We need to make it very, very big, and then it gets really hard to train.\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-27 at 13.25.24.png' width=500>\n",
    "\n",
    "This is where the central idea of deep learning comes into play. Instead, we can also add more layers and make the model deeper. First, it is __parameter efficiency__. We can typically get much more performance with fewer parameters by going deeper rather than wider. Secondly, it is that a lot of natural phenomena tend to have a hierarchical structure which deep models naturally capture.\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-27 at 13.26.11.png' width=500>\n",
    "\n",
    "If we poke at a model for images, for example, and visualize what the model learns, we'll often find very simple things at the lowest layers, like lines or edges. Once we move up, we tend to see more complicated things like geometric shapes. Go further up and we start seeing things like objects, faces.\n",
    "\n",
    "This is very powerful because the model structure matches the kind of abstractions that we might expect to see in your data. As a result, the model has an easier time learning them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 5. Save and Restore TensorFlow Models <a name='Save and Restore TensorFlow Models'></a>\n",
    "\n",
    "Training a model can take hours. But once we close our TensorFlow session, we lose all the trained weights and biases. If we were to reuse the model in the future, we would have to train it all over again!\n",
    "\n",
    "Fortunately, TensorFlow gives the ability to save your progress using a class called [tf.train.Saver](https://www.tensorflow.org/api_docs/python/tf/train/Saver). This class provides the functionality to save any [tf.Variable](https://www.tensorflow.org/api_docs/python/tf/Variable) to our file system.\n",
    "\n",
    "### 5.1. Saving Variables <a name='Saving Variables'></a>\n",
    "\n",
    "Let's start with a simple example of saving weights and bias Tensors. For the first example we'll just save two variables. Later examples will save all the weights in a practical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      "[[-1.1214844  -0.21496792 -0.78367382]\n",
      " [-1.11662614 -0.88709235 -0.73158282]]\n",
      "Bias:\n",
      "[-0.21182585  0.50396538  0.25480953]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# The file path to save the data\n",
    "save_file = './model.ckpt'\n",
    "\n",
    "# Two Tensor Variables: weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "# Class used to save and/or restore Tensor Variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all the Variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Show the values of weights and bias\n",
    "    print('Weights:')\n",
    "    print(sess.run(weights))\n",
    "    print('Bias:')\n",
    "    print(sess.run(bias))\n",
    "\n",
    "    # Save the model\n",
    "    saver.save(sess, save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The Tensors __weights__ and __bias__ are set to random values using the [tf.truncated_normal()](https://www.tensorflow.org/api_docs/python/tf/truncated_normal) function. The values are then saved to the __save_file__ location, \"model.ckpt\", using the [tf.train.Saver.save()](https://www.tensorflow.org/api_docs/python/tf/train/Saver#save) function. (The \".ckpt\" extension stands for \"checkpoint\".)\n",
    "\n",
    "If we're using TensorFlow 0.11.0RC1 or newer, a file called \"model.ckpt.meta\" will also be created. This file contains the TensorFlow graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.2. Save a Trained Model <a name='Save a Trained Model'></a>\n",
    "\n",
    "Let's see how to train a model and save its weights.\n",
    "\n",
    "First start with a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Remove previous Tensors and Operations\n",
    "tf.reset_default_graph()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('datasets/mnist', one_hot=True)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(\\\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n",
    "    .minimize(cost)\n",
    "\n",
    "# Calculate accuracy`\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's train that model, then save the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   - Validation Accuracy: 0.14180000126361847\n",
      "Epoch 10  - Validation Accuracy: 0.2818000018596649\n",
      "Epoch 20  - Validation Accuracy: 0.42899999022483826\n",
      "Epoch 30  - Validation Accuracy: 0.5163999795913696\n",
      "Epoch 40  - Validation Accuracy: 0.573199987411499\n",
      "Epoch 50  - Validation Accuracy: 0.6126000285148621\n",
      "Epoch 60  - Validation Accuracy: 0.6448000073432922\n",
      "Epoch 70  - Validation Accuracy: 0.673799991607666\n",
      "Epoch 80  - Validation Accuracy: 0.6953999996185303\n",
      "Epoch 90  - Validation Accuracy: 0.7164000272750854\n",
      "Trained Model Saved.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "save_file = './train_model.ckpt'\n",
    "batch_size = 128\n",
    "n_epochs = 100\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(n_epochs):\n",
    "        total_batch = math.ceil(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_features, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            sess.run(\n",
    "                optimizer,\n",
    "                feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "        # Print status for every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            valid_accuracy = sess.run(\n",
    "                accuracy,\n",
    "                feed_dict={\n",
    "                    features: mnist.validation.images,\n",
    "                    labels: mnist.validation.labels})\n",
    "            print('Epoch {:<3} - Validation Accuracy: {}'.format(\n",
    "                epoch,\n",
    "                valid_accuracy))\n",
    "\n",
    "    # Save the model\n",
    "    saver.save(sess, save_file)\n",
    "    print('Trained Model Saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.3. Load a Trained Model <a name='Load a Trained Model'></a>\n",
    "\n",
    "Let's load the weights and bias from memory, then check the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7372000217437744\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_file)\n",
    "\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: mnist.test.images, labels: mnist.test.labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 6. Loading the Weights and Biases into a New Model <a name='Loading the Weights and Biases into a New Model'></a>\n",
    "\n",
    "Sometimes we might want to adjust, or \"finetune\" a model that we have already trained and saved.\n",
    "\n",
    "However, loading saved Variables directly into a modified model can generate errors. Let's go over how to avoid these problems.\n",
    "\n",
    "### 6.1. Naming Error <a name='Naming Error'></a>\n",
    "\n",
    "TensorFlow uses a string identifier for Tensors and Operations called __name__. If a name is not given, TensorFlow will create one automatically. TensorFlow will give the first node the name __< Type \\>__, and then give the name **< Type >\\_< number >** for the subsequent nodes. Let's see how this can affect loading a model with a different order of __weights__ and __bias__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Weights: Variable:0\n",
      "Save Bias: Variable_1:0\n",
      "Load Weights: Variable_1:0\n",
      "Load Bias: Variable:0\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Assign requires shapes of both tensors to match. lhs shape= [3] rhs shape= [2,3]\n\t [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable, save/RestoreV2)]]\n\nCaused by op 'save/Assign', defined at:\n  File \"//anaconda/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"//anaconda/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"//anaconda/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"//anaconda/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"//anaconda/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"//anaconda/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"//anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"//anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"//anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"//anaconda/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"//anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"//anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"//anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-14-0e1713b00e31>\", line 29, in <module>\n    saver = tf.train.Saver()\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1051, in __init__\n    self.build()\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1081, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 675, in build\n    restore_sequentially, reshape)\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 414, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 155, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [3] rhs shape= [2,3]\n\t [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable, save/RestoreV2)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [3] rhs shape= [2,3]\n\t [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable, save/RestoreV2)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0e1713b00e31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Load the weights and bias - ERROR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1437\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1439\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [3] rhs shape= [2,3]\n\t [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable, save/RestoreV2)]]\n\nCaused by op 'save/Assign', defined at:\n  File \"//anaconda/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"//anaconda/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"//anaconda/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"//anaconda/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"//anaconda/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"//anaconda/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"//anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"//anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"//anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"//anaconda/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"//anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"//anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"//anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-14-0e1713b00e31>\", line 29, in <module>\n    saver = tf.train.Saver()\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1051, in __init__\n    self.build()\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1081, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 675, in build\n    restore_sequentially, reshape)\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 414, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 155, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [3] rhs shape= [2,3]\n\t [[Node: save/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@Variable\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable, save/RestoreV2)]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Remove the previous weights and bias\n",
    "tf.reset_default_graph()\n",
    "\n",
    "save_file = './model.ckpt'\n",
    "\n",
    "# Two Tensor Variables: weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Save Weights: {}'.format(weights.name))\n",
    "print('Save Bias: {}'.format(bias.name))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.save(sess, save_file)\n",
    "\n",
    "# Remove the previous weights and bias\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Two Variables: weights and bias\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Load Weights: {}'.format(weights.name))\n",
    "print('Load Bias: {}'.format(bias.name))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Load the weights and bias - ERROR\n",
    "    saver.restore(sess, save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You'll notice that the __name__ properties for __weights__ and __bias__ are different than when you saved the model. This is why the code produces the \"Assign requires shapes of both tensors to match\" error. The code __saver.restore(sess, save\\_file)__ is trying to load weight data into __bias__ and bias data into __weights__.\n",
    "\n",
    "Instead of letting TensorFlow set the __name__ property, let's set it manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "save_file = './model.ckpt'\n",
    "\n",
    "# Two Tensor Variables: weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]), name='weights_0')\n",
    "bias = tf.Variable(tf.truncated_normal([3]), name='bias_0')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Save Weights: {}'.format(weights.name))\n",
    "print('Save Bias: {}'.format(bias.name))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.save(sess, save_file)\n",
    "\n",
    "# Remove the previous weights and bias\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Two Variables: weights and bias\n",
    "bias = tf.Variable(tf.truncated_normal([3]), name='bias_0')\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]) ,name='weights_0')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Load Weights: {}'.format(weights.name))\n",
    "print('Load Bias: {}'.format(bias.name))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Load the weights and bias - No Error\n",
    "    saver.restore(sess, save_file)\n",
    "\n",
    "print('Loaded Weights and Bias successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "That worked! The Tensor names match and the data loaded correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 7. Regularization <a name='Regularization'></a>\n",
    "\n",
    "Why did we not figure out earlier that deep-models were effective? Many reasons, but mostly because deep-models only really shine if \n",
    "having enough data to train them. It's only in recent years that large enough data sets have made their way to the academic world. Another reason, we know better today how to train very, very big models using better regularization techniques. \n",
    "\n",
    "There is a general issue when we're doing numerical optimization, so called the skinny jeans problem. Skinny jeans look great, they fit perfectly, but they're really, really hard to get into. So, most people end up wearing jeans that are just a bit too big. It's exactly the same with deep networks. The network that's just the right size for data is very, very hard to optimize. So in practice, we always try networks that are way too big for our data and then we try our best to prevent them from overfitting.\n",
    "\n",
    "### 7.1. Investigation of Performance Under Validation Set <a name='Investigation of Performance Under Validation Set'></a>\n",
    "\n",
    "The first way we prevent over fitting is by looking at the performance under validation set, and stopping to train as soon as we stop improving. It's called early termination, and it's still the best way to prevent network from over-optimizing on the training set.\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-27 at 16.35.34.png' width=500>\n",
    "\n",
    "Another way is to apply regularization. Regularizing means applying artificial constraints on network that implicitly reduce the number of free parameters while not making it more difficult to optimize.\n",
    "\n",
    "In the skinny jeans analogy, think stretch pants. They fit just as well, but because they're flexible,  they don't make things harder to fit in. The stretch pants of are called L2 Regularization. The idea is to add another term to the loss, which penalizes large weights. \n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-27 at 16.37.50.png' width=400>\n",
    "\n",
    "### 7.2. L2 Regularization <a name='L2 Regularization'></a>\n",
    "\n",
    "It's typically achieved by adding the L2 norm of weights to the loss, multiplied by a small constant. \n",
    "\n",
    "The nice thing about L2 regularization is that its very, very simple. Because we just add it to loss, the structure of our network\n",
    "doesn't have to change. We can even compute its derivative by hand. Remember that the L2 norm stands for the sum of the squares of\n",
    "the individual elements in a vector, so the derivative of the L2 norm of a vector is just $W$.\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-27 at 16.46.18.png' width=400>\n",
    "\n",
    "We know that the derivative of $(\\frac{1}{2}x^2)'$ in one dimension is simply $x$. So when we take that derivative for each of the components of vector, we get the same components. Therefore, the answer is the third one, $W$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 8. Dropout <a name='Dropout'></a>\n",
    "\n",
    "There's another important technique for regularization that only emerged relatively recently and works amazingly well. It's called dropout and it works likes this.\n",
    "\n",
    "Imagine that we have one layer that connects to another layer.\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-27 at 17.37.43.png' width=400>\n",
    "\n",
    "The values that go from one layer to the next are often called activations. Now take those activations, and for every example we train the network on, randomly set half of them to zero. \n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-27 at 17.52.25.png' width=300>\n",
    "\n",
    "Completely randomly, we basically take half of the data that's flowing through the network and just destroy it.\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-27 at 17.55.56.png' width=300>\n",
    "\n",
    "And then randomly again.\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-27 at 17.56.42.png' width=300>\n",
    "\n",
    "So what happens with dropout? \n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-27 at 18.01.25.png' width=500>\n",
    "\n",
    "The network can never rely on any given activation to be present, because they might be squashed at any given moment. So it is forced to learn a redundant representation for everything to make sure that at least some of the information remains.\n",
    "\n",
    "It's like a game of whack-a-mole. One activations gets smashed,\n",
    "but there's always one or more that do the same job and that don't get killed. So everything remains fine at the end. \n",
    "\n",
    "Forcing our network to learn redundant representations might sound very inefficient. But in practice, it makes things more robust and prevents over fitting. It also makes the network act as if taking the consensus over an ensemble of networks, which is always a good way to improve performance.\n",
    "\n",
    "Dropout is one of the most important techniques to emerge in the last few years. If drop out doesn't work for, we should probably\n",
    "be using a bigger network.\n",
    "\n",
    "When we evaluate the network that's been trained with drop out, we obviously no longer want this randomness. We want something deterministic. Instead, we're going to want to take the consensus over these redundant models. We get the consensus opinion by averaging the activations. \n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-27 at 18.11.11.png' width=400>\n",
    "\n",
    "We want $Y_e$ here to be the average of all the $y_t$s that we\n",
    "got during training. \n",
    "\n",
    "Here's a trick to make sure this expectation holds. During training, not only do you use zero out the activations that we drop out, but we also scale the remaining activations by a factor of 2.\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-27 at 18.16.14.png' width=400>\n",
    "\n",
    "This way, when it comes time to average them during evaluation, we just remove these dropouts and scaling operations from our neural net.\n",
    "\n",
    "<img src='Figures4/Screen Shot 2017-03-27 at 18.19.46.png' width=400>\n",
    "\n",
    "And the result is an average of these activations that is properly scaled.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 8.1. TensorFlow Dropout\n",
    "\n",
    "<img src='Figures4/dropout-node.jpg' width=400>\n",
    "$$ Figure 1: Taken from the paper \"[Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)\"\n",
    "\n",
    "Dropout is a regularization technique for reducing overfitting. The technique temporarily drops units ([artificial neurons](https://en.wikipedia.org/wiki/Artificial_neuron)) from the network, along with all of those units' incoming and outgoing connections. Figure 1 illustrates how dropout works.\n",
    "\n",
    "TensorFlow provides the [tf.nn.dropout()](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) function, which we can use to implement dropout.\n",
    "\n",
    "```Python\n",
    "keep_prob = tf.placeholder(tf.float32) # probability to keep units\n",
    "\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "```\n",
    "\n",
    "The code above illustrates how to apply dropout to a neural network.\n",
    "\n",
    "The __tf.nn.dropout()__ function takes in two parameters:\n",
    "1. __hidden_layer__: the tensor to which we would like to apply dropout\n",
    "2. __keep_prob__: the probability of keeping (i.e. not dropping) any given unit\n",
    "\n",
    "__keep_prob__ allows to adjust the number of units to drop. In order to compensate for dropped units, __tf.nn.dropout()__ multiplies all units that are kept (i.e. not dropped) by $\\frac{1}{\\text{keep_prob}}$.\n",
    "\n",
    "During training, a good starting value for keep_prob is 0.5.\n",
    "\n",
    "During testing, use a keep_prob value of 1.0 to keep all units and maximize the power of the model.\n",
    "\n",
    "__Quiz 1:__\n",
    "\n",
    "Take a look at the code snippet below. Do you see what's wrong?\n",
    "\n",
    "There's nothing wrong with the syntax, however the test accuracy is extremely low.\n",
    "\n",
    "```Python\n",
    "...\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32) # probability to keep units\n",
    "\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "\n",
    "...\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i in range(batches):\n",
    "            ....\n",
    "\n",
    "            sess.run(optimizer, feed_dict={\n",
    "                features: batch_features,\n",
    "                labels: batch_labels,\n",
    "                keep_prob: 0.5})\n",
    "\n",
    "    validation_accuracy = sess.run(accuracy, feed_dict={\n",
    "        features: test_features,\n",
    "        labels: test_labels,\n",
    "        keep_prob: 0.5})\n",
    "```\n",
    "\n",
    "__Answer 1:__\n",
    "\n",
    "__keep_prob__ should be set to 1.0 when evaluating validation accuracy. We should only drop units while training the model. During validation or testing, we should keep all of the units to maximize accuracy.\n",
    "\n",
    "__Quiz 2:__\n",
    "\n",
    "This quiz will be starting with the code from the ReLU Quiz and applying a dropout layer. Build a model with a ReLU layer and dropout layer using the __keep_prob__ placeholder to pass in a probability of __0.5__. Print the logits from the model.\n",
    "\n",
    "__Note__: Output will be different every time the code is run. This is caused by dropout randomizing the units it drops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "hidden_layer_weights = [\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.4, 0.6, 0.6],\n",
    "    [0.5, 0.9, 0.1],\n",
    "    [0.8, 0.2, 0.8]]\n",
    "out_weights = [\n",
    "    [0.1, 0.6],\n",
    "    [0.2, 0.1],\n",
    "    [0.7, 0.9]]\n",
    "\n",
    "# Weights and biases\n",
    "weights = [\n",
    "    tf.Variable(hidden_layer_weights),\n",
    "    tf.Variable(out_weights)]\n",
    "biases = [\n",
    "    tf.Variable(tf.zeros(3)),\n",
    "    tf.Variable(tf.zeros(2))]\n",
    "\n",
    "# Input\n",
    "features = tf.Variable([[0.0, 2.0, 3.0, 4.0], [0.1, 0.2, 0.3, 0.4], [11.0, 12.0, 13.0, 14.0]])\n",
    "\n",
    "# TODO: Create Model with Dropout\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob=0.5)\n",
    "\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "\n",
    "# TODO: Print logits from a session\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    output = session.run(logits)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Answer 2:__\n",
    "\n",
    "```Python\n",
    "# Quiz Solution\n",
    "# Note: You can't run code in this tab\n",
    "import tensorflow as tf\n",
    "\n",
    "hidden_layer_weights = [\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.4, 0.6, 0.6],\n",
    "    [0.5, 0.9, 0.1],\n",
    "    [0.8, 0.2, 0.8]]\n",
    "out_weights = [\n",
    "    [0.1, 0.6],\n",
    "    [0.2, 0.1],\n",
    "    [0.7, 0.9]]\n",
    "\n",
    "# Weights and biases\n",
    "weights = [\n",
    "    tf.Variable(hidden_layer_weights),\n",
    "    tf.Variable(out_weights)]\n",
    "biases = [\n",
    "    tf.Variable(tf.zeros(3)),\n",
    "    tf.Variable(tf.zeros(2))]\n",
    "\n",
    "# Input\n",
    "features = tf.Variable([[0.0, 2.0, 3.0, 4.0], [0.1, 0.2, 0.3, 0.4], [11.0, 12.0, 13.0, 14.0]])\n",
    "\n",
    "# TODO: Create Model with Dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "\n",
    "# TODO: Print logits from a session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(logits, feed_dict={keep_prob: 0.5}))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
