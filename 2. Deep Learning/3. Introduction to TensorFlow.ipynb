{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1. [Tensor](#Tensor)\n",
    "2. [Session](#Session)\n",
    "3. [Input](#Input)\n",
    "    1. [tf.placeholder()](#tf.placeholder)\n",
    "    2. [Session's feed_dict](#feed_dict)\n",
    "4. [TensorFlow Math](#TensorFlow Math)\n",
    "    1. [Addition](#Addition)\n",
    "    2. [Substraction and Multiplication](#Substraction and Multiplication)\n",
    "    3. [Converting types](#Converting types)\n",
    "5. [Classification (Multinomial logistic classification)](#Classification)\n",
    "    1. [Logistic classifier](#Logistic classifier)\n",
    "    2. [TensorFlow Linear Function](#TensorFlow Linear Function)\n",
    "    3. [Weights and Bias in TensorFlow](#Weights and Bias in TensorFlow)\n",
    "    4. [TensorFlow Softmax](#TensorFlow Softmax)\n",
    "    5. [One-Hot Encoding](#One-Hot Encoding)\n",
    "    6. [Cross Entropy](#Cross Entropy)\n",
    "    8. [Minimizing Cross Entropy](#Minimizing Cross Entropy)\n",
    "6. [Practical Aspects of Learning](#Practical Aspects of Learning)\n",
    "    1. [Numerical Stability](#Numerical Stability)\n",
    "    2. [Validation set](#validation set)\n",
    "    3. [Stochastic Gradient Descent](#Stochastic Gradient Descent)\n",
    "    4. [Parameter Hyperspace](#Parameter Hyperspace)\n",
    "    5. [Mini-batch](#Mini-batch)\n",
    "    6. [Epochs](#Epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let’s analyze the Hello World script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "hello_constant = tf.constant('Hello World!')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "# 1. Tensor <a name='Tensor'></a>\n",
    "\n",
    "In TensorFlow, data isn’t stored as integers, floats, or strings. _These values are encapsulated in an object called a tensor_. In the case of __hello_constant = tf.constant('Hello World!')__, __hello_constant__ is a 0-dimensional string tensor, but tensors come in a variety of sizes as shown below:\n",
    "\n",
    "```Python\n",
    "# A is a 0-dimensional int32 tensor\n",
    "A = tf.constant(1234) \n",
    "# B is a 1-dimensional int32 tensor\n",
    "B = tf.constant([123,456,789]) \n",
    " # C is a 2-dimensional int32 tensor\n",
    "C = tf.constant([ [123,456,789], [222,333,444] ])\n",
    "```\n",
    "\n",
    "__tf.constant()__ is one of many TensorFlow operations you will use in this lesson. The tensor returned by __tf.constant()__ is called a __constant tensor__, because _the value of the tensor never changes_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "# 2. Session <a name='Session'></a>\n",
    "\n",
    "TensorFlow’s api is built around the idea of a computational graph, a way of visualizing a mathematical process which you learned about in the MiniFlow lesson. Let’s take the TensorFlow code you ran and turn that into a graph:\n",
    "\n",
    "<img src='Figures3/session.png' width = 500>\n",
    "\n",
    "A \"TensorFlow Session\", as shown above, is _an environment for running a graph_. The session is in charge of allocating the operations to GPU(s) and/or CPU(s), including remote machines. Let’s see how you use it.\n",
    "\n",
    "```Python\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(hello_constant)\n",
    "```\n",
    "\n",
    "The code has already created the tensor, __hello_constant__, from the previous lines. The next step is to evaluate the tensor in a session.\n",
    "\n",
    "The code creates a session instance, __sess__, using __tf.Session__. The __sess.run()__ function then evaluates the tensor and returns the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "# 3. Input <a name='Input'></a>\n",
    "\n",
    "In the last section, you passed a tensor into a session and it returned the result. What if you want to use a non-constant? This is where __tf.placeholder()__ and __feed_dict__ come into place. In this section, you'll go over the basics of feeding data into TensorFlow.\n",
    "\n",
    "### 3.1. tf.placeholder() <a name='tf.placeholder'></a>\n",
    "\n",
    "Sadly you can’t just set __x__ to your dataset and put it in TensorFlow, because over time you'll want your TensorFlow model to take in different datasets with different parameters. You need __tf.placeholder()__!\n",
    "\n",
    "__tf.placeholder()__ returns a tensor that gets its value from data passed to the __tf.session.run()__ function, allowing you to set the input right before the session runs.\n",
    "\n",
    "### 3.2. Session's feed_dict <a name='feed_dict'></a>\n",
    "\n",
    "```Python\n",
    "x = tf.placeholder(tf.string)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Hello World'})\n",
    "```\n",
    "\n",
    "Use the __feed_dict__ parameter in __tf.session.run()__ to set the placeholder tensor. The above example shows the tensor __x__ being set to the string __\"Hello, world\"__. It's also possible to set more than one tensor using __feed_dict__ as shown below.\n",
    "\n",
    "```Python\n",
    "x = tf.placeholder(tf.string)\n",
    "y = tf.placeholder(tf.int32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Test String', y: 123, z: 45.67})\n",
    "```\n",
    "\n",
    "__Note__: If the data passed to the **feed\\_dict** doesn’t match the tensor type and can’t be cast into the tensor type, you’ll get the error “__ValueError: invalid literal for__...”.\n",
    "\n",
    "__Quiz__:\n",
    "Let's see how well you understand __tf.placeholder()__ and __feed_dict__. The code below throws an error, but I want you to make it return the number __123__. Change line 11, so that the code returns the number __123__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def run():\n",
    "    output = None\n",
    "    x = tf.placeholder(tf.int32)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # TODO: Feed the x tensor 123\n",
    "        output = sess.run(x)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Answer__:\n",
    "\n",
    "```Python\n",
    "# Quiz Solution\n",
    "# Note: You can't run code in this tab\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def run():\n",
    "    output = None\n",
    "    x = tf.placeholder(tf.int32)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        output = sess.run(x, feed_dict={x: 123})\n",
    "\n",
    "    return output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "# 4. TensorFlow Math <a name='TensorFlow Math'></a>\n",
    "\n",
    "[Documentation](https://www.tensorflow.org/api_guides/python/math_ops)\n",
    "\n",
    "### 4.1. Addition <a name='Addition'></a>\n",
    "\n",
    "```Python\n",
    "x = tf.add(5, 2)  # 7\n",
    "```\n",
    "\n",
    "The __tf.add()__ function takes in two numbers, two tensors, or one of each, and returns their sum as a tensor.\n",
    "\n",
    "### 4.2. Substraction and Multiplication <a name='Substraction and Multiplication'></a>\n",
    "\n",
    "```Python\n",
    "x = tf.subtract(10, 4) # 6\n",
    "y = tf.multiply(2, 5)  # 10\n",
    "```\n",
    "\n",
    "### 4.3. Converting types <a name='Converting types'></a>\n",
    "\n",
    "It may be necessary to convert between types to make certain operators work together. For example, if you tried the following, it would fail with an exception:\n",
    "\n",
    "```Python\n",
    "tf.subtract(tf.constant(2.0),tf.constant(1))  # Fails with ValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32:\n",
    "```\n",
    "\n",
    "That's because the constant __1__ is an integer but the constant __2.0__ is a floating point value and __subtract__ expects them to match.\n",
    "\n",
    "In cases like these, you can either make sure your data is all of the same type, or you can cast a value to another type. In this case, converting the __2.0__ to an integer before subtracting, like so, will give the correct result:\n",
    "\n",
    "```Python\n",
    "tf.subtract(tf.cast(tf.constant(2.0), tf.int32), tf.constant(1))   # 1\n",
    "```\n",
    "\n",
    "__Quiz:__\n",
    "\n",
    "Convert the following algorithm in regular Python to TensorFlow and print the results of the session. You can use tf.constant() for the values 10, 2, and 1.\n",
    "\n",
    "```Python\n",
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "# TODO: Convert the following to TensorFlow:\n",
    "x = 10\n",
    "y = 2\n",
    "z = x/y - 1\n",
    "\n",
    "# TODO: Print z from a session\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "# TODO: Convert the following to TensorFlow:\n",
    "x = 10\n",
    "y = 2\n",
    "z = x/y - 1\n",
    "\n",
    "# TODO: Print z from a session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Anser__:\n",
    "\n",
    "```Python\n",
    "# Quiz Solution\n",
    "# Note: You can't run code in this tab\n",
    "import tensorflow as tf\n",
    "\n",
    "# TODO: Convert the following to TensorFlow:\n",
    "x = tf.constant(10)\n",
    "y = tf.constant(2)\n",
    "z = tf.subtract(tf.divide(x,y),tf.cast(tf.constant(1), tf.float64))\n",
    "\n",
    "# TODO: Print z from a session\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(z)\n",
    "    print(output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "# 5. Classification (Multinomial logistic classification) <a name='Classification'></a>\n",
    "\n",
    "Classification is the task of taking an input, and giving it a label. \n",
    "\n",
    "The typical setting for classification is a lot of examples called the training set which is already been sorted in. Then when a completely new example is come out, the goal is going to be to figure out which of those classes it belons to. \n",
    "\n",
    "There is a lot more to machine learning that just classification. But classification, or more generally prediction, is the central building block of machine learning. Once we know how to classify thins, it's very each, for example, to learn how to detect them, or to rank them, or for regression, or for reinforcement learning. \n",
    "\n",
    "### 5.1. Logistic classifier <a name='Logistic classifier'></a>\n",
    "\n",
    "This is basic but very important classifier, which is what's called the __linear classifier__.\n",
    "\n",
    "$$\n",
    "W\\mathbf{x} + b = y\n",
    "$$\n",
    "\n",
    "It takes the input $X$, for example, the pixels in an image, and applies a linear function to them to generate its predictions, $Y$. A linear function is just a giant matrix multiply. The weights, $W$, of the matrix and the bias, $b$, is where the machine learning comes in. We are going to train that model, which means going to try to find the values for the weights and bias, which are good at performing those predictions.\n",
    "\n",
    "How are we going to use the scores, $y$, to perform the classification? For example, each image that we have as an input can have one and only one possible label. So, we are going to turn the scores, $y$, into probabilities of which the correct class which is very close to 1 and the probability for every other class to be close to 0.\n",
    "\n",
    "<img src='Figures3/Screen Shot 2017-03-22 at 12.33.23.png' width=400>\n",
    "\n",
    "The way to turn scores into probabilities is to use a softmax function, $S(y_i)$. \n",
    "\n",
    "$$\n",
    "S(y_i) = \\frac{y_i}{\\sum_j e^{y_j}}\n",
    "$$\n",
    "\n",
    "<img src='Figures3/Screen Shot 2017-03-22 at 12.35.11.png' width=400>\n",
    "\n",
    "Scores in the context of logistic regression are often also called __logits__. \n",
    "\n",
    "### 5.2. TensorFlow Linear Function <a name='TensorFlow Linear Function'></a>\n",
    "\n",
    "Let’s derive the function $y = W\\mathbf{x} + b$. We want to translate our input, $x$, to labels, $y$.\n",
    "\n",
    "For example, imagine we want to classify images as digits.\n",
    "\n",
    "$x$ would be our list of pixel values, and $y$ would be the logits, one for each digit. Let's take a look at $y = Wx$, where the weights, $W$, determine the influence of $x$ at predicting each $y$.\n",
    "\n",
    "<img src='Figures3/wx-1.jpg' width = 500>\n",
    "$$ \\text{Function } y=W\\mathbf{x} $$\n",
    "\n",
    "$y = W\\mathbf{x}$ allows us to segment the data into their respective labels using a line.\n",
    "\n",
    "However, this line has to pass through the origin, because whenever $x$ equals 0, then $y$ is also going to equal 0.\n",
    "\n",
    "We want the ability to shift the line away from the origin to fit more complex data. The simplest solution is to add a number to the function, which we call “bias”.\n",
    "\n",
    "<img src='Figures3/wx-b.jpg' width = 500>\n",
    "$$ \\text{Function } y=W\\mathbf{x}+b $$\n",
    "\n",
    "Our new function becomes $W\\mathbf{x} + b$, allowing us to create predictions on linearly separable data. Let’s use a concrete example and calculate the logits.\n",
    "\n",
    "For example, \n",
    "\n",
    "<img src='Figures3/codecogseqn-13.gif' width=500>\n",
    "$$ y=W\\mathbf{x}+b $$\n",
    "\n",
    "We've been using the $y = W\\mathrm{x} + b$ function for our linear function.\n",
    "\n",
    "But there's another function that does the same thing, y = $\\mathrm{x}W + b$. These functions do the same thing and are interchangeable, except for the dimensions of the matrices involved.\n",
    "\n",
    "To shift from one function to the other, you simply have to swap the row and column dimensions of each matrix. This is called transposition.\n",
    "\n",
    "For example,\n",
    "\n",
    "<img src='Figures3/codecogseqn-18.gif' width=500>\n",
    "$$ \\text{Function } y= \\text{x}W+b $$\n",
    "\n",
    "The above example is identical, except that the matrices are transposed.\n",
    "\n",
    "$\\mathrm{x}$ now has the dimensions 1x3, $W$ now has the dimensions 3x2, and $b$ now has the dimensions 1x2. Calculating this will produce a matrix with the dimension of 1x2.\n",
    "\n",
    "You'll notice that the elements in this 1x2 matrix are the same as the elements in the 2x1 matrix from the equation before transposition. Again, these matrices are simply transposed.\n",
    "\n",
    "<img src='Figures3/codecogseqn-20.gif' width=200>\n",
    "\n",
    "We now have our logits! The columns represent the logits for our two labels.\n",
    "\n",
    "Now you can learn how to train this function in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.3. Weights and Bias in TensorFlow <a name='Weights and Bias in TensorFlow'></a>\n",
    "\n",
    "__The goal of training a neural network is to modify weights and biases to best predict the labels__. In order to use weights and bias, you'll need a Tensor that can be modified. This leaves out __tf.placeholder()__ and __tf.constant()__, since those Tensors can't be modified. This is where __tf.Variable__ class comes in.\n",
    "\n",
    "#### a) tf.Variable():\n",
    "\n",
    "```Python\n",
    "x = tf.Variable(5)\n",
    "```\n",
    "\n",
    "The [tf.Variable](https://www.tensorflow.org/api_docs/python/tf/Variable) class creates a tensor with an initial value that can be modified, much like a normal Python variable. This tensor stores its state in the session, so you must initialize the state of the tensor manually. You'll use the [tf.global_variables_initializer()](https://www.tensorflow.org/programmers_guide/variables) function to initialize the state of all the Variable tensors.\n",
    "\n",
    "```Python\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "```\n",
    "\n",
    "The __tf.global_variables_initializer()__ call returns an operation that will initialize all TensorFlow variables from the graph. You call the operation using a session to initialize all the variables as shown above. Using the tf.Variable class allows us to change the weights and bias, but an initial value needs to be chosen.\n",
    "\n",
    "Initializing the weights with random numbers from a normal distribution is good practice. Randomizing the weights helps the model from becoming stuck in the same place every time you train it. You'll learn more about this in the next lesson, when you study gradient descent.\n",
    "\n",
    "Similarly, choosing weights from a normal distribution prevents any one weight from overwhelming other weights. You'll use the __tf.truncated_normal()__ function to generate random numbers from a normal distribution.\n",
    "\n",
    "#### tf.truncated_normal()\n",
    "\n",
    "```Python\n",
    "n_features = 120\n",
    "n_labels = 5\n",
    "weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "```\n",
    "The [tf.truncated_normal()](https://www.tensorflow.org/api_docs/python/tf/truncated_normal) function returns a tensor with random values from a normal distribution whose magnitude is no more than 2 standard deviations from the mean.\n",
    "\n",
    "Since the weights are already helping prevent the model from getting stuck, you don't need to randomize the bias. Let's use the simplest solution, setting the bias to 0.\n",
    "\n",
    "#### tf.zeros()\n",
    "```Python\n",
    "n_labels = 5\n",
    "bias = tf.Variable(tf.zeros(n_labels))\n",
    "```\n",
    "\n",
    "The [tf.zeros()](https://www.tensorflow.org/api_docs/python/tf/zeros) function returns a tensor with all zeros.\n",
    "\n",
    "__Linear Classifier Quiz:__\n",
    "\n",
    "<img src='Figures3/mnist-012.png' width=500>\n",
    "$$ \\text{Subset of MNIST dataset.} $$\n",
    "\n",
    "You'll be classifying the handwritten numbers 0, 1, and 2 from the MNIST dataset using TensorFlow. The above is a small sample of the data you'll be training on. Notice how some of the 1s are written with a serif at the top and at different angles. The similarities and differences will play a part in shaping the weights of the model.\n",
    "\n",
    "<img src='Figures3/weights-0-1-2.png' width=500>\n",
    "$$ \\text{Left: Weights for labeling 0. Middle: Weights for labeling 1. Right: Weights for labeling 2.} $$\n",
    "\n",
    "The images above are trained weights for each label (0, 1, and 2). The weights display the unique properties of each digit they have found. Complete this quiz to train your own weights using the MNIST dataset.\n",
    "\n",
    "1. Open quiz.py.\n",
    "    1. Implement get_weights to return a tf.Variable of weights\n",
    "    2. Implement get_biases to return a tf.Variable of biases\n",
    "    3. Implement xW + b in the linear function\n",
    "1. Open sandbox.py\n",
    "    1. Initialize all weights\n",
    "\n",
    "Since $xW$ in $xW + b$ is matrix multiplication, you have to use the [tf.matmul()](https://www.tensorflow.org/api_docs/python/tf/matmul) function instead of [tf.multiply()](https://www.tensorflow.org/api_docs/python/tf/multiply). Don't forget that order matters in matrix multiplication, so __tf.matmul(a,b)__ is not the same as __tf.matmul(b,a)__.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# quiz.py\n",
    "# Solution is available in the other \"quiz_solution.py\" tab\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_weights(n_features, n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow weights\n",
    "    :param n_features: Number of features\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow weights\n",
    "    \"\"\"\n",
    "    # TODO: Return weights\n",
    "    return tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "\n",
    "\n",
    "def get_biases(n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow bias\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow bias\n",
    "    \"\"\"\n",
    "    # TODO: Return biases\n",
    "    return tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "\n",
    "def linear(input, w, b):\n",
    "    \"\"\"\n",
    "    Return linear function in TensorFlow\n",
    "    :param input: TensorFlow input\n",
    "    :param w: TensorFlow weights\n",
    "    :param b: TensorFlow biases\n",
    "    :return: TensorFlow linear function\n",
    "    \"\"\"\n",
    "    # TODO: Linear Function (xW + b)\n",
    "    return tf.add(tf.matmul(input, w), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Loss: 4.6871771812438965\n"
     ]
    }
   ],
   "source": [
    "# sandbox.py\n",
    "\n",
    "# Solution is available in the other \"sandbox_solution.py\" tab\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#from quiz import get_weights, get_biases, linear\n",
    "\n",
    "\n",
    "def mnist_features_labels(n_labels):\n",
    "    \"\"\"\n",
    "    Gets the first <n> labels from the MNIST dataset\n",
    "    :param n_labels: Number of labels to use\n",
    "    :return: Tuple of feature list and label list\n",
    "    \"\"\"\n",
    "    mnist_features = []\n",
    "    mnist_labels = []\n",
    "\n",
    "    mnist = input_data.read_data_sets('datasets/mnist', one_hot=True)\n",
    "    #mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "    \n",
    "    # In order to make quizzes run faster, we're only looking at 10000 images\n",
    "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
    "\n",
    "        # Add features and labels if it's for the first <n>th labels\n",
    "        if mnist_label[:n_labels].any():\n",
    "            mnist_features.append(mnist_feature)\n",
    "            mnist_labels.append(mnist_label[:n_labels])\n",
    "\n",
    "    return mnist_features, mnist_labels\n",
    "\n",
    "\n",
    "# Number of features (28*28 image is 784 features)\n",
    "n_features = 784\n",
    "# Number of labels\n",
    "n_labels = 3\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32)\n",
    "\n",
    "# Weights and Biases\n",
    "w = get_weights(n_features, n_labels)\n",
    "b = get_biases(n_labels)\n",
    "\n",
    "# Linear Function xW + b\n",
    "logits = linear(features, w, b)\n",
    "\n",
    "# Training data\n",
    "train_features, train_labels = mnist_features_labels(n_labels)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    # TODO: Initialize session variables\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Softmax\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Cross entropy\n",
    "    # This quantifies how far off the predictions were.\n",
    "    # You'll learn more about this in future lessons.\n",
    "    cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    "\n",
    "    # Training loss\n",
    "    # You'll learn more about this in future lessons.\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    # Rate at which the weights are changed\n",
    "    # You'll learn more about this in future lessons.\n",
    "    learning_rate = 0.08\n",
    "\n",
    "    # Gradient Descent\n",
    "    # This is the method used to train the model\n",
    "    # You'll learn more about this in future lessons.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    # Run optimizer and get loss\n",
    "    _, l = session.run(\n",
    "        [optimizer, loss],\n",
    "        feed_dict={features: train_features, labels: train_labels})\n",
    "\n",
    "# Print loss\n",
    "print('Loss: {}'.format(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.4. TensorFlow Softmax <a name='TensorFlow Softmax'></a>\n",
    "\n",
    "The next step is to assign a probability to each label, which you can then use to classify the data. Use the softmax function to turn your logits into probabilities.\n",
    "\n",
    "$$\n",
    "S(y_i) = \\frac{e^{y_i}}{\\sum_j e^{y_j}}\n",
    "$$\n",
    "\n",
    "We can do this by using the formula above, which uses the input of $y$ values and the mathematical constant \"$e$\" which is approximately equal to 2.718. By taking \"$e$\" to the power of any real value we always get back a positive value, this then helps us scale when having negative $y$ values. The summation symbol on the bottom of the divisor indicates that we add together all the $e$^(input $y$ value) elements in order to get our calculated probability outputs.\n",
    "\n",
    "Implement a $softmax(x)$ function that takes in $x$, a one or two dimensional array of logits.\n",
    "\n",
    "In the one dimensional case, the array is just a single set of logits. In the two dimensional case, each column in the array is a set of logits. The $softmax(x)$ function should return a NumPy array of the same shape as $x$.\n",
    "\n",
    "For example, given a one-dimensional array:\n",
    "\n",
    "```Python\n",
    "# logits is a one-dimensional array with 3 elements\n",
    "logits = [1.0, 2.0, 3.0]\n",
    "# softmax will return a one-dimensional array with 3 elements\n",
    "print softmax(logits)\n",
    "```\n",
    "\n",
    "```Python\n",
    "-> [ 0.09003057  0.24472847  0.66524096]\n",
    "```\n",
    "\n",
    "Given a two-dimensional array where each column represents a set of logits:\n",
    "\n",
    "```Python\n",
    "# logits is a two-dimensional array\n",
    "logits = np.array([\n",
    "    [1, 2, 3, 6],\n",
    "    [2, 4, 5, 6],\n",
    "    [3, 8, 7, 6]])\n",
    "# softmax will return a two-dimensional array with the same shape\n",
    "print softmax(logits)\n",
    "```\n",
    "\n",
    "```Python\n",
    "->[\n",
    "    [ 0.09003057  0.00242826  0.01587624  0.33333333]\n",
    "    [ 0.24472847  0.01794253  0.11731043  0.33333333]\n",
    "    [ 0.66524096  0.97962921  0.86681333  0.33333333]\n",
    "  ]\n",
    "```\n",
    "\n",
    "The probabilities for each column must sum to 1. Feel free to test your function with the inputs above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    # TODO: Compute and return softmax(x)\n",
    "    return np.exp(x) / sum(np.exp(x))\n",
    "\n",
    "logits = [3.0, 1.0, 0.2]\n",
    "print(softmax(logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Answer:__\n",
    "```Python\n",
    "# Quiz Solution\n",
    "# Note: You can't run code in this tab\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "logits = [3.0, 1.0, 0.2]\n",
    "print(softmax(logits))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that you've built a softmax function from scratch, let's see how softmax is done in TensorFlow.\n",
    "\n",
    "```Python\n",
    "x = tf.nn.softmax([2.0, 1.0, 0.2])\n",
    "```\n",
    "\n",
    "Easy as that! __tf.nn.softmax()__ implements the softmax function for you. It takes in logits and returns softmax activations.\n",
    "\n",
    "Use the softmax function in the quiz below to return the softmax of the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def run():\n",
    "    output = None\n",
    "    logit_data = [2.0, 1.0, 0.1]\n",
    "    #logit_data = [20.0, 10.0, 1] # multiply the logits by 10. (Probabilities get closer to 0.0 or 1.0)\n",
    "    logit_data = [2.0, 1.0, 0.1] # divide the logits by 10. (The probabilities get close to the uniform distribution)\n",
    "\n",
    "    logits = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # TODO: Calculate the softmax of the logits\n",
    "    softmax = tf.nn.softmax(logits)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # TODO: Feed in the logit data\n",
    "        output = sess.run(softmax, feed_dict={logits: logit_data})\n",
    "\n",
    "    return output\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Answer:__\n",
    "```Python\n",
    "# Quiz Solution\n",
    "# Note: You can't run code in this tab\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def run():\n",
    "    output = None\n",
    "    logit_data = [2.0, 1.0, 0.1]\n",
    "    logits = tf.placeholder(tf.float32)\n",
    "\n",
    "    softmax = tf.nn.softmax(logits)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        output = sess.run(softmax, feed_dict={logits: logit_data})\n",
    "\n",
    "    return output\n",
    "```\n",
    "\n",
    "- If we multiply the logits by 10, probabilities get closer to 0.0 or 1.0)\n",
    "- If we divide the logits by 10, the probabilities get close to the uniform distribution, since all the scores decrease in magnitude, the resulting softmax probabilities will be closer to each other.\n",
    "\n",
    "In other words, if we increase the size of the outputs from the linear function, the classifier becomes very confident about its predictions. But if we reduce the size of the outputs,  the classifier becomes very unsure.\n",
    "\n",
    "<img src='Figures3/Screen Shot 2017-03-24 at 14.15.13.png' width=400>\n",
    "\n",
    "We'll want the classifier to not be too sure of itself in the beginning, and then over time, it will gain confidence as it learns. \n",
    "\n",
    "Hot encoding works very well for most problems until we get into situations where we have tens of thousands, or even millions of classes. In that case, the vector becomes really really large and has mostly zeros everywhere and that becomes very inefficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.5. One-Hot Encoding <a name='One-Hot Encoding'></a>\n",
    "\n",
    "Now, we have the probabilities, and let them for the correct class be close to 1, and the probability for all the others be close to 0. This is often called one-hot encoding.\n",
    "\n",
    "<img src='Figures3/Screen Shot 2017-03-24 at 14.13.49.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.6. Cross Entropy<a name='Cross Entropy'></a>\n",
    "\n",
    "We can now measure how well we're doing by simply comparing two vectors; one that comes out of the classifiers (out of sigmoid function) containing the probabilities of the classes, and the one hot encoded vector that corresponds to the labels. \n",
    "\n",
    "The natural way to measure the distance between those two probability vectors is called the cross-entropy, denoted by $D$ for distance. \n",
    "$$\n",
    "D(S, L) = - \\sum_i L_i log(s_i).\n",
    "$$\n",
    "\n",
    "The cross-entropy is not symmetric, $D(S,L) \\not= D(L,S)$ and a nasty log in there, because we do not want to take log of zero.\n",
    "\n",
    "<img src='Figures3/Screen Shot 2017-03-24 at 16.36.59.png' width=400>\n",
    "\n",
    "This is the overview of our task.\n",
    "\n",
    "<img src='Figures3/Screen Shot 2017-03-24 at 16.41.10.png' width=400>\n",
    "\n",
    "This entire setting is often called __multinomial logistic classification__.\n",
    "- Input is going to be turned into logits using a linear model, which is basically matrix multiply and a bias. \n",
    "- Logits which are scores are goind to be fed into a softmax to turn them into probabilities.\n",
    "- We compare those probabilities to the one hot encoded labels using the cross entropy function. \n",
    "\n",
    "To create a cross entropy function in TensorFlow, we'll need to use two new functions:\n",
    "\n",
    "- Reduce Sum __tf.reduce_sum()__ : This function function takes an array of numbers and sums them together.\n",
    "```Python\n",
    "x = tf.reduce_sum([1, 2, 3, 4, 5])  # 15\n",
    "```\n",
    "- Natural Log __tf.log()__ : This function does exactly what you would expect it to do. tf.log() takes the natural log of a number.\n",
    "```Python\n",
    "x = tf.log(100)  # 4.60517\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "softmax_data = [0.7, 0.2, 0.1]\n",
    "one_hot_data = [1.0, 0.0, 0.0]\n",
    "\n",
    "softmax = tf.placeholder(tf.float32)\n",
    "one_hot = tf.placeholder(tf.float32)\n",
    "\n",
    "cross_entropy = - tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))\n",
    "#cross_entropy = tf.reduce_sum(one_hot * tf.log(softmax))\n",
    "\n",
    "# TODO: Print cross entropy from session\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(cross_entropy, feed_dict={one_hot:one_hot_data , softmax: softmax_data})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Answer:__\n",
    "```Python\n",
    "# Quiz Solution\n",
    "# Note: You can't run code in this tab\n",
    "import tensorflow as tf\n",
    "\n",
    "softmax_data = [0.7, 0.2, 0.1]\n",
    "one_hot_data = [1.0, 0.0, 0.0]\n",
    "\n",
    "softmax = tf.placeholder(tf.float32)\n",
    "one_hot = tf.placeholder(tf.float32)\n",
    "\n",
    "# ToDo: Print cross entropy from session\n",
    "cross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(cross_entropy, feed_dict={softmax: softmax_data, one_hot: one_hot_data}))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 5.7. Minimizing Cross Entropy<a name='Minimizing Cross Entropy'></a>\n",
    "\n",
    "The question of course is how we're going to find those weights $w$ and those biases $b$ that will get the classifier to do what we want it to do; that is have a low distance for the correct class but have a high distance for the incorrect class. \n",
    "\n",
    "<img src='Figures3/Screen Shot 2017-03-24 at 17.54.57.png' width=250>\n",
    "\n",
    "One thing we can do is measure that distance averaged over the entire training sets for all the inputs and all the labels that we have available. \n",
    "\n",
    "<img src='Figures3/Screen Shot 2017-03-24 at 17.56.53.png' width=300>\n",
    "\n",
    "We want all the distances to be small, in other words, the minimum loss (average cross-entrop), which would mean we're doing a good job at classifying every example in the training data.  \n",
    "\n",
    "<img src='Figures3/Screen Shot 2017-03-24 at 17.59.52.png' width=350>\n",
    "\n",
    "The loss is a function of the weights and the biases, so we are simply going to try and minimize that function.\n",
    "\n",
    "In our example, the loss is a function of two weights (weight 1 and weight 2). The simplest way of numerical optimizations is gradient descent; take the derivative of the loss, with respect to the parameters, and follow that derivative by taking astep backwards, and repeat until we get to the bottom. \n",
    "\n",
    "<img src='Figures3/Screen Shot 2017-03-24 at 18.05.48.png' width=400>\n",
    "\n",
    "But for typical problem, it could be a function of thousands, millions or even billions of parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 6. Practical Aspects of Learning <a name='Practical Aspects of Learning'></a>\n",
    "\n",
    "### 6.1. Numerical Stability <a name='Numerical Stability'></a>\n",
    "\n",
    "When we do numerical computations, we always have to worry a bit about caculating values that are too large or too small. Inparticular, adding very small values to a very large value can introduce a lot of erros. \n",
    "\n",
    "<img src='Figures3/Screen Shot 2017-03-24 at 18.16.03.png' width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a = 1000000000\n",
    "for i in range(1000000):\n",
    "    a = a + 1e-6\n",
    "print(a - 1000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "One good guiding principle is that we always want the variables to have zero mean and equal variance whenever possible. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Mean: } x_i &= 0\\\\\n",
    "\\text{Variance: } \\sigma(x_i) &= \\sigma(x_j)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "There are also really good mathematical reasons to keep values we compute when we are doing optimization. A badly conditioned problem means that the optimizer has to do a lot of searching to go and find a good solution. A well conditioned problem makes it a lot easier for the optimizer to do its job. \n",
    "\n",
    "<img src='Figures3/Screen Shot 2017-03-24 at 18.26.37.png' width=500>\n",
    "\n",
    "In an example to dealing with images, pixel values are ranged from 0 to 255, so the regularization is,\n",
    "\n",
    "$$\n",
    "\\frac{Red - 128}{128}, \\frac{Green - 128}{128}, \\frac{Blue - 128}{128}.\n",
    "$$\n",
    "\n",
    "We also want the wieghts and biases to be initialized at a good enough starting point for the gradient descent to proceed. The general method is to draw the weights randomly from a Gaussian distribution with mean zero and standard deviation sigma. \n",
    "\n",
    "<img src='Figures3/Screen Shot 2017-03-24 at 18.36.43.png' width=500>\n",
    "\n",
    "The sigma value determines the order of magnitude of the outputs at the initial point of the optimization. Because of the softmax on top of it, the order of magnitude also determines the peakiness of the inital probability distribution. A large sigma means that the distribution will have large peacks; it's going to be very opinionated. A small sigma means that the distribution is very uncertain about things. \n",
    "\n",
    "It's usually better to begin with an uncertain distribution and let the optimization become more confident as the train progress. \n",
    "\n",
    "<img src='Figures3/Screen Shot 2017-03-24 at 18.39.32.png' width=400>\n",
    "\n",
    "And optimaztion is as follow,\n",
    "\n",
    "<img src='Figures3/Screen Shot 2017-03-24 at 18.41.12.png' width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 6.2. Validation Set <a name='validation set'></a>\n",
    "\n",
    "The problem is that the classifier memorizes has memorized the training set and it fails to generalize to new examples. Our job is to help for the classifier to generalize to new data. So, how do we measure the generalization instead of measuring how well the classifier memorize the data? \n",
    "\n",
    "The simplest way is to take a small subset of the training set, not use it in training and measure the error on that test data. \n",
    "\n",
    "But there is still a problem, because training a classifier is usually a process of trial and error. For example, we try a classifier, we measure its performance, and then we try another one, and we measure again and another and another, tweak the model and parameters, and finally we have what we think is the perfect classifier. Then if we get and try more data, and score its performance on that new data, however it doesn't do nearly as well.\n",
    "\n",
    "There are many ways to solve this problem, and popular way of it is to use validation set. Using validation set also can be a solution of overfitting problem. \n",
    "\n",
    "__Validation Set Size:__\n",
    "\n",
    "How big does the validation and test sets need to be? Well, the bigger the validation set, the more precise numbers will be. The bigger the test set, the less noisy the accuracy measurement will be. \n",
    "\n",
    "A useful rule of thumb is that a change that a change that affects 30 examples in the validation set, one way or another, is usually statistically significant, and typically can be trusted. \n",
    "\n",
    "Based on the __rule of 30__, for example, if we have 3000 examples in our varidation set with 80% accuracy, we can trust accuracy improvement with 81% of its change. \n",
    "\n",
    "$$\n",
    "\\frac{1.0 \\times 3000}{100} = 30\n",
    "$$\n",
    "\n",
    "If the accuracy improvement is 80.5%, we cannot trust it and can assume it's noisy.\n",
    "\n",
    "$$\n",
    "\\frac{0.5 \\times 3000}{100} = 15\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 6.3. Stochastic Gradient Descent <a name='Stochastic Gradient Descent'></a>\n",
    "\n",
    "The problem with scaling gradient descent is that if computing the loss takes $n$ floating point operations, computing its gradient takes about three times that compute.\n",
    "\n",
    "<img src='Figures3/Screen Shot 2017-03-24 at 20.46.41.png' width=500>\n",
    "\n",
    "The loss function is huge. It depends on every single element in the training set. That can be a lot of compute if the data set is big. And because gradient descent is iterative, we have to do that for many steps. \n",
    "\n",
    "So, instead of computing the loss, we're going to compute an estimate of it, a very bad estimate, in fact. It simply computing the average loss for a very small random fraction of the training data. Random is very important, because if the way we pick the samples isn't random enough, it no longer works at all. \n",
    "\n",
    "<img src='Figures3/Screen Shot 2017-03-24 at 20.53.13.png' width=400>\n",
    "\n",
    "We're going to take a very small sliver of the training data, compute the loss for that sample, compute the derivative for that smaple, and pretend that that derivative is the right direction to use to do gradient descent. \n",
    "\n",
    "<img src='Figures3/Screen Shot 2017-03-24 at 20.55.42.png' width=400>\n",
    "\n",
    "It is not at all the right direction, and in fact, at times it might increase the real loss, not reduce it. But we're going to compensate by doing this many many times, taking very very small steps each time, because each step is a lot cheaper to compute. But we pay more price. We have to take many more smaller steps instead of one large step. On balance, though, we win by a lot, because doing this this is vastly more efficient than doing gradient descent. \n",
    "\n",
    "This technique is called __stochastic gradient descent (S.G.D.)__. S.G.D. scales well with both data and model size, and we want both big data and big models. S.G.D. is nice and scalable. However, it comes with a lot of issues in practice, because it is the only one that's fast enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 6.4. Momentum and Learning Rate Decay <a name='Momentum and Learning Rate Decay'></a>\n",
    "\n",
    "Making the inputs zero mean and equal variance, and to have relatively small variace are very important for S.G.D. However, another thing can help S.G.D.'s some issues in practice. \n",
    "\n",
    "The first one is __momentum__. At each step, we're taking a very small step in a random direction in S.G.D., but that on aggregate, those steps take us toward the minimum of the loss. _We can take advantage of the knowledge that we've accumulated from previous steps about where we should be headed_. A cheap way to do that is to keep a running average of the gradients, and to use that running average instead of the direction of the current batch of the data. \n",
    "\n",
    "<img src='Figures3/Screen Shot 2017-03-24 at 22.42.26.png' width=500>\n",
    "\n",
    "The second one is __learning rate decay__. When replacing gradient descent with SGD, we're going to take smaller, noisier steps towards our objective. How small should that step be? It's beneficial to make that step smaller and smaller as we train. So, lowering it over time is the key thing.\n",
    "\n",
    "<img src='Figures3/Screen Shot 2017-03-24 at 23.03.52.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 6.5. Mini-batch <a name='Mini-batch'></a>\n",
    "\n",
    "Mini-batching is a technique for training on subsets of the dataset instead of all the data at one time. This provides the ability to train a model, even if a computer lacks the memory to store the entire dataset.\n",
    "\n",
    "Mini-batching is computationally inefficient, since you can't calculate the loss simultaneously across all samples. However, this is a small price to pay in order to be able to run the model at all.\n",
    "\n",
    "It's also quite useful combined with SGD. The idea is to randomly shuffle the data at the start of each epoch, then create the mini-batches. For each mini-batch, you train the network weights with gradient descent. Since these batches are random, you're performing SGD with each batch.\n",
    "\n",
    "Let's look at the MNIST dataset with weights and a bias to see if your machine can handle it.\n",
    "\n",
    "```Python\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "```\n",
    "\n",
    "__Question 1:__\n",
    "Calculate the memory size of train_features, train_labels, weights, and bias in bytes. Ignore memory for overhead, just calculate the memory required for the stored data.\n",
    "\n",
    "You may have to look up how much memory a float32 requires, using this link.\n",
    "- train_features Shape: (55000, 784) Type: float32\n",
    "- train_labels Shape: (55000, 10) Type: float32\n",
    "- weights Shape: (784, 10) Type: float32\n",
    "- bias Shape: (10,) Type: float32\n",
    "\n",
    "__Answer 1:__\n",
    "- train_features Shape: (55000, 784) Type: float32 = 172,480,000\n",
    "- train_labels Shape: (55000, 10) Type: float32 = 2,200,000\n",
    "- weights Shape: (784, 10) Type: float32 = 31,360\n",
    "- bias Shape: (10,) Type: float32 = 40\n",
    "\n",
    "The total memory space required for the inputs, weights and bias is around 174 megabytes, which isn't that much memory. You could train this whole dataset on most CPUs and GPUs.\n",
    "\n",
    "But larger datasets that you'll use in the future measured in gigabytes or more. It's possible to purchase more memory, but it's expensive. A Titan X GPU with 12 GB of memory costs over $1,000.\n",
    "\n",
    "Instead, in order to run large models on your machine, you'll learn how to use mini-batching.\n",
    "\n",
    "Let's look at how you implement mini-batching in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In order to use mini-batching, you must first divide your data into batches.\n",
    "\n",
    "Unfortunately, it's sometimes impossible to divide the data into batches of exactly equal size. For example, imagine you'd like to create batches of 128 samples each from a dataset of 1000 samples. Since 128 does not evenly divide into 1000, you'd wind up with 7 batches of 128 samples, and 1 batch of 104 samples. $(7 \\times 128 + 1 \\times 104 = 1000)$\n",
    "\n",
    "In that case, the size of the batches would vary, so you need to take advantage of TensorFlow's __tf.placeholder()__ function to receive the varying batch sizes.\n",
    "\n",
    "Continuing the example, if each sample had __n_input = 784__ features and __n_classes = 10__ possible labels, the dimensions for __features__ would be __[None, n_input]__ and __labels__ would be __[None, n_classes]__.\n",
    "\n",
    "```Python\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "```\n",
    "\n",
    "What does __None__ do here?\n",
    "\n",
    "The __None__ dimension is a placeholder for the batch size. At runtime, TensorFlow will accept any batch size greater than 0.\n",
    "\n",
    "Going back to our earlier example, this setup allows you to feed __features__ and __labels__ into the model as either the batches of 128 samples or the single batch of 104 samples.\n",
    "\n",
    "__Question 2:__\n",
    "Use the parameters below, how many batches are there, and what is the last batch size?\n",
    "\n",
    "- features is (50000, 400)\n",
    "- labels is (50000, 10)\n",
    "- batch_size is 128\n",
    "\n",
    "__Answer 2:__\n",
    "\n",
    "There are 391 batches, and the last batch size is 80."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Question 3:__\n",
    "\n",
    "Implement the batches function to batch features and labels. The function should return each batch with a maximum size of batch_size. To help you with the quiz, look at the following example output of a working batches function.\n",
    "\n",
    "```Python\n",
    "# 4 Samples of features\n",
    "example_features = [\n",
    "    ['F11','F12','F13','F14'],\n",
    "    ['F21','F22','F23','F24'],\n",
    "    ['F31','F32','F33','F34'],\n",
    "    ['F41','F42','F43','F44']]\n",
    "# 4 Samples of labels\n",
    "example_labels = [\n",
    "    ['L11','L12'],\n",
    "    ['L21','L22'],\n",
    "    ['L31','L32'],\n",
    "    ['L41','L42']]\n",
    "\n",
    "example_batches = batches(3, example_features, example_labels)\n",
    "```\n",
    "\n",
    "The example_batches variable would be the following:\n",
    "\n",
    "```Python\n",
    "[\n",
    "    # 2 batches:\n",
    "    #   First is a batch of size 3.\n",
    "    #   Second is a batch of size 1\n",
    "    [\n",
    "        # First Batch is size 3\n",
    "        [\n",
    "            # 3 samples of features.\n",
    "            # There are 4 features per sample.\n",
    "            ['F11', 'F12', 'F13', 'F14'],\n",
    "            ['F21', 'F22', 'F23', 'F24'],\n",
    "            ['F31', 'F32', 'F33', 'F34']\n",
    "        ], [\n",
    "            # 3 samples of labels.\n",
    "            # There are 2 labels per sample.\n",
    "            ['L11', 'L12'],\n",
    "            ['L21', 'L22'],\n",
    "            ['L31', 'L32']\n",
    "        ]\n",
    "    ], [\n",
    "        # Second Batch is size 1.\n",
    "        # Since batch size is 3, there is only one sample left from the 4 samples.\n",
    "        [\n",
    "            # 1 sample of features.\n",
    "            ['F41', 'F42', 'F43', 'F44']\n",
    "        ], [\n",
    "            # 1 sample of labels.\n",
    "            ['L41', 'L42']\n",
    "        ]\n",
    "    ]\n",
    "]\n",
    "```\n",
    "\n",
    "Implement the __batches function__ in the \"quiz.py\" file below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# quiz.py\n",
    "\n",
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    # TODO: Implement batching\n",
    "\n",
    "    output_batches = []\n",
    "    sample_size = len(features)\n",
    "    \n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        output_batches.append(batch)\n",
    "        \n",
    "    return output_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# from quiz import batches\n",
    "from pprint import pprint\n",
    "\n",
    "# 4 Samples of features\n",
    "example_features = [\n",
    "    ['F11','F12','F13','F14'],\n",
    "    ['F21','F22','F23','F24'],\n",
    "    ['F31','F32','F33','F34'],\n",
    "    ['F41','F42','F43','F44']]\n",
    "# 4 Samples of labels\n",
    "example_labels = [\n",
    "    ['L11','L12'],\n",
    "    ['L21','L22'],\n",
    "    ['L31','L32'],\n",
    "    ['L41','L42']]\n",
    "\n",
    "# PPrint prints data structures like 2d arrays, so they are easier to read\n",
    "pprint(batches(3, example_features, example_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Answer 3:__\n",
    "\n",
    "```Python\n",
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    # TODO: Implement batching\n",
    "    output_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        output_batches.append(batch)\n",
    "        \n",
    "    return output_batches\n",
    "```\n",
    "\n",
    "Let's use mini-batching to feed batches of MNIST features and labels into a linear model.\n",
    "\n",
    "Set the batch size and run the optimizer over all the __batches__ with the batches function. The recommended batch size is 128. If you have memory restrictions, feel free to make it smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# helper.py\n",
    "\n",
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    outout_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        outout_batches.append(batch)\n",
    "        \n",
    "    return outout_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Test Accuracy: 0.11140000075101852\n"
     ]
    }
   ],
   "source": [
    "# quiz.py\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# from helper import batches\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('datasets/mnist/', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "# TODO: Set batch size\n",
    "batch_size = 128\n",
    "assert batch_size is not None, 'You must set the batch size'\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # TODO: Train optimizer on all batches\n",
    "    for batch_features, batch_labels in batches(batch_size, train_features, train_labels):\n",
    "        sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Answer:__\n",
    "\n",
    "```Python\n",
    "# quiz.py\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from helper import batches\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "# TODO: Set batch size\n",
    "batch_size = 128\n",
    "assert batch_size is not None, 'You must set the batch size'\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # TODO: Train optimizer on all batches\n",
    "    for batch_features, batch_labels in batches(batch_size, train_features, train_labels):\n",
    "        sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))\n",
    "```\n",
    "\n",
    "The accuracy is low, but you probably know that you could train on the dataset more than once. You can train a model using the dataset multiple times. You'll go over this subject in the next section where we talk about \"epochs\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 6.6. Epochs <a name='Epochs'></a>\n",
    "\n",
    "An epoch is a single forward and backward pass of the whole dataset. This is used to increase the accuracy of the model without requiring more data. This section will cover epochs in TensorFlow and how to choose the right number of epochs.\n",
    "\n",
    "The following TensorFlow code trains a model using 10 epochs.\n",
    "\n",
    "```Python\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from helper import batches  # Helper function created in Mini-batching section\n",
    "\n",
    "\n",
    "def print_epoch_stats(epoch_i, sess, last_features, last_labels):\n",
    "    \"\"\"\n",
    "    Print cost and validation accuracy of an epoch\n",
    "    \"\"\"\n",
    "    current_cost = sess.run(\n",
    "        cost,\n",
    "        feed_dict={features: last_features, labels: last_labels})\n",
    "    valid_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: valid_features, labels: valid_labels})\n",
    "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(\n",
    "        epoch_i,\n",
    "        current_cost,\n",
    "        valid_accuracy))\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "valid_features = mnist.validation.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "valid_labels = mnist.validation.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "learn_rate = 0.001\n",
    "\n",
    "train_batches = batches(batch_size, train_features, train_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        # Loop over all batches\n",
    "        for batch_features, batch_labels in train_batches:\n",
    "            train_feed_dict = {\n",
    "                features: batch_features,\n",
    "                labels: batch_labels,\n",
    "                learning_rate: learn_rate}\n",
    "            sess.run(optimizer, feed_dict=train_feed_dict)\n",
    "\n",
    "        # Print cost and validation accuracy of an epoch\n",
    "        print_epoch_stats(epoch_i, sess, batch_features, batch_labels)\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))\n",
    "```\n",
    "\n",
    "Running the code will output the following:\n",
    "\n",
    "```Python\n",
    "Epoch: 0    - Cost: 11.0     Valid Accuracy: 0.204\n",
    "Epoch: 1    - Cost: 9.95     Valid Accuracy: 0.229\n",
    "Epoch: 2    - Cost: 9.18     Valid Accuracy: 0.246\n",
    "Epoch: 3    - Cost: 8.59     Valid Accuracy: 0.264\n",
    "Epoch: 4    - Cost: 8.13     Valid Accuracy: 0.283\n",
    "Epoch: 5    - Cost: 7.77     Valid Accuracy: 0.301\n",
    "Epoch: 6    - Cost: 7.47     Valid Accuracy: 0.316\n",
    "Epoch: 7    - Cost: 7.2      Valid Accuracy: 0.328\n",
    "Epoch: 8    - Cost: 6.96     Valid Accuracy: 0.342\n",
    "Epoch: 9    - Cost: 6.73     Valid Accuracy: 0.36 \n",
    "Test Accuracy: 0.3801000118255615\n",
    "```\n",
    "\n",
    "Each epoch attempts to move to a lower cost, leading to better accuracy.\n",
    "\n",
    "This model continues to improve accuracy up to Epoch 9. Let's increase the number of epochs to 100.\n",
    "\n",
    "```Python\n",
    "...\n",
    "Epoch: 79   - Cost: 0.111    Valid Accuracy: 0.86\n",
    "Epoch: 80   - Cost: 0.11     Valid Accuracy: 0.869\n",
    "Epoch: 81   - Cost: 0.109    Valid Accuracy: 0.869\n",
    "....\n",
    "Epoch: 85   - Cost: 0.107    Valid Accuracy: 0.869\n",
    "Epoch: 86   - Cost: 0.107    Valid Accuracy: 0.869\n",
    "Epoch: 87   - Cost: 0.106    Valid Accuracy: 0.869\n",
    "Epoch: 88   - Cost: 0.106    Valid Accuracy: 0.869\n",
    "Epoch: 89   - Cost: 0.105    Valid Accuracy: 0.869\n",
    "Epoch: 90   - Cost: 0.105    Valid Accuracy: 0.869\n",
    "Epoch: 91   - Cost: 0.104    Valid Accuracy: 0.869\n",
    "Epoch: 92   - Cost: 0.103    Valid Accuracy: 0.869\n",
    "Epoch: 93   - Cost: 0.103    Valid Accuracy: 0.869\n",
    "Epoch: 94   - Cost: 0.102    Valid Accuracy: 0.869\n",
    "Epoch: 95   - Cost: 0.102    Valid Accuracy: 0.869\n",
    "Epoch: 96   - Cost: 0.101    Valid Accuracy: 0.869\n",
    "Epoch: 97   - Cost: 0.101    Valid Accuracy: 0.869\n",
    "Epoch: 98   - Cost: 0.1      Valid Accuracy: 0.869\n",
    "Epoch: 99   - Cost: 0.1      Valid Accuracy: 0.869\n",
    "Test Accuracy: 0.8696000006198883\n",
    "```\n",
    "\n",
    "From looking at the output above, you can see the model doesn't increase the validation accuracy after epoch 80. Let's see what happens when we increase the learning rate.\n",
    "\n",
    "_learn_rate = 0.1_\n",
    "\n",
    "```Python\n",
    "Epoch: 76   - Cost: 0.214    Valid Accuracy: 0.752\n",
    "Epoch: 77   - Cost: 0.21     Valid Accuracy: 0.756\n",
    "Epoch: 78   - Cost: 0.21     Valid Accuracy: 0.756\n",
    "...\n",
    "Epoch: 85   - Cost: 0.207    Valid Accuracy: 0.756\n",
    "Epoch: 86   - Cost: 0.209    Valid Accuracy: 0.756\n",
    "Epoch: 87   - Cost: 0.205    Valid Accuracy: 0.756\n",
    "Epoch: 88   - Cost: 0.208    Valid Accuracy: 0.756\n",
    "Epoch: 89   - Cost: 0.205    Valid Accuracy: 0.756\n",
    "Epoch: 90   - Cost: 0.202    Valid Accuracy: 0.756\n",
    "Epoch: 91   - Cost: 0.207    Valid Accuracy: 0.756\n",
    "Epoch: 92   - Cost: 0.204    Valid Accuracy: 0.756\n",
    "Epoch: 93   - Cost: 0.206    Valid Accuracy: 0.756\n",
    "Epoch: 94   - Cost: 0.202    Valid Accuracy: 0.756\n",
    "Epoch: 95   - Cost: 0.2974   Valid Accuracy: 0.756\n",
    "Epoch: 96   - Cost: 0.202    Valid Accuracy: 0.756\n",
    "Epoch: 97   - Cost: 0.2996   Valid Accuracy: 0.756\n",
    "Epoch: 98   - Cost: 0.203    Valid Accuracy: 0.756\n",
    "Epoch: 99   - Cost: 0.2987   Valid Accuracy: 0.756\n",
    "Test Accuracy: 0.7556000053882599\n",
    "```\n",
    "\n",
    "Looks like the learning rate was increased too much. The final accuracy was lower, and it stopped improving earlier. Let's stick with the previous learning rate, but change the number of epochs to 80.\n",
    "\n",
    "```Python\n",
    "Epoch: 65   - Cost: 0.122    Valid Accuracy: 0.868\n",
    "Epoch: 66   - Cost: 0.121    Valid Accuracy: 0.868\n",
    "Epoch: 67   - Cost: 0.12     Valid Accuracy: 0.868\n",
    "Epoch: 68   - Cost: 0.119    Valid Accuracy: 0.868\n",
    "Epoch: 69   - Cost: 0.118    Valid Accuracy: 0.868\n",
    "Epoch: 70   - Cost: 0.118    Valid Accuracy: 0.868\n",
    "Epoch: 71   - Cost: 0.117    Valid Accuracy: 0.868\n",
    "Epoch: 72   - Cost: 0.116    Valid Accuracy: 0.868\n",
    "Epoch: 73   - Cost: 0.115    Valid Accuracy: 0.868\n",
    "Epoch: 74   - Cost: 0.115    Valid Accuracy: 0.868\n",
    "Epoch: 75   - Cost: 0.114    Valid Accuracy: 0.868\n",
    "Epoch: 76   - Cost: 0.113    Valid Accuracy: 0.868\n",
    "Epoch: 77   - Cost: 0.113    Valid Accuracy: 0.868\n",
    "Epoch: 78   - Cost: 0.112    Valid Accuracy: 0.868\n",
    "Epoch: 79   - Cost: 0.111    Valid Accuracy: 0.868\n",
    "Epoch: 80   - Cost: 0.111    Valid Accuracy: 0.869\n",
    "Test Accuracy: 0.86909999418258667\n",
    "```\n",
    "\n",
    "The accuracy only reached 0.86, but that could be because the learning rate was too high. Lowering the learning rate would require more epochs, but could ultimately achieve better accuracy.\n",
    "\n",
    "In the upcoming TensorFLow Lab, you'll get the opportunity to choose your own learning rate, epoch count, and batch size to improve the model's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
