{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Table of contents\n",
    "1. [What is a Neural Network?](#What is a Neural Network?)\n",
    "    1. [Forward Propagation](#Forward Propagation short)\n",
    "    2. [Graphs](#Graphs)\n",
    "2. [MiniFlow Architecture](#MiniFlow Architecture)\n",
    "    1. [ Nodes that Calculate](#Nodes that Calculate)\n",
    "    2. [The Add Subclass](#The Add Subclass)\n",
    "3. [Forward Propagation](#Forward Propagation)\n",
    "4. [Learning and Loss](#Learning and Loss)\n",
    "5. [Linear Transform](#Linear Transform)\n",
    "6. [sigmoid function](#sigmoid function)\n",
    "7. [Cost](#cost)\n",
    "8. [Gradient Descent](#Gradient Descent)\n",
    "9. [Backpropagation](#Backpropagation)\n",
    "    1. [Derivatives](#Derivatives)\n",
    "    2. [Chain Rule](#Chain Rule)\n",
    "    3. [New Code](#New Code)\n",
    "10. [Stochastic Gradient Descent](#Stochastic Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. What is a Neural Network? <a name=\"What is a Neural Network?\"></a>\n",
    "\n",
    "<img src=\"Figures2/example-neural-network.png\" width=400>\n",
    "\n",
    "A neural network is a graph of mathematical functions such as [linear combinations](https://en.wikipedia.org/wiki/Linear_combination) and activation functions. The graph consists of __nodes__ and __edges__.\n",
    "\n",
    "Nodes in each layer (except for nodes in the input layer) perform mathematical functions using inputs from nodes in the previous layers. For example, a node could represent $f(x,y)=x+y$, where $x$ and $y$ are input values from nodes in the previous layer.\n",
    "\n",
    "Similarly, each node creates an output value which may be passed to nodes in the next layer. The output value from the output layer does not get passed to a future layer (last layer!)\n",
    "\n",
    "Layers between the input layer and the output layer are called __hidden layers__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.1. Forward Propagation <a name=\"Forward Propagation short\"></a>\n",
    "\n",
    "By propagating values from the first layer (the input layer) through all the mathematical functions represented by each node, the network outputs a value. This process is called a __forward pass__.\n",
    "\n",
    "### 1.2. Graphs <a name=\"Graphs\"></a>\n",
    "\n",
    "<img src=\"Figures2/addition-graph.png\" width = 300>\n",
    "\n",
    "The nodes and edges create a graph structure. There are generally two steps to create neural networks:\n",
    "- Define the graph of nodes and edges.\n",
    "- Propagate values through the graph.\n",
    "\n",
    "__MiniFlow__ works the same way. You'll define the nodes and edges of your network with one method and then propagate values through the graph with another method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. MiniFlow Architecture <a name=\"MiniFlow Architecture\"></a>\n",
    "\n",
    "Let's consider how to implement this graph structure in MiniFlow. We'll use a Python class to represent a generic node.\n",
    "\n",
    "```Python\n",
    "class Node(object):\n",
    "    def __init__(self):\n",
    "        # Properties will go here!\n",
    "```\n",
    "\n",
    "We know that each node might receive input from multiple other nodes. We also know that each node creates a single output, which will likely be passed to other nodes. Let's add two lists: one to store references to the inbound nodes, and the other to store references to the outbound nodes.\n",
    "\n",
    "```Python\n",
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        # Node(s) from which this Node receives values\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # Node(s) to which this Node passes values\n",
    "        self.outbound_nodes = []\n",
    "        # For each inbound Node here, add this Node as an outbound Node to _that_ Node.\n",
    "        for n in self.inbound_nodes:\n",
    "            n.outbound_nodes.append(self)\n",
    "```\n",
    "\n",
    "Each node will eventually calculate a value that represents its output. Let's initialize the __value__ to __None__ to indicate that it exists but hasn't been set yet.\n",
    "\n",
    "```Python\n",
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        # Node(s) from which this Node receives values\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # Node(s) to which this Node passes values\n",
    "        self.outbound_nodes = []\n",
    "        # For each inbound Node here, add this Node as an outbound Node to _that_ Node.\n",
    "        for n in self.inbound_nodes:\n",
    "            n.outbound_nodes.append(self)\n",
    "        # A calculated value\n",
    "        self.value = None\n",
    "```\n",
    "\n",
    "Each node will need to be able to pass values forward and perform backpropagation (more on that later). For now, let's add a placeholder method for forward propagation. We'll deal with backpropagation later on.\n",
    "\n",
    "```Python\n",
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        # Node(s) from which this Node receives values\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # Node(s) to which this Node passes values\n",
    "        self.outbound_nodes = []\n",
    "        # For each inbound Node here, add this Node as an outbound Node to _that_ Node.\n",
    "        for n in self.inbound_nodes:\n",
    "            n.outbound_nodes.append(self)\n",
    "        # A calculated value\n",
    "        self.value = None\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        Compute the output value based on `inbound_nodes` and\n",
    "        store the result in self.value.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.1. Nodes that Calculate <a name=\"Nodes that Calculate\"> </a>\n",
    "\n",
    "While __Node__ defines the base set of properties that every node holds, only specialized [subclasses](https://docs.python.org/3/tutorial/classes.html#inheritance) of __Node__ will end up in the graph. As part of this lab, you'll build the subclasses of __Node__ that can perform calculations and hold values. For example, consider the __Input__ subclass of __Node__.\n",
    "\n",
    "```Python\n",
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        # An Input node has no inbound nodes,\n",
    "        # so no need to pass anything to the Node instantiator.\n",
    "        Node.__init__(self)\n",
    "\n",
    "    # NOTE: Input node is the only node where the value\n",
    "    # may be passed as an argument to forward().\n",
    "    #\n",
    "    # All other node implementations should get the value\n",
    "    # of the previous node from self.inbound_nodes\n",
    "    #\n",
    "    # Example:\n",
    "    # val0 = self.inbound_nodes[0].value\n",
    "    def forward(self, value=None):\n",
    "        # Overwrite the value if one is passed in.\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "```\n",
    "\n",
    "Unlike the other subclasses of __Node__, the __Input__ subclass does not actually calculate anything. The __Input__ subclass just holds a __value__, such as a data feature or a model parameter (weight/bias).\n",
    "\n",
    "You can set __value__ either explicitly or with the __forward()__ method. This value is then fed through the rest of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2.2. The Add Subclass <a name=\"The Add Subclass\"></a>\n",
    "\n",
    "__Add__, which is another subclass of __Node__, actually can perform a calculation (addition).\n",
    "\n",
    "```Python\n",
    "class Add(Node):\n",
    "    def __init__(self, x, y):\n",
    "        Node.__init__(self, [x, y])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        You'll be writing code here in the next quiz!\n",
    "        \"\"\"\n",
    "```\n",
    "\n",
    "Notice the difference in the **\\__init\\__** method, **Add.\\__init\\__(self, [x, y])**. Unlike the __Input__ class, which has no inbound nodes, the __Add__ class takes 2 inbound nodes, __x__ and __y__, and adds the values of those nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3. Forward Propagation <a name=\"Forward Propagation\"></a>\n",
    "\n",
    "__MiniFlow__ has two methods to help you define and then run values through your graphs: __topological_sort()__ and __forward_pass()__.\n",
    "\n",
    "<img src=\"Figures2/topological-sort.001.jpg\" width = 500>\n",
    "$$ \\text{An example of topological sorting} $$\n",
    "\n",
    "In order to define your network, you'll __need to define the order of operations for your nodes__. Given that the input to some node depends on the outputs of others, you need to flatten the graph in such a way where all the input dependencies for each node are resolved before trying to run its calculation. This is a technique called a [topological sort](https://en.wikipedia.org/wiki/Topological_sorting).\n",
    "\n",
    "The __topological_sort()__ function implements topological sorting using [Kahn's Algorithm](https://en.wikipedia.org/wiki/Topological_sorting#Kahn.27s_algorithm). The details of this method are not important, the result is; __topological_sort()__ returns a sorted list of nodes in which all of the calculations can run in series. __topological_sort()__ takes in a __feed_dict__, which is how we initially set a value for an Input node. The __feed_dict__ is represented by the Python dictionary data structure. Here's an example use case:\n",
    "\n",
    "```Python\n",
    "# Define 2 `Input` nodes.\n",
    "x, y = Input(), Input()\n",
    "\n",
    "# Define an `Add` node, the two above`Input` nodes being the input.\n",
    "add = Add(x, y)\n",
    "\n",
    "# The value of `x` and `y` will be set to 10 and 20 respectively.\n",
    "feed_dict = {x: 10, y: 20}\n",
    "\n",
    "# Sort the nodes with topological sort.\n",
    "sorted_nodes = topological_sort(feed_dict=feed_dict)\n",
    "```\n",
    "\n",
    "(You can find the source code for __topological_sort()__ in miniflow.py in the programming quiz below.)\n",
    "\n",
    "The other method at your disposal is __forward_pass()__, which actually runs the network and outputs a value.\n",
    "\n",
    "```Python\n",
    "def forward_pass(output_node, sorted_nodes):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_node`: The output node of the graph (no outgoing edges).\n",
    "        `sorted_nodes`: a topologically sorted list of nodes.\n",
    "\n",
    "    Returns the output node's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "\n",
    "    return output_node.value\n",
    "```\n",
    "\n",
    "### Quiz 1 - Passing Values Forward\n",
    "\n",
    "Creat and run this graph.\n",
    "\n",
    "<img src=\"Figures2/addition-graph.png\" width = 300>\n",
    "\n",
    "The neural network architecture is already there for you in nn.py. It's your job to finish __MiniFlow__ to make it work.\n",
    "\n",
    "#### Recipe 1) [Python List pop() Method](https://www.tutorialspoint.com/python/list_pop.htm)\n",
    "    - The method pop() removes and returns last object or obj from the list.\n",
    "    - pop(0) means it removes the element in the index that is first element of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You need to change the Add() class below.\n",
    "\"\"\"\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        # Nodes from which this Node receives values\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # Nodes to which this Node passes values\n",
    "        self.outbound_nodes = []\n",
    "        # A calculated value\n",
    "        self.value = None\n",
    "        # Add this node as an outbound node on its inputs.\n",
    "        for n in self.inbound_nodes:\n",
    "            n.outbound_nodes.append(self)\n",
    "\n",
    "    # These will be implemented in a subclass.\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        Compute the output value based on `inbound_nodes` and\n",
    "        store the result in self.value.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        # an Input node has no inbound nodes,\n",
    "        # so no need to pass anything to the Node instantiator\n",
    "        Node.__init__(self)\n",
    "\n",
    "    # NOTE: Input node is the only node that may\n",
    "    # receive its value as an argument to forward().\n",
    "    #\n",
    "    # All other node implementations should calculate their\n",
    "    # values from the value of previous nodes, using\n",
    "    # self.inbound_nodes\n",
    "    #\n",
    "    # Example:\n",
    "    # val0 = self.inbound_nodes[0].value\n",
    "    def forward(self, value=None):\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "\n",
    "\n",
    "class Add(Node):\n",
    "    def __init__(self, x, y):\n",
    "        # You could access `x` and `y` in forward with\n",
    "        # self.inbound_nodes[0] (`x`) and self.inbound_nodes[1] (`y`)\n",
    "        Node.__init__(self, [x, y])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set the value of this node (`self.value`) to the sum of its inbound_nodes.\n",
    "\n",
    "        Your code here!\n",
    "        \"\"\"\n",
    "        self.value = 0\n",
    "        for input_node in self.inbound_nodes:\n",
    "            self.value += input_node.value\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "No need to change anything below here!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort generic nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` node and the value is the respective value feed to that node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(output_node, sorted_nodes):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_node`: A node in the graph, should be the output node (have no outgoing edges).\n",
    "        `sorted_nodes`: A topologically sorted list of nodes.\n",
    "\n",
    "    Returns the output Node's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "\n",
    "    return output_node.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script builds and runs a graph with miniflow.\n",
    "\n",
    "There is no need to change anything to solve this quiz!\n",
    "\n",
    "However, feel free to play with the network! Can you also\n",
    "build a network that solves the equation below?\n",
    "\n",
    "(x + y) + y\n",
    "\"\"\"\n",
    "\n",
    "#from miniflow import *\n",
    "\n",
    "x, y = Input(), Input()\n",
    "\n",
    "f = Add(x, y)\n",
    "\n",
    "feed_dict = {x: 10, y: 5}Mb\n",
    "\n",
    "sorted_nodes = topological_sort(feed_dict)\n",
    "output = forward_pass(f, sorted_nodes)\n",
    "\n",
    "# NOTE: because topological_sort set the values for the `Input` nodes we could also access\n",
    "# the value for x with x.value (same goes for y).\n",
    "print(\"{} + {} = {} (according to miniflow)\".format(feed_dict[x], feed_dict[y], output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Learning and Loss <a name=\"Learning and Loss\"></a>\n",
    "Like __MiniFlow__ in its current state, neural networks take inputs and produce outputs. But unlike __MiniFlow__ in its current state, neural networks can improve the accuracy of their outputs over time (it's hard to imagine improving the accuracy of __Add__ over time!). To explore why accuracy matters, I want you to first implement a trickier (and more useful!) node than __Add__: the __Linear__ node.\n",
    "\n",
    "#### The Linear Function\n",
    "\n",
    "<img src='Figures2/vlcsnap-2016-10-26-21h47m38s028.png' width = 400>\n",
    "\n",
    "A simple artificial neuron depends on three components:\n",
    "- inputs, x (vector)\n",
    "- weights, w (vector)\n",
    "- bias, b (scalar)\n",
    "\n",
    "The output, o, is just the weighted sum of the inputs plus the bias:\n",
    "\n",
    "$$\n",
    "o = \\sum_i x_i w_i +b\n",
    "$$\n",
    "\n",
    "Remember, by varying the weights, you can vary the amount of influence any given input has on the output. The learning aspect of neural networks takes place during a process known as backpropagation. In backpropogation, the network modifies the weights to improve the network's output accuracy. You'll be applying all of this shortly.\n",
    "\n",
    "In this next quiz, you'll try to build a linear neuron that generates an output by applying a simplified version of Equation. __Linear__ should take an list of inbound nodes of length n, a list of weights of length n, and a bias.\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "1. Open nn.py below. Read through the neural network to see the expected output of Linear.\n",
    "2. Open miniflow.py below. Modify Linear, which is a subclass of Node, to generate an output with the Equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## miniflow.py\n",
    "\"\"\"\n",
    "Write the Linear#forward method below!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        # Nodes from which this Node receives values\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # Nodes to which this Node passes values\n",
    "        self.outbound_nodes = []\n",
    "        # A calculated value\n",
    "        self.value = None\n",
    "        # Add this node as an outbound node on its inputs.\n",
    "        for n in self.inbound_nodes:\n",
    "            n.outbound_nodes.append(self)\n",
    "\n",
    "    # These will be implemented in a subclass.\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "\n",
    "        Compute the output value based on `inbound_nodes` and\n",
    "        store the result in self.value.\n",
    "        \"\"\"\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        # An Input Node has no inbound nodes,\n",
    "        # so no need to pass anything to the Node instantiator\n",
    "        Node.__init__(self)\n",
    "\n",
    "        # NOTE: Input Node is the only Node where the value\n",
    "        # may be passed as an argument to forward().\n",
    "        #\n",
    "        # All other Node implementations should get the value\n",
    "        # of the previous nodes from self.inbound_nodes\n",
    "        #\n",
    "        # Example:\n",
    "        # val0 = self.inbound_nodes[0].value\n",
    "    def forward(self, value=None):\n",
    "        # Overwrite the value if one is passed in.\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "\n",
    "\n",
    "class Linear(Node):\n",
    "    def __init__(self, inputs, weights, bias):\n",
    "        Node.__init__(self, [inputs, weights, bias])\n",
    "\n",
    "        # NOTE: The weights and bias properties here are not\n",
    "        # numbers, but rather references to other nodes.\n",
    "        # The weight and bias values are stored within the\n",
    "        # respective nodes.\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set self.value to the value of the linear function output.\n",
    "\n",
    "        Your code goes here!\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        weight_sum = []\n",
    "        print(self.inbound_nodes[0].value)\n",
    "        for x, w in zip(self.inbound_nodes[0].value, self.inbound_nodes[1].value):\n",
    "            weight_sum.append(w*x)\n",
    "        self.value = sum(weight_sum) +self.inbound_nodes[2].value\n",
    "        \"\"\"\n",
    "        \n",
    "        inputs = self.inbound_nodes[0].value\n",
    "        weights = self.inbound_nodes[1].value\n",
    "        bias = self.inbound_nodes[2].value\n",
    "        self.value = bias\n",
    "        for x, w in zip(inputs, weights):\n",
    "            self.value += x * w\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(output_node, sorted_nodes):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_node`: A node in the graph, should be the output node (have no outgoing edges).\n",
    "        `sorted_nodes`: A topologically sorted list of nodes.\n",
    "\n",
    "    Returns the output Node's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "\n",
    "    return output_node.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE: Here we're using an Input node for more than a scalar.\n",
    "In the case of weights and inputs the value of the Input node is\n",
    "actually a python list!\n",
    "\n",
    "In general, there's no restriction on the values that can be passed to an Input node.\n",
    "\"\"\"\n",
    "#from miniflow import *\n",
    "\n",
    "inputs, weights, bias = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(inputs, weights, bias)\n",
    "\n",
    "feed_dict = {\n",
    "    inputs: [6, 14, 3],\n",
    "    weights: [0.5, 0.25, 1.4],\n",
    "    bias: 2\n",
    "}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "print(output) # should be 12.7 with this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 4. Linear Transform <a name=\"Linear Transform\"></a>\n",
    "\n",
    "<img src=\"Figures2/screen-shot-2016-10-21-at-15.43.05.png\" width=200>\n",
    "\n",
    "Linear algebra nicely reflects the idea of transforming values between layers in a graph. In fact, the concept of a [transform](https://www.khanacademy.org/math/linear-algebra/matrix-transformations/linear-transformations/v/vector-transformations) does exactly what a layer should do - it converts inputs to outputs in many dimensions.\n",
    "\n",
    "Let's go back to our equation for the output.\n",
    "\n",
    "$$\n",
    "o = \\sum_i x_i w_i + b \\\\\n",
    "\\text{Equation(1)}\n",
    "$$\n",
    "\n",
    "For the rest of this section we'll denote x as X and w as W since they are now matrices, and b is now a vector instead of a scalar.\n",
    "\n",
    "Consider a __Linear__ node with 1 input and k outputs (mapping 1 input to k outputs). In this context an input/output is synonymous with a feature.\n",
    "\n",
    "In this case X is a 1 by 1 matrix.\n",
    "<img src=\"Figures2/newx.png\" width = 100>\n",
    "$$ \\text{1 by 1 matrix, 1 element.} $$\n",
    "\n",
    "W becomes a 1 by k matrix (looks like a row).\n",
    "\n",
    "<img src=\"Figures2/neww.png\" width = 280>\n",
    "$$ \\text{A 1 by k weights row matrix.} $$\n",
    "\n",
    "The result of the matrix multiplication of X and W is a 1 by k matrix. Since b is also a 1 by k row matrix (1 bias per output), b is added to the output of the X and W matrix multiplication.\n",
    "\n",
    "What if we are mapping n inputs to k outputs?\n",
    "\n",
    "Then X is now a 1 by n matrix and W is a n by k matrix. The result of the matrix multiplication is still a 1 by k matrix so the use of the biases remain the same.\n",
    "\n",
    "<img src=\"Figures2/newx-1n.png\" width = 280>\n",
    "$$ \\text{X is now a 1 by n matrix, n inputs/features.}$$\n",
    "\n",
    "<img src=\"Figures2/w-nk.png\" width = 300>\n",
    "$$ \\text{W is now a n by k matrix.} $$\n",
    "\n",
    "<img src=\"Figures2/b-1byk.png\" width = 250>\n",
    "$$ \\text{Row matrix of biases, one for each output.}$$\n",
    "\n",
    "Let's take a look at an example of n inputs. Consider an 28px by 28px greyscale image, as is in the case of images in the MNIST dataset. We can reshape the image such that it's a 1 by 784 matrix, n = 784. Each pixel is an input/feature. Here's an animated example emphasizing a pixel is a feature.\n",
    "\n",
    "In practice, it's common to feed in multiple data examples in each forward pass rather than just 1. The reasoning for this is the examples can be processed in parallel, resulting in big performance gains. The number of examples is called the batch size. Common numbers for the batch size are 32, 64, 128, 256, 512. Generally, it's the most we can comfortably fit in memory.\n",
    "\n",
    "What does this mean for X, W and b?\n",
    "\n",
    "X becomes a m by n matrix and W and b remain the same. The result of the matrix multiplication is now m by k, so the addition of b is [broadcast](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) over each row.\n",
    "\n",
    "<img src=\"Figures2/x-mn.png\" width = 250>\n",
    "$$ \\text{X is now an m by n matrix. Each row has n inputs/features.} $$\n",
    "\n",
    "In the context of MNIST each row of X is an image reshaped from 28 by 28 to 1 by 784.\n",
    "Equation (1) turns into:\n",
    "\n",
    "$$\n",
    "Z = XW + b \\\\\n",
    "\\text{Equation(2.)}\n",
    "$$\n",
    "\n",
    "Equation (2) can also be viewed as $Z = XW + B$ where B is the biases vector, b, stacked m times as a row. Due to broadcasting it's abbreviated to $Z = XW + b$.\n",
    "\n",
    "I want you to rebuild __Linear__ to handle matrices and vectors using the venerable Python math package __numpy__ to make your life easier. __numpy__ is often abbreviated as __np__, so we'll refer to it as __np__ when referring to code.\n",
    "\n",
    "I used __np.array__ ([documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html)) to create the matrices and vectors. You'll want to use __np.dot__, which functions as matrix multiplication for 2D arrays ([documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html), to multiply the input and weights matrices from Equation (2). It's also worth noting that numpy actually overloads the **\\__add\\__** operator so you can use it directly with __np.array__ (eg. __np.array() + np.array()__)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# miniflow.py\n",
    "\n",
    "\"\"\"\n",
    "Modify Linear#forward so that it linearly transforms\n",
    "input matrices, weights matrices and a bias vector to\n",
    "an output.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        self.value = None\n",
    "        self.outbound_nodes = []\n",
    "        for node in inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "\n",
    "    def forward():\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    \"\"\"\n",
    "    While it may be strange to consider an input a node when\n",
    "    an input is only an individual node in a node, for the sake\n",
    "    of simpler code we'll still use Node as the base class.\n",
    "\n",
    "    Think of Input as collating many individual input nodes into\n",
    "    a Node.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # An Input node has no inbound nodes,\n",
    "        # so no need to pass anything to the Node instantiator\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "\n",
    "class Linear(Node):\n",
    "    def __init__(self, X, W, b):\n",
    "        # Notice the ordering of the input nodes passed to the\n",
    "        # Node constructor.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set the value of this node to the linear transform output.\n",
    "\n",
    "        Your code goes here!\n",
    "        \"\"\"\n",
    "        X = self.inbound_nodes[0].value\n",
    "        W = self.inbound_nodes[1].value\n",
    "        b = self.inbound_nodes[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "        # self.value = np.dot(self.inbound_nodes[0].value, self.inbound_nodes[1].value) + self.inbound_nodes[2].value\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(output_node, sorted_nodes):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted Nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_node`: A Node in the graph, should be the output node (have no outgoing edges).\n",
    "        `sorted_nodes`: a topologically sorted list of nodes.\n",
    "\n",
    "    Returns the output node's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "\n",
    "    return output_node.value\n",
    "\n",
    "\"\"\"\n",
    "The setup is similar to the prevous `Linear` node you wrote\n",
    "except you're now using NumPy arrays instead of python lists.\n",
    "\n",
    "Update the Linear class in miniflow.py to work with\n",
    "numpy vectors (arrays) and matrices.\n",
    "\n",
    "Test your code here!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "#from miniflow import *\n",
    "\n",
    "X, W, b = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(X, W, b)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2., -3], [2., -3]])\n",
    "b_ = np.array([-3., -5])\n",
    "\n",
    "feed_dict = {X: X_, W: W_, b: b_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "\"\"\"\n",
    "Output should be:\n",
    "[[-9., 4.],\n",
    "[-9., 4.]]\n",
    "\"\"\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 6. Sigmoid Function <a name=\"sigmoid function\"></a>\n",
    "\n",
    "Neural networks take advantage of alternating transforms and activation functions to better categorize outputs. The sigmoid function is among the most common activation functions.\n",
    "\n",
    "$$\n",
    "sigmoid(x) = \\frac{1}{1+ e^{-x}} \\\\\n",
    "\\text{Equation(3)}\n",
    "$$\n",
    "\n",
    "<img src=\"Figures2/pasted-image-at-2016-10-25-01-17-pm.png\" width=400>\n",
    "$$\\text{Graph of the sigmoid function. Notice the \"S\" shape.}$$\n",
    "\n",
    "Linear transforms are great for simply shifting values, but neural networks often require a more nuanced transform. For instance, one of the original designs for an artificial neuron, [the perceptron](https://en.wikipedia.org/wiki/Perceptron), exhibit binary output behavior. Perceptrons compare a weighted input to a threshold. When the weighted input exceeds the threshold, the perceptron is activated and outputs __1__, otherwise it outputs __0__.\n",
    "\n",
    "You could model a perceptron's behavior as a step function.\n",
    "<img src=\"Figures2/save-2.png\", width=400>\n",
    "$$ \\text{Example of a step function (The jump between y = 0 and y = 1 should be instantaneous).} $$\n",
    "\n",
    "Activation, the idea of binary output behavior, generally makes sense for classification problems. For example, if you ask the network to hypothesize if a handwritten image is a '9', you're effectively asking for a binary output - yes, this is a '9', or no, this is not a '9'. A step function is the starkest form of a binary output, which is great, but step functions are not continuous and not differentiable, which is very bad. Differentiation is what makes gradient descent possible.\n",
    "\n",
    "The sigmoid function, Equation (3) above, replaces thresholding with a beautiful S-shaped curve (also shown above) that mimics the activation behavior of a perceptron while maintaining continuity, and thus differentiability. As a bonus, the sigmoid function has a very simple derivative that looks remarkably similar to the sigmoid itself.\n",
    "\n",
    "$$ \n",
    "\\sigma '(x) = \\sigma(x) \\cdot (1 - \\sigma(x)) \\\\\n",
    "\\text{Equation (4). } \\sigma \\text{ represents Equation (3)}\n",
    "$$\n",
    "\n",
    "Notice that the sigmoid function only has one parameter. Remember that sigmoid is an activation function (non-linearity), meaning it takes a single input and performs a mathematical operation on it.\n",
    "\n",
    "Conceptually, the sigmoid function makes decisions. When given weighted features from some data, it indicates whether or not the features contribute to a classification. In that way, a sigmoid activation works well following a linear transformation. As it stands right now with random weights and bias, the sigmoid node's output is also random. The process of learning through backpropagation and gradient descent, which you will implement soon, modifies the weights and bias such that activation of the sigmoid node begins to match expected outputs.\n",
    "\n",
    "Now that I've given you the equation for the sigmoid function, I want you to add it to the __MiniFlow__ library. To do so, you'll want to use __np.exp__ ([documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)) to make your life much easier.\n",
    "\n",
    "You'll be using __Sigmoid__ in conjunction with __Linear__. Here's how it should look:\n",
    "\n",
    "<img src=\"Figures2/screen-shot-2016-10-26-at-19.28.34.png\" width = 400>\n",
    "$$ \\text{Inputs > Linear Transform > Sigmoid} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fix the Sigmoid class so that it computes the sigmoid function\n",
    "on the forward pass!\n",
    "\n",
    "Scroll down to get started.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        self.value = None\n",
    "        self.outbound_nodes = []\n",
    "        for node in inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "\n",
    "    def forward():\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        # An Input node has no inbound nodes,\n",
    "        # so no need to pass anything to the Node instantiator\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "\n",
    "class Linear(Node):\n",
    "    def __init__(self, X, W, b):\n",
    "        # Notice the ordering of the input nodes passed to the\n",
    "        # Node constructor.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        X = self.inbound_nodes[0].value\n",
    "        W = self.inbound_nodes[1].value\n",
    "        b = self.inbound_nodes[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    You need to fix the `_sigmoid` and `forward` methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used later with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "\n",
    "        Return the result of the sigmoid function.\n",
    "\n",
    "        Your code here!\n",
    "        \"\"\"\n",
    "        return 1 / ( 1 + np.exp(-x) )\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set the value of this node to the result of the\n",
    "        sigmoid function, `_sigmoid`.\n",
    "\n",
    "        Your code here!\n",
    "        \"\"\"\n",
    "        # This is a dummy value to prevent numpy errors\n",
    "        # if you test without changing this method.\n",
    "        self.value = self._sigmoid(self.inbound_nodes[0].value)\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(output_node, sorted_nodes):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted Nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_node`: A Node in the graph, should be the output node (have no outgoing edges).\n",
    "        `sorted_nodes`: a topologically sorted list of nodes.\n",
    "\n",
    "    Returns the output node's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "\n",
    "    return output_node.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This network feeds the output of a linear transform\n",
    "to the sigmoid function.\n",
    "\n",
    "Finish implementing the Sigmoid class in miniflow.py!\n",
    "\n",
    "Feel free to play around with this network, too!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "#from miniflow import *\n",
    "\n",
    "X, W, b = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(X, W, b)\n",
    "g = Sigmoid(f)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2., -3], [2., -3]])\n",
    "b_ = np.array([-3., -5])\n",
    "\n",
    "feed_dict = {X: X_, W: W_, b: b_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(g, graph)\n",
    "\n",
    "\"\"\"\n",
    "Output should be:\n",
    "[[  1.23394576e-04   9.82013790e-01]\n",
    " [  1.23394576e-04   9.82013790e-01]]\n",
    "\"\"\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Answer__:\n",
    "\n",
    "```Python\n",
    "class Sigmoid(Node):\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x)) # the `.` ensures that `1` is a float\n",
    "\n",
    "    def forward(self):\n",
    "        input_value = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "```\n",
    "\n",
    "It may have seemed strange that **\\_sigmoid** was a separate method. As seen in the derivative of the sigmoid function, Equation (4), the sigmoid function is actually a part of its own derivative. Keeping **\\_sigmoid** separate means you won't have to implement it twice for forward and backward propagations.\n",
    "\n",
    "This is exciting! At this point, you have used weights and biases to compute outputs. And you've used an activation function to categorize the output. As you may recall, neural networks improve the __accuracy__ of their outputs by modifying weights and biases in response to training against labeled datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 7. Cost <a name=\"cost\"></a>\n",
    "\n",
    "There are many techniques for defining the accuracy of a neural network, all of which center on the network's ability to produce values that come as close as possible to known correct values. People use different names for this accuracy measurement, often terming it __loss__ or __cost__. I'll use the term cost most often.\n",
    "\n",
    "For this lab, you will calculate the cost using the mean squared error (MSE). It looks like so:\n",
    "\n",
    "$$\n",
    "C(w, b) = \\frac{1}{m} \\sum_x (y(x) - a)^2 \\\\\n",
    "\\text{Equation (5)}\n",
    "$$\n",
    "\n",
    "Here $w$ denotes the collection of all weights in the network, b all the biases, m is the total number of training examples, a is the approximation of y(x) by the network, both a and y(x) are vectors of the same length.\n",
    "\n",
    "The collection of weights is all the weight matrices flattened into vectors and concatenated to one big vector. The same goes for the collection of biases except they're already vectors so there's no need to flatten them prior to the concatenation.\n",
    "\n",
    "Here's an example of creating w in code:\n",
    "\n",
    "```Python\n",
    "# 2 by 2 matrices\n",
    "w1  = np.array([[1, 2], [3, 4]])\n",
    "w2  = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "# flatten\n",
    "w1_flat = np.reshape(w1, -1)\n",
    "w2_flat = np.reshape(w2, -1)\n",
    "\n",
    "w = np.concatenate((w1_flat, w2_flat))\n",
    "# array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "```\n",
    "\n",
    "It's a nice way to abstract all the weights and biases used in the neural network and makes some things easier to write as we'll see soon in the upcoming gradient descent sections.\n",
    "\n",
    "__NOTE__: It's not required you do this in your code! It's just easier to do this talk about the weights and biases as a collective than consider them invidually.\n",
    "\n",
    "The cost, C, depends on the difference between the correct output, y(x), and the network's output, a. It's easy to see that no difference between y(x) and a (for all values of x) leads to a cost of 0.\n",
    "\n",
    "This is the ideal situation, and in fact the learning process revolves around minimizing the cost as much as possible.\n",
    "\n",
    "I want you to calculate the cost now.\n",
    "\n",
    "You implemented this network in the forward direction in the last quiz.\n",
    "\n",
    "As it stands right now, it outputs gibberish. The activation of the sigmoid node means nothing because the network has no labeled output against which to compare. Furthermore, the weights and bias cannot change and learning cannot happen without a cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    \"\"\"\n",
    "    Base class for nodes in the network.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `inbound_nodes`: A list of nodes with edges into this node.\n",
    "    \"\"\"\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        \"\"\"\n",
    "        Node's constructor (runs when the object is instantiated). Sets\n",
    "        properties that all nodes need.\n",
    "        \"\"\"\n",
    "        # A list of nodes with edges into this node.\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # The eventual value of this node. Set by running\n",
    "        # the forward() method.\n",
    "        self.value = None\n",
    "        # A list of nodes that this node outputs to.\n",
    "        self.outbound_nodes = []\n",
    "        # Sets this node as an outbound node for all of\n",
    "        # this node's inputs.\n",
    "        for node in inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `forward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    \"\"\"\n",
    "    A generic input into the network.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # The base class constructor has to run to set all\n",
    "        # the properties here.\n",
    "        #\n",
    "        # The most important property on an Input is value.\n",
    "        # self.value is set during `topological_sort` later.\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "\n",
    "class Linear(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs a linear transform.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, W, b):\n",
    "        # The base class (Node) constructor. Weights and bias\n",
    "        # are treated like inbound nodes.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Performs the math behind a linear transform.\n",
    "        \"\"\"\n",
    "        X = self.inbound_nodes[0].value\n",
    "        W = self.inbound_nodes[1].value\n",
    "        b = self.inbound_nodes[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        # The base class constructor.\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform the sigmoid function and set the value.\n",
    "        \"\"\"\n",
    "        input_value = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "\n",
    "\n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last node for a network.\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "        # TODO: your code here\n",
    "        \n",
    "        self.value = (np.sum( (y - a)**2 )) / len(y)\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_pass(graph):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted Nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test your MSE method with this script!\n",
    "\n",
    "No changes necessary, but feel free to play\n",
    "with this script to test your network.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "# from miniflow import *\n",
    "\n",
    "y, a = Input(), Input()\n",
    "cost = MSE(y, a)\n",
    "\n",
    "y_ = np.array([1, 2, 3])\n",
    "a_ = np.array([4.5, 5, 10])\n",
    "\n",
    "feed_dict = {y: y_, a: a_}\n",
    "graph = topological_sort(feed_dict)\n",
    "# forward pass\n",
    "forward_pass(graph)\n",
    "\n",
    "\"\"\"\n",
    "Expected output\n",
    "\n",
    "23.4166666667\n",
    "\"\"\"\n",
    "print(cost.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__: \n",
    "\n",
    "```Python\n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last node for a network.\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "        m = self.inbound_nodes[0].value.shape[0]\n",
    "\n",
    "        diff = y - a\n",
    "        self.value = np.mean(diff**2)\n",
    "        # or\n",
    "        # self.value = np.square(diff)\n",
    "        # or\n",
    "        # self.value = np.mean(np.square(y-a))\n",
    "```\n",
    "\n",
    "The math behind __MSE__ reflects Equation (5), where y is target output and a is output computed by the neural network. We then square the difference __diff\\**2__, alternatively, this could be __np.square(diff)__. Lastly we need to sum the squared differences and divide by the total number of examples m. This can be achieved in with __np.mean__ or __(1 /m) \\* np.sum(diff\\**2)__.\n",
    "\n",
    "Note the order of __y__ and __a__ doesn't actually matter, we could switch them around __(a - y)__ and get the same value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Gradient Descent <a name='Gradient Descent'></a>\n",
    "\n",
    "Great! We've successfully calculated a full forward pass and found the cost. Next we need to start a backwards pass, which starts with backpropagation. Backpropagation is the process by which the network runs error values backwards.\n",
    "\n",
    "During this process, the network calculates the way in which the weights need to change (also called the gradient) to reduce the overall error of the network. Changing the weights usually occurs through a technique called gradient descent.\n",
    "\n",
    "<img src=\"Figures2/pasted-image-at-2016-10-25-01-24-pm.png\" width=400>\n",
    "$$ \\text{A point on a three diemnsion surface} $$\n",
    "\n",
    "Imagine a point on a three dimensional surface. In real-life, a ball sitting on the slope of a valley makes a nice analogy. In this case, the height of the point represents the difference between the current output of the network and the correct output given the current parameter values (hence why you need data with known outputs). Each dimension of the plane represents another parameter to the network. A network with $m$ parameters would be a hyperplane of $m$ dimensions.\n",
    "\n",
    "(Imagining more than three dimensions is tricky. The good news is that the ball and valley example describes the behavior of gradient descent well, the only difference between three dimensional and n dimensional situations being the number of parameters in the calculations.)\n",
    "\n",
    "In the ideal situation, the ball rests at the bottom of the valley, indicating the minimum difference between the output of the network and the known correct output.\n",
    "\n",
    "The learning process starts with random weights and biases. In the ball analogy, the ball starts at a random point near the valley.\n",
    "\n",
    "Gradient descent works by first calculating the slope of the plane at the current point, which includes calculating the partial derivatives of the loss with respect to all of the parameters. This set of partial derivatives is called the __gradient__. Then it uses the gradient to modify the weights such that the next forward pass through the network moves the output lower in the hyperplane. Physically, this would be the same as measuring the slope of the valley at the location of the ball, and then moving the ball a small amount in the direction of the slope. Over time, it's possible to find the bottom of the valley with many small movements.\n",
    "\n",
    "<img src=\"Figures2/pasted-image-at-2016-10-25-01-24-pm2.png\" width=400>\n",
    "\n",
    "While gradient descent works remarkably well, the technique isn't guaranteed to find the absolute minimum difference between the network's output and the known output. \n",
    "\n",
    "- There may be more than one absolute minima. If there are two absolute minima the function is not convex so we could end up in a local minima.\n",
    "- It may get studck in a local minimum while the absolute minimum may be \"over the next hill\". Gradient descent does not know the difference between a local or global minima.\n",
    "- The learning rate (determines how much the point moves) will cause the point to overshoot the absolute minimum. If the learning rate is too large the neural network might overshoot the minima and diverge!\n",
    "\n",
    "__How do we know which way is downhill?__ Technically, the gradient actually points uphill, in the direction of __steepest ascent__. But if we put a $-$ sign at the front this value, we get the direction of __steepest descent__, which is what we want.\n",
    "\n",
    "Just think of the gradient as a vector of numbers. Each number represents the amount by which we should adjust a corresponding weight or bias in the neural network. Adjusting all of the weights and biases by the gradient values reduces the cost (or error) of the network.\n",
    "\n",
    "The next thing to consider is __how much force should be applied to the push__. This is known as the _learning rate_, which is an apt name since this value determines how quickly or slowly the neural network learns.\n",
    "\n",
    "You might be tempted to set a really big learning rate, so the network learns really fast, right?\n",
    "\n",
    "Be careful! If the value is too large you could overshoot the target and eventually diverge. Yikes!\n",
    "\n",
    "<img src=\"Figures2/gradient-descent-convergence.gif\" width=400>\n",
    "$$ \\textbf{Convergence. } \\text{This is the ideal behaviour.}$$\n",
    "\n",
    "<img src=\"Figures2/gradient-descent-divergence.gif\" width=400>\n",
    "$$ \\textbf{Divergence. } \\text{This can happen when the learning rate is too large.}$$\n",
    "\n",
    "This is more of a guessing game than anything else but empirically values in the range 0.1 to 0.0001 work well. The range 0.001 to 0.0001 is popular, as 0.1 and 0.01 are sometimes too large.\n",
    "\n",
    "Here's the formula for gradient descent (pseudocode):\n",
    "\n",
    "```Python\n",
    "x = x - learning_rate * gradient_of_x\n",
    "```\n",
    "\n",
    "__x__ is a parameter used by the neural network (i.e. a single weight or bias).\n",
    "\n",
    "We multiply __gradient_of_x__ (the uphill direction) by __learning_rate__ (the force of the push) and then subtract that from __x__ to make the push go downhill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent_update(x, gradx, learning_rate):\n",
    "    \"\"\"\n",
    "    Performs a gradient descent update.\n",
    "    \"\"\"\n",
    "    # TODO: Implement gradient descent.\n",
    "    \n",
    "    # Return the new value for x\n",
    "    x -= learning_rate * gradx\n",
    "    return x\n",
    "\n",
    "\"\"\"\n",
    "Given the starting point of any `x` gradient descent\n",
    "should be able to find the minimum value of x for the\n",
    "cost function `f` defined below.\n",
    "\"\"\"\n",
    "import random\n",
    "#from gd import gradient_descent_update\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Quadratic function.\n",
    "\n",
    "    It's easy to see the minimum value of the function\n",
    "    is 5 when is x=0.\n",
    "    \"\"\"\n",
    "    return x**2 + 5\n",
    "\n",
    "\n",
    "def df(x):\n",
    "    \"\"\"\n",
    "    Derivative of `f` with respect to `x`.\n",
    "    \"\"\"\n",
    "    return 2*x\n",
    "\n",
    "\n",
    "# Random number better 0 and 10,000. Feel free to set x whatever you like.\n",
    "x = random.randint(0, 10000)\n",
    "# TODO: Set the learning rate\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "\n",
    "for i in range(epochs+1):\n",
    "    cost = f(x)\n",
    "    gradx = df(x)\n",
    "    print(\"EPOCH {}: Cost = {:.3f}, x = {:.3f}\".format(i, cost, gradx))\n",
    "    x = gradient_descent_update(x, gradx, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__:\n",
    "\n",
    "```Python\n",
    "def gradient_descent_update(x, gradx, learning_rate):\n",
    "    \"\"\"\n",
    "    Performs a gradient descent update.\n",
    "    \"\"\"\n",
    "    x = x - learning_rate * gradx\n",
    "    # Return the new value for x\n",
    "    return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Backpropagation <a name='Backpropagation'></a>\n",
    "\n",
    "```\n",
    "In order to figure out how we should alter a parameter to minimize the cost, we must first find out what effect that parameter has on the cost.\n",
    "```\n",
    "\n",
    "That makes sense. After all, we can't just blindly change parameter values and hope to get meaningful results. The gradient takes into account the effect each parameter has on the cost, so that's how we find the direction of steepest ascent.\n",
    "\n",
    "How do we determine the effect a parameter has on the cost? This technique is famously known as __backpropagation__ or __reverse-mode differentiation__. Those names might sound intimidating, but behind it all, it's just a clever application of the __chain rule__. Before we get into the chain rule let's revisit plain old derivatives.\n",
    "\n",
    "### 9.1. Derivatives <a name='Derivatives'></a>\n",
    "\n",
    "```\n",
    "In calculus, the derivative tells us how something changes with respect to something else. Or, put differently, how sensitive something is to something else.\n",
    "```\n",
    "\n",
    "Let's take the function $f(x)=x^2$ as an example. In this case, the derivative of $f(x)$ is $2x$. Another way to state this is, \"the derivative of $f(x)$ with respect to $x$ is $2x$\".\n",
    "\n",
    "Using the derivative, we can say how much a change in $x$ effects $f(x)$. For example, when $x$ is $4$, the derivative is $8$, $(2x=2∗4=8)$. This means that if $x$ is increased or decreased by 1 unit, then $f(x)$ will increase or decrease by $8$.\n",
    "\n",
    "<img src=\"Figures2/23.png\" width=500>\n",
    "$$ f(x) \\text{ and the tangent line of } f(x) \\text{ when } x=4. $$\n",
    "\n",
    "Notice that $f(4)=16$ and $f(5)=25$. $25 - 16 = 9$, which isn't the same as $8$.\n",
    "\n",
    "But we just calculated that increasing $x$ by 1 unit would change $f(x)$ by 8. What happened?\n",
    "\n",
    "The answer is that the slope (or derivative) itself changes as $x$ changes. If we calculate the derivative when $x$ is 4.5, it's now 9, which matches the difference between $f(4)=16$ and $f(5)=25$.\n",
    "\n",
    "### 9.2. Chain Rule <a name='Chain Rule'></a>\n",
    "\n",
    "Let's return to neural networks and the original goal of figuring out what effect a parameter has on the cost.\n",
    "\n",
    "We simply calculate the derivative of the cost with respect to each parameter in the network. The gradient is a vector of all these derivatives.\n",
    "\n",
    "In reality, neural networks are a composition of functions, so computing the derivative of the cost w.r.t a parameter isn't quite as straightforward as calculating the derivative of a polynomial function like $f(x)=x^2$. This is where the chain rule comes into play.\n",
    "\n",
    "Say we have a new function $f \\circ g(x)=f(g(x))$. We can calculate the derivative of $f \\circ g$ w.r.t $x$ , denoted $\\frac{\\partial f \\circ g}{\\partial x}$, by applying the chain rule.\n",
    "$$\n",
    "\\frac{\\partial f \\circ g}{\\partial x} = \\frac{\\partial g}{\\partial x} \\frac{\\partial f}{\\partial g}\n",
    "$$\n",
    "The way to think about this is:\n",
    "```\n",
    "In order to know the effect x has on f, we first need to know the effect x has on g, and then the effect g has on f.\n",
    "```\n",
    "\n",
    "Let's now look at a more complex example. Consider the following neural network in MiniFlow:\n",
    "\n",
    "```Python\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(l2, y)\n",
    "```\n",
    "\n",
    "This also can be written as a composition of functions __MSE(Linear(Sigmoid(Linear(X, W1, b1)), W2, b2), y)__. Our goal is to adjust the weights and biases represented by the Input nodes __W1, b1, W2, b2__, such that the cost is minimized.\n",
    "\n",
    "<img src=\"Figures2/miniflow-nn-graph.001.jpg\" width=600>\n",
    "$$ \\text{Graph of the above neural network. The backward pass and gradients flowing through are illustrated.} $$\n",
    "\n",
    "In the upcoming quiz you'll implement the __backward__ method for the __Sigmoid__ node, so let's focus on that.\n",
    "\n",
    "First, we unwrap the derivative of __cost__ w.r.t. __l1__ (the input to the __Sigmoid__ node). Once again, apply the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial cost}{\\partial l_1} = \\frac{\\partial s_1}{\\partial l_1} \\frac{\\partial cost}{\\partial s_1}\n",
    "$$\n",
    "\n",
    "We can unwrap $\\frac{\\partial cost}{\\partial s_1}$ further:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial cost}{\\partial s_1} = \\frac{\\partial l_2}{\\partial s_1} \\frac{\\partial cost}{\\partial l_2}\n",
    "$$\n",
    "\n",
    "In order to calculate the derivative of __cost__ w.r.t __l1__ we need to figure out these 3 values:\n",
    "\n",
    "- $\\frac{\\partial s_1}{\\partial l_1}$\n",
    "- $\\frac{\\partial l_2}{\\partial s_1}$\n",
    "- $\\frac{\\partial cost}{\\partial l_2}$\n",
    "\n",
    "Backpropagation makes computing these values convenient.\n",
    "\n",
    "During backpropagation, the derivatives of nodes in the graph are computed back to front. In the case of the 3 above values, $\\frac{\\partial cost}{\\partial l_2}$ would be computed first, followed by $\\frac{\\partial l_2}{\\partial s_1}$ and $\\frac{\\partial s_1}{\\partial l_1}$.\n",
    "Thus, if we compute $\\frac{\\partial s_1}{\\partial l_1}$, then we can also assume the 2 other values have already been computed!\n",
    "\n",
    "This insight makes backpropagation much easier to implement. When computing the backward pass for a Node we only need to concern ourselves with the computation of that node w.r.t its inputs.\n",
    "\n",
    "Additional Resources\n",
    "- [Yes you should understand backprop](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.39yeitvwz) by Andrej Karpathy\n",
    "- [Vector, Matrix, and Tensor Derivatives](http://cs231n.stanford.edu/vecDerivs.pdf) by Erik Learned-Miller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3. New Code <a name='New Code'></a>\n",
    "\n",
    "There have been a couple of changes to MiniFlow since we last took it for a spin:\n",
    "\n",
    "The first being the Node class now has a backward method, as well as a new attribute self.gradients, which is used to store and cache gradients during the backward pass.\n",
    "\n",
    "```Python\n",
    "class Node(object):\n",
    "    \"\"\"\n",
    "    Base class for nodes in the network.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `inbound_nodes`: A list of nodes with edges into this node.\n",
    "    \"\"\"\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        \"\"\"\n",
    "        Node's constructor (runs when the object is instantiated). Sets\n",
    "        properties that all nodes need.\n",
    "        \"\"\"\n",
    "        # A list of nodes with edges into this node.\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # The eventual value of this node. Set by running\n",
    "        # the forward() method.\n",
    "        self.value = None\n",
    "        # A list of nodes that this node outputs to.\n",
    "        self.outbound_nodes = []\n",
    "        # New property! Keys are the inputs to this node and\n",
    "        # their values are the partials of this node with\n",
    "        # respect to that input.\n",
    "        self.gradients = {}\n",
    "        # Sets this node as an outbound node for all of\n",
    "        # this node's inputs.\n",
    "        for node in inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `forward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `backward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "```\n",
    "\n",
    "The second change is to the helper function __forward\\_pass()__. That function has been replaced with __forward\\_and\\_backward()__.\n",
    "\n",
    "```Python\n",
    "def forward_and_backward(graph):\n",
    "    \"\"\"\n",
    "    Performs a forward pass and a backward pass through a list of sorted nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "\n",
    "    # Backward pass\n",
    "    # see: https://docs.python.org/2.3/whatsnew/section-slices.html\n",
    "    for n in graph[::-1]:\n",
    "        n.backward()\n",
    "```\n",
    "\n",
    "Here's the derivative of the $sigmoid$ function w.r.t $x$:\n",
    "$$\n",
    "sigmoid(x) = \\frac{1}{1+e^{-x}} \\\\\n",
    "\\frac{\\partial sigmoid}{\\partial x} = sigmoid(x) * \\big(1 - sigmoid(x)\\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# miniflow.py\n",
    "\"\"\"\n",
    "Implement the backward method of the Sigmoid node.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    \"\"\"\n",
    "    Base class for nodes in the network.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `inbound_nodes`: A list of nodes with edges into this node.\n",
    "    \"\"\"\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        \"\"\"\n",
    "        Node's constructor (runs when the object is instantiated). Sets\n",
    "        properties that all nodes need.\n",
    "        \"\"\"\n",
    "        # A list of nodes with edges into this node.\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # The eventual value of this node. Set by running\n",
    "        # the forward() method.\n",
    "        self.value = None\n",
    "        # A list of nodes that this node outputs to.\n",
    "        self.outbound_nodes = []\n",
    "        # New property! Keys are the inputs to this node and\n",
    "        # their values are the partials of this node with\n",
    "        # respect to that input.\n",
    "        self.gradients = {}\n",
    "        # Sets this node as an outbound node for all of\n",
    "        # this node's inputs.\n",
    "        for node in inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `forward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `backward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    \"\"\"\n",
    "    A generic input into the network.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # The base class constructor has to run to set all\n",
    "        # the properties here.\n",
    "        #\n",
    "        # The most important property on an Input is value.\n",
    "        # self.value is set during `topological_sort` later.\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        # An Input node has no inputs so the gradient (derivative)\n",
    "        # is zero.\n",
    "        # The key, `self`, is reference to this object.\n",
    "        self.gradients = {self: 0}\n",
    "        # Weights and bias may be inputs, so you need to sum\n",
    "        # the gradient from output gradients.\n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            self.gradients[self] += grad_cost * 1\n",
    "\n",
    "\n",
    "class Linear(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs a linear transform.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, W, b):\n",
    "        # The base class (Node) constructor. Weights and bias\n",
    "        # are treated like inbound nodes.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Performs the math behind a linear transform.\n",
    "        \"\"\"\n",
    "        X = self.inbound_nodes[0].value\n",
    "        W = self.inbound_nodes[1].value\n",
    "        b = self.inbound_nodes[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient based on the output values.\n",
    "        \"\"\"\n",
    "        # Initialize a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost with respect to this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            # Set the partial of the loss with respect to this node's inputs.\n",
    "            self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, self.inbound_nodes[1].value.T)\n",
    "            # Set the partial of the loss with respect to this node's weights.\n",
    "            self.gradients[self.inbound_nodes[1]] += np.dot(self.inbound_nodes[0].value.T, grad_cost)\n",
    "            # Set the partial of the loss with respect to this node's bias.\n",
    "            self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        # The base class constructor.\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform the sigmoid function and set the value.\n",
    "        \"\"\"\n",
    "        input_value = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient using the derivative of\n",
    "        the sigmoid function.\n",
    "        \"\"\"\n",
    "        # Initialize the gradients to 0.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost with respect to this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            \"\"\"\n",
    "            TODO: Your code goes here!\n",
    "\n",
    "            Set the gradients property to the gradients with respect to each input.\n",
    "\n",
    "            NOTE: See the Linear node and MSE node for examples.\n",
    "            \"\"\"\n",
    "            sigmoid = self.value\n",
    "            self.gradients[self.inbound_nodes[0]] = grad_cost * sigmoid * (1 - sigmoid)\n",
    "\n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last node for a network.\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "\n",
    "        self.m = self.inbound_nodes[0].value.shape[0]\n",
    "        # Save the computed output for backward.\n",
    "        self.diff = y - a\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the cost.\n",
    "\n",
    "        This is the final node of the network so outbound nodes\n",
    "        are not a concern.\n",
    "        \"\"\"\n",
    "        self.gradients[self.inbound_nodes[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inbound_nodes[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_and_backward(graph):\n",
    "    \"\"\"\n",
    "    Performs a forward pass and a backward pass through a list of sorted Nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "\n",
    "    # Backward pass\n",
    "    # see: https://docs.python.org/2.3/whatsnew/section-slices.html\n",
    "    for n in graph[::-1]:\n",
    "        n.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nn.py\n",
    "\n",
    "\"\"\"\n",
    "Test your network here!\n",
    "\n",
    "No need to change this code, but feel free to tweak it\n",
    "to test your network!\n",
    "\n",
    "Make your changes to backward method of the Sigmoid class in miniflow.py\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "#from miniflow import *\n",
    "\n",
    "X, W, b = Input(), Input(), Input()\n",
    "y = Input()\n",
    "f = Linear(X, W, b)\n",
    "a = Sigmoid(f)\n",
    "cost = MSE(y, a)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2.], [3.]])\n",
    "b_ = np.array([-3.])\n",
    "y_ = np.array([1, 2])\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W: W_,\n",
    "    b: b_,\n",
    "}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "forward_and_backward(graph)\n",
    "# return the gradients for each Input\n",
    "gradients = [t.gradients[t] for t in [X, y, W, b]]\n",
    "\n",
    "\"\"\"\n",
    "Expected output\n",
    "\n",
    "[array([[ -3.34017280e-05,  -5.01025919e-05],\n",
    "       [ -6.68040138e-05,  -1.00206021e-04]]), array([[ 0.9999833],\n",
    "       [ 1.9999833]]), array([[  5.01028709e-05],\n",
    "       [  1.00205742e-04]]), array([ -5.01028709e-05])]\n",
    "\"\"\"\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__:\n",
    "```Python\n",
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        # The base class constructor.\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform the sigmoid function and set the value.\n",
    "        \"\"\"\n",
    "        input_value = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient using the derivative of\n",
    "        the sigmoid function.\n",
    "        \"\"\"\n",
    "        # Initialize the gradients to 0.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Sum the derivative with respect to the input over all the outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            sigmoid = self.value\n",
    "            self.gradients[self.inbound_nodes[0]] += sigmoid * (1 - sigmoid) * grad_cost\n",
    "```\n",
    "\n",
    "The backward method sums the derivative (it's a normal derivative when there's only one variable) with respect to the only input over all the output nodes. The last line implements the derivative, $\\frac{\\partial sigmoid}{\\partial x} \\frac{\\partial cost}{\\partial sigmoid}$.\n",
    "\n",
    "Replacing the math expression with code:\n",
    "\n",
    "$\\frac{\\partial sigmoid}{\\partial x}$ is __sigmoid \\* (1 - sigmoid)__ and $\\frac{\\partial cost}{\\partial sigmoid}$ is __grad_cost__.\n",
    "\n",
    "Now that you have the gradient of the cost with respect to each input (the return value from __forward_and_backward()__) your network can start learning! To do so, you will implement a technique called __Stochastic Gradient Descent__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Stochastic Gradient Descent <a name='Stochastic Gradient Descent'></a>\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is a version of Gradient Descent where on each forward pass a batch of data is randomly sampled from total dataset. Remember when we talked about the batch size earlier? That's the size of the batch. Ideally, the entire dataset would be fed into the neural network on each forward pass, but in practice, it's not practical due to memory constraints. SGD is an approximation of Gradient Descent, the more batches processed by the neural network, the better the approximation.\n",
    "\n",
    "A naïve implementation of SGD involves:\n",
    "\n",
    "1. Randomly sample a batch of data from the total dataset.\n",
    "2. Running the network forward and backward to calculate the gradient (with data from (1)).\n",
    "3. Apply the gradient descent update.\n",
    "4. Repeat steps 1-3 until convergence or the loop is stopped by another mechanism (i.e. the number of epochs).\n",
    "\n",
    "If all goes well, the network's loss should generally trend downwards, indicating more useful weights and biases over time.\n",
    "\n",
    "So far, MiniFlow can already do step 2. In the following quiz, steps 1 and 4 are already implemented. It will be your job to implement step 3.\n",
    "\n",
    "As a reminder, here's the gradient descent update equation, where $\\alpha$ represents the learning rate:\n",
    "\n",
    "$$\n",
    "x = x - \\alpha * \\frac{\\partial cost}{\\partial x}\n",
    "$$\n",
    "\n",
    "We're also going to use an actual dataset for this quiz, the [Boston Housing dataset](https://archive.ics.uci.edu/ml/datasets/Housing). After training the network will be able to predict prices of Boston housing! \n",
    "```\n",
    "By Robbie Shade (Flickr: Boston's Back Bay) [CC BY 2.0 (http://creativecommons.org/licenses/by/2.0)], via Wikimedia Commons.\n",
    "```\n",
    "\n",
    "Each example in the dataset is a description of a house in the Boston suburbs, the description consists of 13 numerical values (features). Each example also has an associated price. With SGD, we're going to minimize the MSE between the actual price and the price predicted by the neural network based on the features.\n",
    "\n",
    "If all goes well the output should look something like this:\n",
    "\n",
    "When the batch size is 11:\n",
    "```Python\n",
    "Total number of examples = 506\n",
    "Epoch: 1, Loss: 140.256\n",
    "Epoch: 2, Loss: 34.570\n",
    "Epoch: 3, Loss: 27.501\n",
    "Epoch: 4, Loss: 25.343\n",
    "Epoch: 5, Loss: 20.421\n",
    "Epoch: 6, Loss: 17.600\n",
    "Epoch: 7, Loss: 18.176\n",
    "Epoch: 8, Loss: 16.812\n",
    "Epoch: 9, Loss: 15.531\n",
    "Epoch: 10, Loss: 16.429\n",
    "```\n",
    "\n",
    "When the batch size is the same as the total number of examples (batch is the whole dataset):\n",
    "```Python\n",
    "Total number of examples = 506\n",
    "Epoch: 1, Loss: 646.134\n",
    "Epoch: 2, Loss: 587.867\n",
    "Epoch: 3, Loss: 510.707\n",
    "Epoch: 4, Loss: 446.558\n",
    "Epoch: 5, Loss: 407.695\n",
    "Epoch: 6, Loss: 324.440\n",
    "Epoch: 7, Loss: 295.542\n",
    "Epoch: 8, Loss: 251.599\n",
    "Epoch: 9, Loss: 219.888\n",
    "Epoch: 10, Loss: 216.155\n",
    "```\n",
    "\n",
    "Notice the cost or loss trending towards 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# miniflow.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    \"\"\"\n",
    "    Base class for nodes in the network.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `inbound_nodes`: A list of nodes with edges into this node.\n",
    "    \"\"\"\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        \"\"\"\n",
    "        Node's constructor (runs when the object is instantiated). Sets\n",
    "        properties that all nodes need.\n",
    "        \"\"\"\n",
    "        # A list of nodes with edges into this node.\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # The eventual value of this node. Set by running\n",
    "        # the forward() method.\n",
    "        self.value = None\n",
    "        # A list of nodes that this node outputs to.\n",
    "        self.outbound_nodes = []\n",
    "        # New property! Keys are the inputs to this node and\n",
    "        # their values are the partials of this node with\n",
    "        # respect to that input.\n",
    "        self.gradients = {}\n",
    "        # Sets this node as an outbound node for all of\n",
    "        # this node's inputs.\n",
    "        for node in inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `forward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `backward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    \"\"\"\n",
    "    A generic input into the network.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # The base class constructor has to run to set all\n",
    "        # the properties here.\n",
    "        #\n",
    "        # The most important property on an Input is value.\n",
    "        # self.value is set during `topological_sort` later.\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        # An Input node has no inputs so the gradient (derivative)\n",
    "        # is zero.\n",
    "        # The key, `self`, is reference to this object.\n",
    "        self.gradients = {self: 0}\n",
    "        # Weights and bias may be inputs, so you need to sum\n",
    "        # the gradient from output gradients.\n",
    "        for n in self.outbound_nodes:\n",
    "            self.gradients[self] += n.gradients[self]\n",
    "\n",
    "class Linear(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs a linear transform.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, W, b):\n",
    "        # The base class (Node) constructor. Weights and bias\n",
    "        # are treated like inbound nodes.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Performs the math behind a linear transform.\n",
    "        \"\"\"\n",
    "        X = self.inbound_nodes[0].value\n",
    "        W = self.inbound_nodes[1].value\n",
    "        b = self.inbound_nodes[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient based on the output values.\n",
    "        \"\"\"\n",
    "        # Initialize a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost with respect to this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            # Set the partial of the loss with respect to this node's inputs.\n",
    "            self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, self.inbound_nodes[1].value.T)\n",
    "            # Set the partial of the loss with respect to this node's weights.\n",
    "            self.gradients[self.inbound_nodes[1]] += np.dot(self.inbound_nodes[0].value.T, grad_cost)\n",
    "            # Set the partial of the loss with respect to this node's bias.\n",
    "            self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        # The base class constructor.\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform the sigmoid function and set the value.\n",
    "        \"\"\"\n",
    "        input_value = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient using the derivative of\n",
    "        the sigmoid function.\n",
    "        \"\"\"\n",
    "        # Initialize the gradients to 0.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Sum the partial with respect to the input over all the outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            sigmoid = self.value\n",
    "            self.gradients[self.inbound_nodes[0]] += sigmoid * (1 - sigmoid) * grad_cost\n",
    "\n",
    "\n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last node for a network.\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "\n",
    "        self.m = self.inbound_nodes[0].value.shape[0]\n",
    "        # Save the computed output for backward.\n",
    "        self.diff = y - a\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the cost.\n",
    "        \"\"\"\n",
    "        self.gradients[self.inbound_nodes[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inbound_nodes[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_and_backward(graph):\n",
    "    \"\"\"\n",
    "    Performs a forward pass and a backward pass through a list of sorted Nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "\n",
    "    # Backward pass\n",
    "    # see: https://docs.python.org/2.3/whatsnew/section-slices.html\n",
    "    for n in graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "\n",
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    \"\"\"\n",
    "    Updates the value of each trainable with SGD.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `trainables`: A list of `Input` Nodes representing weights/biases.\n",
    "        `learning_rate`: The learning rate.\n",
    "    \"\"\"\n",
    "    # TODO: update all the `trainables` with SGD\n",
    "    # You can access and assign the value of a trainable with `value` attribute.\n",
    "    # Example:\n",
    "    # for t in trainables:\n",
    "    #   t.value = your implementation here\n",
    "\n",
    "    for t in trainables:\n",
    "        t.value = t.value - learning_rate * t.gradients[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n",
      "Epoch: 1, Loss: 159.412\n",
      "Epoch: 2, Loss: 37.355\n",
      "Epoch: 3, Loss: 32.265\n",
      "Epoch: 4, Loss: 29.945\n",
      "Epoch: 5, Loss: 20.282\n",
      "Epoch: 6, Loss: 26.480\n",
      "Epoch: 7, Loss: 19.819\n",
      "Epoch: 8, Loss: 18.054\n",
      "Epoch: 9, Loss: 16.089\n",
      "Epoch: 10, Loss: 13.934\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Check out the new network architecture and dataset!\n",
    "\n",
    "Notice that the weights and biases are\n",
    "generated randomly.\n",
    "\n",
    "No need to change anything, but feel free to tweak\n",
    "to test your network, play around with the epochs, batch size, etc!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "# from miniflow import *\n",
    "\n",
    "# Load data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Neural network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 10\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 11\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "# Step 4\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        forward_and_backward(graph)\n",
    "\n",
    "        # Step 3\n",
    "        sgd_update(trainables)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "\n",
    "    print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__:\n",
    "```Python\n",
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    \"\"\"\n",
    "    Updates the value of each trainable with SGD.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `trainables`: A list of `Input` nodes representing weights/biases.\n",
    "        `learning_rate`: The learning rate.\n",
    "    \"\"\"\n",
    "    # Performs SGD\n",
    "    #\n",
    "    # Loop over the trainables\n",
    "    for t in trainables:\n",
    "        # Change the trainable's value by subtracting the learning rate\n",
    "        # multiplied by the partial of the cost with respect to this\n",
    "        # trainable.\n",
    "        partial = t.gradients[t]\n",
    "        t.value -= learning_rate * partial\n",
    "```\n",
    "\n",
    "There are two keys steps. First, the partial of the cost (C) with respect to the trainable t is accessed.\n",
    "\n",
    "```Python\n",
    "partial = t.gradients[t]\n",
    "```\n",
    "\n",
    "Second, the value of the trainable is updated according to Equation (12).\n",
    "\n",
    "```Python\n",
    "t.value -= learning_rate * partial\n",
    "```\n",
    "\n",
    "This is done for all trainables.\n",
    "\n",
    "$$\n",
    "x' = x - \\eta \\nabla C \\\\\n",
    "\\text{Equation(12)}\n",
    "$$\n",
    "\n",
    "With that, the loss decreases on the next pass through the network.\n",
    "\n",
    "I'm putting the same quiz below again. If you haven't already, set the number of epochs to something like 1000 and watch as the loss decreases!\n",
    "\n",
    "Below is the final codes of miniflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# miniflow.py\n",
    "import numpy as np\n",
    "\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    Base class for nodes in the network.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `inbound_nodes`: A list of nodes with edges into this node.\n",
    "    \"\"\"\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        \"\"\"\n",
    "        Node's constructor (runs when the object is instantiated). Sets\n",
    "        properties that all nodes need.\n",
    "        \"\"\"\n",
    "        # A list of nodes with edges into this node.\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        # The eventual value of this node. Set by running\n",
    "        # the forward() method.\n",
    "        self.value = None\n",
    "        # A list of nodes that this node outputs to.\n",
    "        self.outbound_nodes = []\n",
    "        # New property! Keys are the inputs to this node and\n",
    "        # their values are the partials of this node with\n",
    "        # respect to that input.\n",
    "        self.gradients = {}\n",
    "        # Sets this node as an outbound node for all of\n",
    "        # this node's inputs.\n",
    "        for node in inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `forward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Every node that uses this class as a base class will\n",
    "        need to define its own `backward` method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Input(Node):\n",
    "    \"\"\"\n",
    "    A generic input into the network.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # The base class constructor has to run to set all\n",
    "        # the properties here.\n",
    "        #\n",
    "        # The most important property on an Input is value.\n",
    "        # self.value is set during `topological_sort` later.\n",
    "        Node.__init__(self)\n",
    "\n",
    "    def forward(self):\n",
    "        # Do nothing because nothing is calculated.\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        # An Input node has no inputs so the gradient (derivative)\n",
    "        # is zero.\n",
    "        # The key, `self`, is reference to this object.\n",
    "        self.gradients = {self: 0}\n",
    "        # Weights and bias may be inputs, so you need to sum\n",
    "        # the gradient from output gradients.\n",
    "        for n in self.outbound_nodes:\n",
    "            self.gradients[self] += n.gradients[self]\n",
    "\n",
    "class Linear(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs a linear transform.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, W, b):\n",
    "        # The base class (Node) constructor. Weights and bias\n",
    "        # are treated like inbound nodes.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Performs the math behind a linear transform.\n",
    "        \"\"\"\n",
    "        X = self.inbound_nodes[0].value\n",
    "        W = self.inbound_nodes[1].value\n",
    "        b = self.inbound_nodes[2].value\n",
    "        self.value = np.dot(X, W) + b\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient based on the output values.\n",
    "        \"\"\"\n",
    "        # Initialize a partial for each of the inbound_nodes.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Cycle through the outputs. The gradient will change depending\n",
    "        # on each output, so the gradients are summed over all outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            # Get the partial of the cost with respect to this node.\n",
    "            grad_cost = n.gradients[self]\n",
    "            # Set the partial of the loss with respect to this node's inputs.\n",
    "            self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, self.inbound_nodes[1].value.T)\n",
    "            # Set the partial of the loss with respect to this node's weights.\n",
    "            self.gradients[self.inbound_nodes[1]] += np.dot(self.inbound_nodes[0].value.T, grad_cost)\n",
    "            # Set the partial of the loss with respect to this node's bias.\n",
    "            self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, keepdims=False)\n",
    "\n",
    "\n",
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    Represents a node that performs the sigmoid activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        # The base class constructor.\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "        \"\"\"\n",
    "        return 1. / (1. + np.exp(-x))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform the sigmoid function and set the value.\n",
    "        \"\"\"\n",
    "        input_value = self.inbound_nodes[0].value\n",
    "        self.value = self._sigmoid(input_value)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient using the derivative of\n",
    "        the sigmoid function.\n",
    "        \"\"\"\n",
    "        # Initialize the gradients to 0.\n",
    "        self.gradients = {n: np.zeros_like(n.value) for n in self.inbound_nodes}\n",
    "        # Sum the partial with respect to the input over all the outputs.\n",
    "        for n in self.outbound_nodes:\n",
    "            grad_cost = n.gradients[self]\n",
    "            sigmoid = self.value\n",
    "            self.gradients[self.inbound_nodes[0]] += sigmoid * (1 - sigmoid) * grad_cost\n",
    "\n",
    "\n",
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last node for a network.\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "\n",
    "        self.m = self.inbound_nodes[0].value.shape[0]\n",
    "        # Save the computed output for backward.\n",
    "        self.diff = y - a\n",
    "        self.value = np.mean(self.diff**2)\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Calculates the gradient of the cost.\n",
    "        \"\"\"\n",
    "        self.gradients[self.inbound_nodes[0]] = (2 / self.m) * self.diff\n",
    "        self.gradients[self.inbound_nodes[1]] = (-2 / self.m) * self.diff\n",
    "\n",
    "\n",
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort the nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L\n",
    "\n",
    "\n",
    "def forward_and_backward(graph):\n",
    "    \"\"\"\n",
    "    Performs a forward pass and a backward pass through a list of sorted Nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `graph`: The result of calling `topological_sort`.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    for n in graph:\n",
    "        n.forward()\n",
    "\n",
    "    # Backward pass\n",
    "    # see: https://docs.python.org/2.3/whatsnew/section-slices.html\n",
    "    for n in graph[::-1]:\n",
    "        n.backward()\n",
    "\n",
    "\n",
    "def sgd_update(trainables, learning_rate=1e-2):\n",
    "    \"\"\"\n",
    "    Updates the value of each trainable with SGD.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `trainables`: A list of `Input` Nodes representing weights/biases.\n",
    "        `learning_rate`: The learning rate.\n",
    "    \"\"\"\n",
    "    # Performs SGD\n",
    "    #\n",
    "    # Loop over the trainables\n",
    "    for t in trainables:\n",
    "        # Change the trainable's value by subtracting the learning rate\n",
    "        # multiplied by the partial of the cost with respect to this\n",
    "        # trainable.\n",
    "        partial = t.gradients[t]\n",
    "        t.value -= learning_rate * partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples = 506\n",
      "Epoch: 1, Loss: 559.945\n",
      "Epoch: 2, Loss: 506.640\n",
      "Epoch: 3, Loss: 517.416\n",
      "Epoch: 4, Loss: 463.360\n",
      "Epoch: 5, Loss: 453.670\n",
      "Epoch: 6, Loss: 420.233\n",
      "Epoch: 7, Loss: 429.782\n",
      "Epoch: 8, Loss: 384.249\n",
      "Epoch: 9, Loss: 372.255\n",
      "Epoch: 10, Loss: 322.052\n",
      "Epoch: 11, Loss: 314.792\n",
      "Epoch: 12, Loss: 297.714\n",
      "Epoch: 13, Loss: 325.619\n",
      "Epoch: 14, Loss: 304.276\n",
      "Epoch: 15, Loss: 270.007\n",
      "Epoch: 16, Loss: 245.320\n",
      "Epoch: 17, Loss: 252.519\n",
      "Epoch: 18, Loss: 216.029\n",
      "Epoch: 19, Loss: 202.172\n",
      "Epoch: 20, Loss: 169.561\n",
      "Epoch: 21, Loss: 187.679\n",
      "Epoch: 22, Loss: 169.026\n",
      "Epoch: 23, Loss: 166.482\n",
      "Epoch: 24, Loss: 167.831\n",
      "Epoch: 25, Loss: 138.444\n",
      "Epoch: 26, Loss: 153.677\n",
      "Epoch: 27, Loss: 136.841\n",
      "Epoch: 28, Loss: 126.576\n",
      "Epoch: 29, Loss: 128.894\n",
      "Epoch: 30, Loss: 121.315\n",
      "Epoch: 31, Loss: 110.720\n",
      "Epoch: 32, Loss: 118.341\n",
      "Epoch: 33, Loss: 122.809\n",
      "Epoch: 34, Loss: 96.158\n",
      "Epoch: 35, Loss: 109.002\n",
      "Epoch: 36, Loss: 91.499\n",
      "Epoch: 37, Loss: 90.174\n",
      "Epoch: 38, Loss: 73.261\n",
      "Epoch: 39, Loss: 86.199\n",
      "Epoch: 40, Loss: 68.943\n",
      "Epoch: 41, Loss: 87.530\n",
      "Epoch: 42, Loss: 71.550\n",
      "Epoch: 43, Loss: 77.530\n",
      "Epoch: 44, Loss: 67.796\n",
      "Epoch: 45, Loss: 87.752\n",
      "Epoch: 46, Loss: 70.652\n",
      "Epoch: 47, Loss: 77.620\n",
      "Epoch: 48, Loss: 65.070\n",
      "Epoch: 49, Loss: 73.733\n",
      "Epoch: 50, Loss: 72.065\n",
      "Epoch: 51, Loss: 68.803\n",
      "Epoch: 52, Loss: 65.327\n",
      "Epoch: 53, Loss: 51.971\n",
      "Epoch: 54, Loss: 59.196\n",
      "Epoch: 55, Loss: 57.291\n",
      "Epoch: 56, Loss: 69.161\n",
      "Epoch: 57, Loss: 52.713\n",
      "Epoch: 58, Loss: 50.392\n",
      "Epoch: 59, Loss: 64.495\n",
      "Epoch: 60, Loss: 49.984\n",
      "Epoch: 61, Loss: 45.884\n",
      "Epoch: 62, Loss: 56.850\n",
      "Epoch: 63, Loss: 44.936\n",
      "Epoch: 64, Loss: 41.724\n",
      "Epoch: 65, Loss: 56.185\n",
      "Epoch: 66, Loss: 53.758\n",
      "Epoch: 67, Loss: 64.048\n",
      "Epoch: 68, Loss: 42.443\n",
      "Epoch: 69, Loss: 46.265\n",
      "Epoch: 70, Loss: 46.809\n",
      "Epoch: 71, Loss: 42.647\n",
      "Epoch: 72, Loss: 59.644\n",
      "Epoch: 73, Loss: 43.063\n",
      "Epoch: 74, Loss: 50.379\n",
      "Epoch: 75, Loss: 35.059\n",
      "Epoch: 76, Loss: 47.144\n",
      "Epoch: 77, Loss: 48.754\n",
      "Epoch: 78, Loss: 48.638\n",
      "Epoch: 79, Loss: 48.564\n",
      "Epoch: 80, Loss: 51.266\n",
      "Epoch: 81, Loss: 51.610\n",
      "Epoch: 82, Loss: 55.285\n",
      "Epoch: 83, Loss: 45.945\n",
      "Epoch: 84, Loss: 42.162\n",
      "Epoch: 85, Loss: 37.573\n",
      "Epoch: 86, Loss: 46.286\n",
      "Epoch: 87, Loss: 42.921\n",
      "Epoch: 88, Loss: 59.462\n",
      "Epoch: 89, Loss: 42.901\n",
      "Epoch: 90, Loss: 35.672\n",
      "Epoch: 91, Loss: 37.452\n",
      "Epoch: 92, Loss: 48.765\n",
      "Epoch: 93, Loss: 43.376\n",
      "Epoch: 94, Loss: 37.775\n",
      "Epoch: 95, Loss: 43.531\n",
      "Epoch: 96, Loss: 38.802\n",
      "Epoch: 97, Loss: 38.023\n",
      "Epoch: 98, Loss: 49.833\n",
      "Epoch: 99, Loss: 44.003\n",
      "Epoch: 100, Loss: 48.109\n",
      "Epoch: 101, Loss: 42.475\n",
      "Epoch: 102, Loss: 44.517\n",
      "Epoch: 103, Loss: 45.883\n",
      "Epoch: 104, Loss: 37.649\n",
      "Epoch: 105, Loss: 35.819\n",
      "Epoch: 106, Loss: 38.619\n",
      "Epoch: 107, Loss: 45.511\n",
      "Epoch: 108, Loss: 33.997\n",
      "Epoch: 109, Loss: 50.535\n",
      "Epoch: 110, Loss: 38.810\n",
      "Epoch: 111, Loss: 47.256\n",
      "Epoch: 112, Loss: 50.347\n",
      "Epoch: 113, Loss: 33.042\n",
      "Epoch: 114, Loss: 45.651\n",
      "Epoch: 115, Loss: 42.704\n",
      "Epoch: 116, Loss: 33.522\n",
      "Epoch: 117, Loss: 44.040\n",
      "Epoch: 118, Loss: 48.269\n",
      "Epoch: 119, Loss: 41.437\n",
      "Epoch: 120, Loss: 33.481\n",
      "Epoch: 121, Loss: 43.485\n",
      "Epoch: 122, Loss: 36.503\n",
      "Epoch: 123, Loss: 42.030\n",
      "Epoch: 124, Loss: 36.121\n",
      "Epoch: 125, Loss: 35.119\n",
      "Epoch: 126, Loss: 36.763\n",
      "Epoch: 127, Loss: 39.317\n",
      "Epoch: 128, Loss: 33.563\n",
      "Epoch: 129, Loss: 42.879\n",
      "Epoch: 130, Loss: 31.990\n",
      "Epoch: 131, Loss: 36.203\n",
      "Epoch: 132, Loss: 36.037\n",
      "Epoch: 133, Loss: 39.985\n",
      "Epoch: 134, Loss: 39.887\n",
      "Epoch: 135, Loss: 44.898\n",
      "Epoch: 136, Loss: 47.287\n",
      "Epoch: 137, Loss: 37.290\n",
      "Epoch: 138, Loss: 30.097\n",
      "Epoch: 139, Loss: 34.860\n",
      "Epoch: 140, Loss: 35.161\n",
      "Epoch: 141, Loss: 29.894\n",
      "Epoch: 142, Loss: 33.238\n",
      "Epoch: 143, Loss: 41.548\n",
      "Epoch: 144, Loss: 39.659\n",
      "Epoch: 145, Loss: 32.433\n",
      "Epoch: 146, Loss: 40.417\n",
      "Epoch: 147, Loss: 35.771\n",
      "Epoch: 148, Loss: 37.703\n",
      "Epoch: 149, Loss: 38.305\n",
      "Epoch: 150, Loss: 33.906\n",
      "Epoch: 151, Loss: 37.816\n",
      "Epoch: 152, Loss: 33.060\n",
      "Epoch: 153, Loss: 40.509\n",
      "Epoch: 154, Loss: 31.755\n",
      "Epoch: 155, Loss: 37.949\n",
      "Epoch: 156, Loss: 35.720\n",
      "Epoch: 157, Loss: 32.895\n",
      "Epoch: 158, Loss: 36.225\n",
      "Epoch: 159, Loss: 41.666\n",
      "Epoch: 160, Loss: 42.643\n",
      "Epoch: 161, Loss: 29.961\n",
      "Epoch: 162, Loss: 41.475\n",
      "Epoch: 163, Loss: 34.432\n",
      "Epoch: 164, Loss: 37.029\n",
      "Epoch: 165, Loss: 33.916\n",
      "Epoch: 166, Loss: 36.003\n",
      "Epoch: 167, Loss: 33.395\n",
      "Epoch: 168, Loss: 40.056\n",
      "Epoch: 169, Loss: 35.441\n",
      "Epoch: 170, Loss: 30.085\n",
      "Epoch: 171, Loss: 37.032\n",
      "Epoch: 172, Loss: 36.171\n",
      "Epoch: 173, Loss: 28.439\n",
      "Epoch: 174, Loss: 38.074\n",
      "Epoch: 175, Loss: 37.282\n",
      "Epoch: 176, Loss: 31.890\n",
      "Epoch: 177, Loss: 43.130\n",
      "Epoch: 178, Loss: 30.183\n",
      "Epoch: 179, Loss: 31.708\n",
      "Epoch: 180, Loss: 32.425\n",
      "Epoch: 181, Loss: 36.543\n",
      "Epoch: 182, Loss: 26.569\n",
      "Epoch: 183, Loss: 31.955\n",
      "Epoch: 184, Loss: 36.637\n",
      "Epoch: 185, Loss: 33.368\n",
      "Epoch: 186, Loss: 31.614\n",
      "Epoch: 187, Loss: 28.036\n",
      "Epoch: 188, Loss: 32.024\n",
      "Epoch: 189, Loss: 26.772\n",
      "Epoch: 190, Loss: 28.564\n",
      "Epoch: 191, Loss: 29.153\n",
      "Epoch: 192, Loss: 36.924\n",
      "Epoch: 193, Loss: 32.787\n",
      "Epoch: 194, Loss: 34.648\n",
      "Epoch: 195, Loss: 37.022\n",
      "Epoch: 196, Loss: 30.704\n",
      "Epoch: 197, Loss: 29.839\n",
      "Epoch: 198, Loss: 35.282\n",
      "Epoch: 199, Loss: 32.073\n",
      "Epoch: 200, Loss: 27.118\n",
      "Epoch: 201, Loss: 37.803\n",
      "Epoch: 202, Loss: 29.449\n",
      "Epoch: 203, Loss: 38.293\n",
      "Epoch: 204, Loss: 34.421\n",
      "Epoch: 205, Loss: 33.539\n",
      "Epoch: 206, Loss: 33.517\n",
      "Epoch: 207, Loss: 32.542\n",
      "Epoch: 208, Loss: 37.766\n",
      "Epoch: 209, Loss: 33.735\n",
      "Epoch: 210, Loss: 30.769\n",
      "Epoch: 211, Loss: 30.300\n",
      "Epoch: 212, Loss: 33.182\n",
      "Epoch: 213, Loss: 34.372\n",
      "Epoch: 214, Loss: 34.614\n",
      "Epoch: 215, Loss: 30.621\n",
      "Epoch: 216, Loss: 33.740\n",
      "Epoch: 217, Loss: 30.926\n",
      "Epoch: 218, Loss: 29.524\n",
      "Epoch: 219, Loss: 25.907\n",
      "Epoch: 220, Loss: 34.387\n",
      "Epoch: 221, Loss: 35.626\n",
      "Epoch: 222, Loss: 27.882\n",
      "Epoch: 223, Loss: 30.163\n",
      "Epoch: 224, Loss: 31.649\n",
      "Epoch: 225, Loss: 26.045\n",
      "Epoch: 226, Loss: 26.960\n",
      "Epoch: 227, Loss: 25.076\n",
      "Epoch: 228, Loss: 31.394\n",
      "Epoch: 229, Loss: 34.262\n",
      "Epoch: 230, Loss: 30.592\n",
      "Epoch: 231, Loss: 35.390\n",
      "Epoch: 232, Loss: 32.033\n",
      "Epoch: 233, Loss: 28.667\n",
      "Epoch: 234, Loss: 33.854\n",
      "Epoch: 235, Loss: 37.541\n",
      "Epoch: 236, Loss: 32.174\n",
      "Epoch: 237, Loss: 28.574\n",
      "Epoch: 238, Loss: 27.746\n",
      "Epoch: 239, Loss: 28.863\n",
      "Epoch: 240, Loss: 28.172\n",
      "Epoch: 241, Loss: 27.895\n",
      "Epoch: 242, Loss: 26.139\n",
      "Epoch: 243, Loss: 27.003\n",
      "Epoch: 244, Loss: 23.785\n",
      "Epoch: 245, Loss: 30.264\n",
      "Epoch: 246, Loss: 30.530\n",
      "Epoch: 247, Loss: 27.026\n",
      "Epoch: 248, Loss: 35.855\n",
      "Epoch: 249, Loss: 29.269\n",
      "Epoch: 250, Loss: 28.372\n",
      "Epoch: 251, Loss: 28.827\n",
      "Epoch: 252, Loss: 34.673\n",
      "Epoch: 253, Loss: 29.695\n",
      "Epoch: 254, Loss: 38.133\n",
      "Epoch: 255, Loss: 25.706\n",
      "Epoch: 256, Loss: 22.324\n",
      "Epoch: 257, Loss: 31.041\n",
      "Epoch: 258, Loss: 26.564\n",
      "Epoch: 259, Loss: 26.185\n",
      "Epoch: 260, Loss: 32.990\n",
      "Epoch: 261, Loss: 26.931\n",
      "Epoch: 262, Loss: 30.799\n",
      "Epoch: 263, Loss: 26.349\n",
      "Epoch: 264, Loss: 31.721\n",
      "Epoch: 265, Loss: 26.994\n",
      "Epoch: 266, Loss: 29.733\n",
      "Epoch: 267, Loss: 27.959\n",
      "Epoch: 268, Loss: 27.839\n",
      "Epoch: 269, Loss: 24.668\n",
      "Epoch: 270, Loss: 27.158\n",
      "Epoch: 271, Loss: 32.658\n",
      "Epoch: 272, Loss: 26.617\n",
      "Epoch: 273, Loss: 33.018\n",
      "Epoch: 274, Loss: 28.133\n",
      "Epoch: 275, Loss: 26.212\n",
      "Epoch: 276, Loss: 26.566\n",
      "Epoch: 277, Loss: 28.268\n",
      "Epoch: 278, Loss: 22.098\n",
      "Epoch: 279, Loss: 26.680\n",
      "Epoch: 280, Loss: 24.423\n",
      "Epoch: 281, Loss: 26.268\n",
      "Epoch: 282, Loss: 25.208\n",
      "Epoch: 283, Loss: 31.055\n",
      "Epoch: 284, Loss: 29.013\n",
      "Epoch: 285, Loss: 26.440\n",
      "Epoch: 286, Loss: 30.512\n",
      "Epoch: 287, Loss: 24.420\n",
      "Epoch: 288, Loss: 22.461\n",
      "Epoch: 289, Loss: 28.667\n",
      "Epoch: 290, Loss: 28.478\n",
      "Epoch: 291, Loss: 24.852\n",
      "Epoch: 292, Loss: 27.016\n",
      "Epoch: 293, Loss: 29.469\n",
      "Epoch: 294, Loss: 29.521\n",
      "Epoch: 295, Loss: 27.312\n",
      "Epoch: 296, Loss: 22.502\n",
      "Epoch: 297, Loss: 23.303\n",
      "Epoch: 298, Loss: 33.826\n",
      "Epoch: 299, Loss: 22.985\n",
      "Epoch: 300, Loss: 27.512\n",
      "Epoch: 301, Loss: 24.252\n",
      "Epoch: 302, Loss: 24.034\n",
      "Epoch: 303, Loss: 28.669\n",
      "Epoch: 304, Loss: 23.633\n",
      "Epoch: 305, Loss: 34.065\n",
      "Epoch: 306, Loss: 24.006\n",
      "Epoch: 307, Loss: 24.782\n",
      "Epoch: 308, Loss: 25.857\n",
      "Epoch: 309, Loss: 23.706\n",
      "Epoch: 310, Loss: 25.522\n",
      "Epoch: 311, Loss: 28.692\n",
      "Epoch: 312, Loss: 22.678\n",
      "Epoch: 313, Loss: 20.540\n",
      "Epoch: 314, Loss: 28.259\n",
      "Epoch: 315, Loss: 26.698\n",
      "Epoch: 316, Loss: 24.582\n",
      "Epoch: 317, Loss: 19.447\n",
      "Epoch: 318, Loss: 27.407\n",
      "Epoch: 319, Loss: 27.865\n",
      "Epoch: 320, Loss: 24.034\n",
      "Epoch: 321, Loss: 26.042\n",
      "Epoch: 322, Loss: 23.754\n",
      "Epoch: 323, Loss: 30.792\n",
      "Epoch: 324, Loss: 24.363\n",
      "Epoch: 325, Loss: 27.576\n",
      "Epoch: 326, Loss: 22.822\n",
      "Epoch: 327, Loss: 22.157\n",
      "Epoch: 328, Loss: 24.314\n",
      "Epoch: 329, Loss: 27.012\n",
      "Epoch: 330, Loss: 25.226\n",
      "Epoch: 331, Loss: 29.390\n",
      "Epoch: 332, Loss: 25.673\n",
      "Epoch: 333, Loss: 25.544\n",
      "Epoch: 334, Loss: 22.526\n",
      "Epoch: 335, Loss: 23.712\n",
      "Epoch: 336, Loss: 24.252\n",
      "Epoch: 337, Loss: 26.721\n",
      "Epoch: 338, Loss: 27.004\n",
      "Epoch: 339, Loss: 21.432\n",
      "Epoch: 340, Loss: 28.585\n",
      "Epoch: 341, Loss: 32.909\n",
      "Epoch: 342, Loss: 25.244\n",
      "Epoch: 343, Loss: 24.359\n",
      "Epoch: 344, Loss: 27.551\n",
      "Epoch: 345, Loss: 26.780\n",
      "Epoch: 346, Loss: 18.587\n",
      "Epoch: 347, Loss: 21.815\n",
      "Epoch: 348, Loss: 24.039\n",
      "Epoch: 349, Loss: 24.318\n",
      "Epoch: 350, Loss: 21.856\n",
      "Epoch: 351, Loss: 24.183\n",
      "Epoch: 352, Loss: 22.188\n",
      "Epoch: 353, Loss: 27.581\n",
      "Epoch: 354, Loss: 17.469\n",
      "Epoch: 355, Loss: 22.431\n",
      "Epoch: 356, Loss: 27.929\n",
      "Epoch: 357, Loss: 23.303\n",
      "Epoch: 358, Loss: 19.985\n",
      "Epoch: 359, Loss: 27.104\n",
      "Epoch: 360, Loss: 24.753\n",
      "Epoch: 361, Loss: 27.728\n",
      "Epoch: 362, Loss: 23.666\n",
      "Epoch: 363, Loss: 22.060\n",
      "Epoch: 364, Loss: 18.801\n",
      "Epoch: 365, Loss: 24.782\n",
      "Epoch: 366, Loss: 27.061\n",
      "Epoch: 367, Loss: 22.268\n",
      "Epoch: 368, Loss: 22.419\n",
      "Epoch: 369, Loss: 22.844\n",
      "Epoch: 370, Loss: 19.784\n",
      "Epoch: 371, Loss: 20.560\n",
      "Epoch: 372, Loss: 20.611\n",
      "Epoch: 373, Loss: 24.671\n",
      "Epoch: 374, Loss: 18.950\n",
      "Epoch: 375, Loss: 20.919\n",
      "Epoch: 376, Loss: 19.854\n",
      "Epoch: 377, Loss: 24.518\n",
      "Epoch: 378, Loss: 21.643\n",
      "Epoch: 379, Loss: 22.359\n",
      "Epoch: 380, Loss: 19.734\n",
      "Epoch: 381, Loss: 23.681\n",
      "Epoch: 382, Loss: 22.729\n",
      "Epoch: 383, Loss: 22.408\n",
      "Epoch: 384, Loss: 20.851\n",
      "Epoch: 385, Loss: 24.664\n",
      "Epoch: 386, Loss: 25.802\n",
      "Epoch: 387, Loss: 23.175\n",
      "Epoch: 388, Loss: 19.398\n",
      "Epoch: 389, Loss: 22.711\n",
      "Epoch: 390, Loss: 25.021\n",
      "Epoch: 391, Loss: 22.865\n",
      "Epoch: 392, Loss: 17.505\n",
      "Epoch: 393, Loss: 25.076\n",
      "Epoch: 394, Loss: 22.018\n",
      "Epoch: 395, Loss: 17.573\n",
      "Epoch: 396, Loss: 20.375\n",
      "Epoch: 397, Loss: 23.333\n",
      "Epoch: 398, Loss: 22.551\n",
      "Epoch: 399, Loss: 22.028\n",
      "Epoch: 400, Loss: 19.926\n",
      "Epoch: 401, Loss: 22.620\n",
      "Epoch: 402, Loss: 21.608\n",
      "Epoch: 403, Loss: 22.613\n",
      "Epoch: 404, Loss: 20.487\n",
      "Epoch: 405, Loss: 22.431\n",
      "Epoch: 406, Loss: 18.129\n",
      "Epoch: 407, Loss: 22.772\n",
      "Epoch: 408, Loss: 26.067\n",
      "Epoch: 409, Loss: 24.564\n",
      "Epoch: 410, Loss: 18.970\n",
      "Epoch: 411, Loss: 18.813\n",
      "Epoch: 412, Loss: 24.794\n",
      "Epoch: 413, Loss: 24.195\n",
      "Epoch: 414, Loss: 17.364\n",
      "Epoch: 415, Loss: 23.742\n",
      "Epoch: 416, Loss: 15.815\n",
      "Epoch: 417, Loss: 21.303\n",
      "Epoch: 418, Loss: 20.271\n",
      "Epoch: 419, Loss: 22.729\n",
      "Epoch: 420, Loss: 22.552\n",
      "Epoch: 421, Loss: 20.110\n",
      "Epoch: 422, Loss: 17.495\n",
      "Epoch: 423, Loss: 19.762\n",
      "Epoch: 424, Loss: 21.127\n",
      "Epoch: 425, Loss: 20.838\n",
      "Epoch: 426, Loss: 23.599\n",
      "Epoch: 427, Loss: 22.102\n",
      "Epoch: 428, Loss: 22.724\n",
      "Epoch: 429, Loss: 19.047\n",
      "Epoch: 430, Loss: 19.907\n",
      "Epoch: 431, Loss: 21.435\n",
      "Epoch: 432, Loss: 26.174\n",
      "Epoch: 433, Loss: 19.772\n",
      "Epoch: 434, Loss: 21.866\n",
      "Epoch: 435, Loss: 19.997\n",
      "Epoch: 436, Loss: 26.185\n",
      "Epoch: 437, Loss: 18.588\n",
      "Epoch: 438, Loss: 20.259\n",
      "Epoch: 439, Loss: 20.633\n",
      "Epoch: 440, Loss: 18.241\n",
      "Epoch: 441, Loss: 20.151\n",
      "Epoch: 442, Loss: 19.995\n",
      "Epoch: 443, Loss: 16.939\n",
      "Epoch: 444, Loss: 18.831\n",
      "Epoch: 445, Loss: 20.705\n",
      "Epoch: 446, Loss: 21.585\n",
      "Epoch: 447, Loss: 19.022\n",
      "Epoch: 448, Loss: 21.227\n",
      "Epoch: 449, Loss: 17.038\n",
      "Epoch: 450, Loss: 18.345\n",
      "Epoch: 451, Loss: 22.167\n",
      "Epoch: 452, Loss: 16.104\n",
      "Epoch: 453, Loss: 17.136\n",
      "Epoch: 454, Loss: 24.404\n",
      "Epoch: 455, Loss: 21.083\n",
      "Epoch: 456, Loss: 19.487\n",
      "Epoch: 457, Loss: 17.416\n",
      "Epoch: 458, Loss: 19.430\n",
      "Epoch: 459, Loss: 19.060\n",
      "Epoch: 460, Loss: 20.576\n",
      "Epoch: 461, Loss: 21.523\n",
      "Epoch: 462, Loss: 16.551\n",
      "Epoch: 463, Loss: 16.163\n",
      "Epoch: 464, Loss: 19.273\n",
      "Epoch: 465, Loss: 16.997\n",
      "Epoch: 466, Loss: 19.482\n",
      "Epoch: 467, Loss: 17.347\n",
      "Epoch: 468, Loss: 20.497\n",
      "Epoch: 469, Loss: 21.819\n",
      "Epoch: 470, Loss: 15.497\n",
      "Epoch: 471, Loss: 16.161\n",
      "Epoch: 472, Loss: 20.259\n",
      "Epoch: 473, Loss: 23.607\n",
      "Epoch: 474, Loss: 18.348\n",
      "Epoch: 475, Loss: 18.186\n",
      "Epoch: 476, Loss: 17.614\n",
      "Epoch: 477, Loss: 15.876\n",
      "Epoch: 478, Loss: 21.028\n",
      "Epoch: 479, Loss: 24.932\n",
      "Epoch: 480, Loss: 20.149\n",
      "Epoch: 481, Loss: 22.044\n",
      "Epoch: 482, Loss: 17.327\n",
      "Epoch: 483, Loss: 18.056\n",
      "Epoch: 484, Loss: 18.778\n",
      "Epoch: 485, Loss: 19.323\n",
      "Epoch: 486, Loss: 18.464\n",
      "Epoch: 487, Loss: 21.110\n",
      "Epoch: 488, Loss: 18.108\n",
      "Epoch: 489, Loss: 19.022\n",
      "Epoch: 490, Loss: 20.832\n",
      "Epoch: 491, Loss: 19.014\n",
      "Epoch: 492, Loss: 17.190\n",
      "Epoch: 493, Loss: 16.908\n",
      "Epoch: 494, Loss: 16.929\n",
      "Epoch: 495, Loss: 18.184\n",
      "Epoch: 496, Loss: 17.721\n",
      "Epoch: 497, Loss: 21.080\n",
      "Epoch: 498, Loss: 20.135\n",
      "Epoch: 499, Loss: 21.040\n",
      "Epoch: 500, Loss: 20.106\n",
      "Epoch: 501, Loss: 15.553\n",
      "Epoch: 502, Loss: 18.591\n",
      "Epoch: 503, Loss: 18.974\n",
      "Epoch: 504, Loss: 17.840\n",
      "Epoch: 505, Loss: 17.437\n",
      "Epoch: 506, Loss: 19.020\n",
      "Epoch: 507, Loss: 15.900\n",
      "Epoch: 508, Loss: 21.370\n",
      "Epoch: 509, Loss: 23.979\n",
      "Epoch: 510, Loss: 15.131\n",
      "Epoch: 511, Loss: 23.206\n",
      "Epoch: 512, Loss: 17.459\n",
      "Epoch: 513, Loss: 15.808\n",
      "Epoch: 514, Loss: 19.018\n",
      "Epoch: 515, Loss: 19.229\n",
      "Epoch: 516, Loss: 16.067\n",
      "Epoch: 517, Loss: 15.664\n",
      "Epoch: 518, Loss: 22.196\n",
      "Epoch: 519, Loss: 19.395\n",
      "Epoch: 520, Loss: 17.636\n",
      "Epoch: 521, Loss: 20.902\n",
      "Epoch: 522, Loss: 17.247\n",
      "Epoch: 523, Loss: 19.141\n",
      "Epoch: 524, Loss: 22.595\n",
      "Epoch: 525, Loss: 19.477\n",
      "Epoch: 526, Loss: 15.579\n",
      "Epoch: 527, Loss: 18.302\n",
      "Epoch: 528, Loss: 16.463\n",
      "Epoch: 529, Loss: 14.344\n",
      "Epoch: 530, Loss: 16.644\n",
      "Epoch: 531, Loss: 17.112\n",
      "Epoch: 532, Loss: 16.701\n",
      "Epoch: 533, Loss: 17.531\n",
      "Epoch: 534, Loss: 18.414\n",
      "Epoch: 535, Loss: 20.261\n",
      "Epoch: 536, Loss: 14.294\n",
      "Epoch: 537, Loss: 15.297\n",
      "Epoch: 538, Loss: 14.934\n",
      "Epoch: 539, Loss: 19.089\n",
      "Epoch: 540, Loss: 17.752\n",
      "Epoch: 541, Loss: 16.828\n",
      "Epoch: 542, Loss: 18.019\n",
      "Epoch: 543, Loss: 19.562\n",
      "Epoch: 544, Loss: 20.795\n",
      "Epoch: 545, Loss: 17.014\n",
      "Epoch: 546, Loss: 18.762\n",
      "Epoch: 547, Loss: 21.832\n",
      "Epoch: 548, Loss: 20.247\n",
      "Epoch: 549, Loss: 15.174\n",
      "Epoch: 550, Loss: 19.724\n",
      "Epoch: 551, Loss: 17.447\n",
      "Epoch: 552, Loss: 20.017\n",
      "Epoch: 553, Loss: 20.911\n",
      "Epoch: 554, Loss: 15.759\n",
      "Epoch: 555, Loss: 19.678\n",
      "Epoch: 556, Loss: 19.737\n",
      "Epoch: 557, Loss: 16.456\n",
      "Epoch: 558, Loss: 17.192\n",
      "Epoch: 559, Loss: 15.649\n",
      "Epoch: 560, Loss: 17.960\n",
      "Epoch: 561, Loss: 19.724\n",
      "Epoch: 562, Loss: 18.544\n",
      "Epoch: 563, Loss: 14.241\n",
      "Epoch: 564, Loss: 19.182\n",
      "Epoch: 565, Loss: 18.018\n",
      "Epoch: 566, Loss: 16.539\n",
      "Epoch: 567, Loss: 16.218\n",
      "Epoch: 568, Loss: 16.197\n",
      "Epoch: 569, Loss: 18.161\n",
      "Epoch: 570, Loss: 17.532\n",
      "Epoch: 571, Loss: 18.166\n",
      "Epoch: 572, Loss: 15.028\n",
      "Epoch: 573, Loss: 17.454\n",
      "Epoch: 574, Loss: 16.743\n",
      "Epoch: 575, Loss: 22.243\n",
      "Epoch: 576, Loss: 16.085\n",
      "Epoch: 577, Loss: 18.246\n",
      "Epoch: 578, Loss: 19.148\n",
      "Epoch: 579, Loss: 18.222\n",
      "Epoch: 580, Loss: 16.930\n",
      "Epoch: 581, Loss: 19.472\n",
      "Epoch: 582, Loss: 17.125\n",
      "Epoch: 583, Loss: 14.646\n",
      "Epoch: 584, Loss: 15.624\n",
      "Epoch: 585, Loss: 15.559\n",
      "Epoch: 586, Loss: 18.918\n",
      "Epoch: 587, Loss: 16.383\n",
      "Epoch: 588, Loss: 12.493\n",
      "Epoch: 589, Loss: 18.556\n",
      "Epoch: 590, Loss: 15.577\n",
      "Epoch: 591, Loss: 15.580\n",
      "Epoch: 592, Loss: 19.927\n",
      "Epoch: 593, Loss: 15.579\n",
      "Epoch: 594, Loss: 15.078\n",
      "Epoch: 595, Loss: 15.024\n",
      "Epoch: 596, Loss: 20.142\n",
      "Epoch: 597, Loss: 19.047\n",
      "Epoch: 598, Loss: 19.214\n",
      "Epoch: 599, Loss: 18.060\n",
      "Epoch: 600, Loss: 17.064\n",
      "Epoch: 601, Loss: 16.093\n",
      "Epoch: 602, Loss: 17.476\n",
      "Epoch: 603, Loss: 15.430\n",
      "Epoch: 604, Loss: 16.280\n",
      "Epoch: 605, Loss: 18.474\n",
      "Epoch: 606, Loss: 15.163\n",
      "Epoch: 607, Loss: 13.883\n",
      "Epoch: 608, Loss: 16.657\n",
      "Epoch: 609, Loss: 17.311\n",
      "Epoch: 610, Loss: 18.482\n",
      "Epoch: 611, Loss: 13.946\n",
      "Epoch: 612, Loss: 14.803\n",
      "Epoch: 613, Loss: 16.299\n",
      "Epoch: 614, Loss: 16.420\n",
      "Epoch: 615, Loss: 16.346\n",
      "Epoch: 616, Loss: 16.357\n",
      "Epoch: 617, Loss: 19.700\n",
      "Epoch: 618, Loss: 19.527\n",
      "Epoch: 619, Loss: 16.826\n",
      "Epoch: 620, Loss: 21.175\n",
      "Epoch: 621, Loss: 19.142\n",
      "Epoch: 622, Loss: 19.366\n",
      "Epoch: 623, Loss: 14.447\n",
      "Epoch: 624, Loss: 16.758\n",
      "Epoch: 625, Loss: 15.652\n",
      "Epoch: 626, Loss: 16.097\n",
      "Epoch: 627, Loss: 12.364\n",
      "Epoch: 628, Loss: 12.932\n",
      "Epoch: 629, Loss: 17.974\n",
      "Epoch: 630, Loss: 17.813\n",
      "Epoch: 631, Loss: 16.822\n",
      "Epoch: 632, Loss: 18.139\n",
      "Epoch: 633, Loss: 15.566\n",
      "Epoch: 634, Loss: 13.972\n",
      "Epoch: 635, Loss: 15.281\n",
      "Epoch: 636, Loss: 17.276\n",
      "Epoch: 637, Loss: 17.814\n",
      "Epoch: 638, Loss: 21.425\n",
      "Epoch: 639, Loss: 18.586\n",
      "Epoch: 640, Loss: 17.886\n",
      "Epoch: 641, Loss: 16.567\n",
      "Epoch: 642, Loss: 14.850\n",
      "Epoch: 643, Loss: 15.528\n",
      "Epoch: 644, Loss: 13.775\n",
      "Epoch: 645, Loss: 16.496\n",
      "Epoch: 646, Loss: 15.331\n",
      "Epoch: 647, Loss: 15.688\n",
      "Epoch: 648, Loss: 14.614\n",
      "Epoch: 649, Loss: 13.818\n",
      "Epoch: 650, Loss: 13.819\n",
      "Epoch: 651, Loss: 13.320\n",
      "Epoch: 652, Loss: 21.199\n",
      "Epoch: 653, Loss: 15.896\n",
      "Epoch: 654, Loss: 15.287\n",
      "Epoch: 655, Loss: 19.315\n",
      "Epoch: 656, Loss: 18.161\n",
      "Epoch: 657, Loss: 14.565\n",
      "Epoch: 658, Loss: 16.629\n",
      "Epoch: 659, Loss: 15.920\n",
      "Epoch: 660, Loss: 15.117\n",
      "Epoch: 661, Loss: 18.013\n",
      "Epoch: 662, Loss: 17.109\n",
      "Epoch: 663, Loss: 14.595\n",
      "Epoch: 664, Loss: 16.244\n",
      "Epoch: 665, Loss: 13.641\n",
      "Epoch: 666, Loss: 17.435\n",
      "Epoch: 667, Loss: 13.919\n",
      "Epoch: 668, Loss: 14.841\n",
      "Epoch: 669, Loss: 16.455\n",
      "Epoch: 670, Loss: 16.733\n",
      "Epoch: 671, Loss: 13.316\n",
      "Epoch: 672, Loss: 12.692\n",
      "Epoch: 673, Loss: 16.367\n",
      "Epoch: 674, Loss: 14.854\n",
      "Epoch: 675, Loss: 18.303\n",
      "Epoch: 676, Loss: 15.452\n",
      "Epoch: 677, Loss: 13.308\n",
      "Epoch: 678, Loss: 16.384\n",
      "Epoch: 679, Loss: 12.981\n",
      "Epoch: 680, Loss: 15.403\n",
      "Epoch: 681, Loss: 14.344\n",
      "Epoch: 682, Loss: 17.271\n",
      "Epoch: 683, Loss: 13.673\n",
      "Epoch: 684, Loss: 13.481\n",
      "Epoch: 685, Loss: 15.714\n",
      "Epoch: 686, Loss: 13.181\n",
      "Epoch: 687, Loss: 16.106\n",
      "Epoch: 688, Loss: 12.644\n",
      "Epoch: 689, Loss: 16.399\n",
      "Epoch: 690, Loss: 12.886\n",
      "Epoch: 691, Loss: 14.783\n",
      "Epoch: 692, Loss: 13.611\n",
      "Epoch: 693, Loss: 14.163\n",
      "Epoch: 694, Loss: 18.268\n",
      "Epoch: 695, Loss: 12.499\n",
      "Epoch: 696, Loss: 14.172\n",
      "Epoch: 697, Loss: 16.065\n",
      "Epoch: 698, Loss: 17.587\n",
      "Epoch: 699, Loss: 13.228\n",
      "Epoch: 700, Loss: 16.735\n",
      "Epoch: 701, Loss: 15.039\n",
      "Epoch: 702, Loss: 18.626\n",
      "Epoch: 703, Loss: 17.552\n",
      "Epoch: 704, Loss: 13.442\n",
      "Epoch: 705, Loss: 15.363\n",
      "Epoch: 706, Loss: 15.217\n",
      "Epoch: 707, Loss: 14.637\n",
      "Epoch: 708, Loss: 14.993\n",
      "Epoch: 709, Loss: 16.419\n",
      "Epoch: 710, Loss: 14.569\n",
      "Epoch: 711, Loss: 14.289\n",
      "Epoch: 712, Loss: 14.174\n",
      "Epoch: 713, Loss: 14.445\n",
      "Epoch: 714, Loss: 17.287\n",
      "Epoch: 715, Loss: 18.963\n",
      "Epoch: 716, Loss: 14.528\n",
      "Epoch: 717, Loss: 13.821\n",
      "Epoch: 718, Loss: 14.619\n",
      "Epoch: 719, Loss: 16.094\n",
      "Epoch: 720, Loss: 15.115\n",
      "Epoch: 721, Loss: 13.668\n",
      "Epoch: 722, Loss: 15.599\n",
      "Epoch: 723, Loss: 14.664\n",
      "Epoch: 724, Loss: 13.260\n",
      "Epoch: 725, Loss: 13.195\n",
      "Epoch: 726, Loss: 14.272\n",
      "Epoch: 727, Loss: 16.125\n",
      "Epoch: 728, Loss: 17.100\n",
      "Epoch: 729, Loss: 15.679\n",
      "Epoch: 730, Loss: 13.862\n",
      "Epoch: 731, Loss: 15.762\n",
      "Epoch: 732, Loss: 13.773\n",
      "Epoch: 733, Loss: 16.010\n",
      "Epoch: 734, Loss: 16.634\n",
      "Epoch: 735, Loss: 18.010\n",
      "Epoch: 736, Loss: 12.585\n",
      "Epoch: 737, Loss: 17.015\n",
      "Epoch: 738, Loss: 15.029\n",
      "Epoch: 739, Loss: 14.719\n",
      "Epoch: 740, Loss: 13.643\n",
      "Epoch: 741, Loss: 12.637\n",
      "Epoch: 742, Loss: 14.575\n",
      "Epoch: 743, Loss: 13.167\n",
      "Epoch: 744, Loss: 14.340\n",
      "Epoch: 745, Loss: 15.814\n",
      "Epoch: 746, Loss: 14.469\n",
      "Epoch: 747, Loss: 11.483\n",
      "Epoch: 748, Loss: 16.640\n",
      "Epoch: 749, Loss: 16.577\n",
      "Epoch: 750, Loss: 16.787\n",
      "Epoch: 751, Loss: 14.347\n",
      "Epoch: 752, Loss: 14.732\n",
      "Epoch: 753, Loss: 12.044\n",
      "Epoch: 754, Loss: 14.294\n",
      "Epoch: 755, Loss: 12.425\n",
      "Epoch: 756, Loss: 11.896\n",
      "Epoch: 757, Loss: 17.876\n",
      "Epoch: 758, Loss: 12.880\n",
      "Epoch: 759, Loss: 16.357\n",
      "Epoch: 760, Loss: 17.457\n",
      "Epoch: 761, Loss: 13.140\n",
      "Epoch: 762, Loss: 13.557\n",
      "Epoch: 763, Loss: 16.110\n",
      "Epoch: 764, Loss: 17.559\n",
      "Epoch: 765, Loss: 14.614\n",
      "Epoch: 766, Loss: 16.997\n",
      "Epoch: 767, Loss: 13.656\n",
      "Epoch: 768, Loss: 15.259\n",
      "Epoch: 769, Loss: 15.760\n",
      "Epoch: 770, Loss: 16.540\n",
      "Epoch: 771, Loss: 14.244\n",
      "Epoch: 772, Loss: 15.231\n",
      "Epoch: 773, Loss: 13.341\n",
      "Epoch: 774, Loss: 13.252\n",
      "Epoch: 775, Loss: 13.966\n",
      "Epoch: 776, Loss: 13.850\n",
      "Epoch: 777, Loss: 13.619\n",
      "Epoch: 778, Loss: 15.399\n",
      "Epoch: 779, Loss: 13.514\n",
      "Epoch: 780, Loss: 14.681\n",
      "Epoch: 781, Loss: 12.309\n",
      "Epoch: 782, Loss: 12.387\n",
      "Epoch: 783, Loss: 19.121\n",
      "Epoch: 784, Loss: 15.353\n",
      "Epoch: 785, Loss: 14.844\n",
      "Epoch: 786, Loss: 11.250\n",
      "Epoch: 787, Loss: 13.584\n",
      "Epoch: 788, Loss: 14.076\n",
      "Epoch: 789, Loss: 13.816\n",
      "Epoch: 790, Loss: 13.671\n",
      "Epoch: 791, Loss: 14.166\n",
      "Epoch: 792, Loss: 17.992\n",
      "Epoch: 793, Loss: 12.891\n",
      "Epoch: 794, Loss: 16.373\n",
      "Epoch: 795, Loss: 14.968\n",
      "Epoch: 796, Loss: 13.316\n",
      "Epoch: 797, Loss: 15.298\n",
      "Epoch: 798, Loss: 10.784\n",
      "Epoch: 799, Loss: 16.045\n",
      "Epoch: 800, Loss: 16.943\n",
      "Epoch: 801, Loss: 15.424\n",
      "Epoch: 802, Loss: 17.055\n",
      "Epoch: 803, Loss: 13.801\n",
      "Epoch: 804, Loss: 14.243\n",
      "Epoch: 805, Loss: 18.696\n",
      "Epoch: 806, Loss: 14.667\n",
      "Epoch: 807, Loss: 11.570\n",
      "Epoch: 808, Loss: 13.381\n",
      "Epoch: 809, Loss: 15.076\n",
      "Epoch: 810, Loss: 16.325\n",
      "Epoch: 811, Loss: 15.604\n",
      "Epoch: 812, Loss: 14.522\n",
      "Epoch: 813, Loss: 13.374\n",
      "Epoch: 814, Loss: 12.115\n",
      "Epoch: 815, Loss: 14.862\n",
      "Epoch: 816, Loss: 14.870\n",
      "Epoch: 817, Loss: 11.637\n",
      "Epoch: 818, Loss: 14.886\n",
      "Epoch: 819, Loss: 11.544\n",
      "Epoch: 820, Loss: 11.419\n",
      "Epoch: 821, Loss: 11.376\n",
      "Epoch: 822, Loss: 11.295\n",
      "Epoch: 823, Loss: 14.021\n",
      "Epoch: 824, Loss: 11.775\n",
      "Epoch: 825, Loss: 15.983\n",
      "Epoch: 826, Loss: 11.687\n",
      "Epoch: 827, Loss: 12.970\n",
      "Epoch: 828, Loss: 13.407\n",
      "Epoch: 829, Loss: 12.545\n",
      "Epoch: 830, Loss: 14.487\n",
      "Epoch: 831, Loss: 15.211\n",
      "Epoch: 832, Loss: 14.412\n",
      "Epoch: 833, Loss: 14.572\n",
      "Epoch: 834, Loss: 15.421\n",
      "Epoch: 835, Loss: 14.585\n",
      "Epoch: 836, Loss: 12.870\n",
      "Epoch: 837, Loss: 10.199\n",
      "Epoch: 838, Loss: 12.802\n",
      "Epoch: 839, Loss: 16.383\n",
      "Epoch: 840, Loss: 13.627\n",
      "Epoch: 841, Loss: 14.780\n",
      "Epoch: 842, Loss: 13.627\n",
      "Epoch: 843, Loss: 12.545\n",
      "Epoch: 844, Loss: 14.409\n",
      "Epoch: 845, Loss: 15.542\n",
      "Epoch: 846, Loss: 11.360\n",
      "Epoch: 847, Loss: 15.754\n",
      "Epoch: 848, Loss: 17.024\n",
      "Epoch: 849, Loss: 14.849\n",
      "Epoch: 850, Loss: 13.241\n",
      "Epoch: 851, Loss: 12.104\n",
      "Epoch: 852, Loss: 16.475\n",
      "Epoch: 853, Loss: 13.681\n",
      "Epoch: 854, Loss: 14.807\n",
      "Epoch: 855, Loss: 14.684\n",
      "Epoch: 856, Loss: 13.826\n",
      "Epoch: 857, Loss: 15.497\n",
      "Epoch: 858, Loss: 16.527\n",
      "Epoch: 859, Loss: 11.789\n",
      "Epoch: 860, Loss: 14.085\n",
      "Epoch: 861, Loss: 13.135\n",
      "Epoch: 862, Loss: 14.918\n",
      "Epoch: 863, Loss: 11.647\n",
      "Epoch: 864, Loss: 15.590\n",
      "Epoch: 865, Loss: 13.189\n",
      "Epoch: 866, Loss: 16.268\n",
      "Epoch: 867, Loss: 12.882\n",
      "Epoch: 868, Loss: 15.476\n",
      "Epoch: 869, Loss: 10.609\n",
      "Epoch: 870, Loss: 12.230\n",
      "Epoch: 871, Loss: 11.826\n",
      "Epoch: 872, Loss: 15.557\n",
      "Epoch: 873, Loss: 12.192\n",
      "Epoch: 874, Loss: 16.583\n",
      "Epoch: 875, Loss: 14.015\n",
      "Epoch: 876, Loss: 13.493\n",
      "Epoch: 877, Loss: 13.326\n",
      "Epoch: 878, Loss: 12.720\n",
      "Epoch: 879, Loss: 15.274\n",
      "Epoch: 880, Loss: 15.974\n",
      "Epoch: 881, Loss: 12.180\n",
      "Epoch: 882, Loss: 16.892\n",
      "Epoch: 883, Loss: 11.975\n",
      "Epoch: 884, Loss: 10.520\n",
      "Epoch: 885, Loss: 15.905\n",
      "Epoch: 886, Loss: 13.544\n",
      "Epoch: 887, Loss: 13.694\n",
      "Epoch: 888, Loss: 14.167\n",
      "Epoch: 889, Loss: 12.329\n",
      "Epoch: 890, Loss: 15.319\n",
      "Epoch: 891, Loss: 15.026\n",
      "Epoch: 892, Loss: 14.042\n",
      "Epoch: 893, Loss: 14.349\n",
      "Epoch: 894, Loss: 14.543\n",
      "Epoch: 895, Loss: 15.030\n",
      "Epoch: 896, Loss: 13.356\n",
      "Epoch: 897, Loss: 15.814\n",
      "Epoch: 898, Loss: 12.619\n",
      "Epoch: 899, Loss: 13.263\n",
      "Epoch: 900, Loss: 12.584\n",
      "Epoch: 901, Loss: 13.995\n",
      "Epoch: 902, Loss: 12.599\n",
      "Epoch: 903, Loss: 10.388\n",
      "Epoch: 904, Loss: 14.566\n",
      "Epoch: 905, Loss: 13.263\n",
      "Epoch: 906, Loss: 14.223\n",
      "Epoch: 907, Loss: 15.042\n",
      "Epoch: 908, Loss: 11.749\n",
      "Epoch: 909, Loss: 17.030\n",
      "Epoch: 910, Loss: 13.963\n",
      "Epoch: 911, Loss: 14.289\n",
      "Epoch: 912, Loss: 13.032\n",
      "Epoch: 913, Loss: 15.254\n",
      "Epoch: 914, Loss: 9.923\n",
      "Epoch: 915, Loss: 12.336\n",
      "Epoch: 916, Loss: 10.721\n",
      "Epoch: 917, Loss: 10.347\n",
      "Epoch: 918, Loss: 12.388\n",
      "Epoch: 919, Loss: 13.660\n",
      "Epoch: 920, Loss: 13.229\n",
      "Epoch: 921, Loss: 10.464\n",
      "Epoch: 922, Loss: 14.420\n",
      "Epoch: 923, Loss: 13.166\n",
      "Epoch: 924, Loss: 11.253\n",
      "Epoch: 925, Loss: 14.294\n",
      "Epoch: 926, Loss: 13.491\n",
      "Epoch: 927, Loss: 13.504\n",
      "Epoch: 928, Loss: 11.343\n",
      "Epoch: 929, Loss: 9.985\n",
      "Epoch: 930, Loss: 12.087\n",
      "Epoch: 931, Loss: 12.342\n",
      "Epoch: 932, Loss: 17.108\n",
      "Epoch: 933, Loss: 10.532\n",
      "Epoch: 934, Loss: 13.474\n",
      "Epoch: 935, Loss: 13.697\n",
      "Epoch: 936, Loss: 10.989\n",
      "Epoch: 937, Loss: 16.640\n",
      "Epoch: 938, Loss: 14.175\n",
      "Epoch: 939, Loss: 14.469\n",
      "Epoch: 940, Loss: 12.422\n",
      "Epoch: 941, Loss: 14.809\n",
      "Epoch: 942, Loss: 14.666\n",
      "Epoch: 943, Loss: 11.130\n",
      "Epoch: 944, Loss: 12.821\n",
      "Epoch: 945, Loss: 14.289\n",
      "Epoch: 946, Loss: 13.743\n",
      "Epoch: 947, Loss: 11.096\n",
      "Epoch: 948, Loss: 12.348\n",
      "Epoch: 949, Loss: 11.173\n",
      "Epoch: 950, Loss: 11.814\n",
      "Epoch: 951, Loss: 13.412\n",
      "Epoch: 952, Loss: 13.770\n",
      "Epoch: 953, Loss: 13.995\n",
      "Epoch: 954, Loss: 10.626\n",
      "Epoch: 955, Loss: 14.823\n",
      "Epoch: 956, Loss: 12.431\n",
      "Epoch: 957, Loss: 13.476\n",
      "Epoch: 958, Loss: 13.160\n",
      "Epoch: 959, Loss: 13.773\n",
      "Epoch: 960, Loss: 12.454\n",
      "Epoch: 961, Loss: 11.848\n",
      "Epoch: 962, Loss: 12.825\n",
      "Epoch: 963, Loss: 14.221\n",
      "Epoch: 964, Loss: 14.740\n",
      "Epoch: 965, Loss: 15.362\n",
      "Epoch: 966, Loss: 12.377\n",
      "Epoch: 967, Loss: 10.456\n",
      "Epoch: 968, Loss: 10.639\n",
      "Epoch: 969, Loss: 11.442\n",
      "Epoch: 970, Loss: 13.283\n",
      "Epoch: 971, Loss: 15.230\n",
      "Epoch: 972, Loss: 11.619\n",
      "Epoch: 973, Loss: 12.860\n",
      "Epoch: 974, Loss: 14.803\n",
      "Epoch: 975, Loss: 15.208\n",
      "Epoch: 976, Loss: 12.858\n",
      "Epoch: 977, Loss: 14.073\n",
      "Epoch: 978, Loss: 15.508\n",
      "Epoch: 979, Loss: 11.742\n",
      "Epoch: 980, Loss: 12.122\n",
      "Epoch: 981, Loss: 10.541\n",
      "Epoch: 982, Loss: 10.883\n",
      "Epoch: 983, Loss: 13.929\n",
      "Epoch: 984, Loss: 12.176\n",
      "Epoch: 985, Loss: 11.659\n",
      "Epoch: 986, Loss: 14.464\n",
      "Epoch: 987, Loss: 12.434\n",
      "Epoch: 988, Loss: 16.038\n",
      "Epoch: 989, Loss: 11.841\n",
      "Epoch: 990, Loss: 14.956\n",
      "Epoch: 991, Loss: 13.862\n",
      "Epoch: 992, Loss: 13.405\n",
      "Epoch: 993, Loss: 12.886\n",
      "Epoch: 994, Loss: 13.451\n",
      "Epoch: 995, Loss: 12.163\n",
      "Epoch: 996, Loss: 10.043\n",
      "Epoch: 997, Loss: 10.438\n",
      "Epoch: 998, Loss: 12.585\n",
      "Epoch: 999, Loss: 13.578\n",
      "Epoch: 1000, Loss: 12.725\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Have fun with the number of epochs!\n",
    "\n",
    "Be warned that if you increase them too much,\n",
    "the VM will time out :)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.utils import shuffle, resample\n",
    "#from miniflow import *\n",
    "\n",
    "# Load data\n",
    "data = load_boston()\n",
    "X_ = data['data']\n",
    "y_ = data['target']\n",
    "\n",
    "# Normalize data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "n_features = X_.shape[1]\n",
    "n_hidden = 10\n",
    "W1_ = np.random.randn(n_features, n_hidden)\n",
    "b1_ = np.zeros(n_hidden)\n",
    "W2_ = np.random.randn(n_hidden, 1)\n",
    "b2_ = np.zeros(1)\n",
    "\n",
    "# Neural network\n",
    "X, y = Input(), Input()\n",
    "W1, b1 = Input(), Input()\n",
    "W2, b2 = Input(), Input()\n",
    "\n",
    "l1 = Linear(X, W1, b1)\n",
    "s1 = Sigmoid(l1)\n",
    "l2 = Linear(s1, W2, b2)\n",
    "cost = MSE(y, l2)\n",
    "\n",
    "feed_dict = {\n",
    "    X: X_,\n",
    "    y: y_,\n",
    "    W1: W1_,\n",
    "    b1: b1_,\n",
    "    W2: W2_,\n",
    "    b2: b2_\n",
    "}\n",
    "\n",
    "epochs = 1000\n",
    "# Total number of examples\n",
    "m = X_.shape[0]\n",
    "batch_size = 11\n",
    "steps_per_epoch = m // batch_size\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "trainables = [W1, b1, W2, b2]\n",
    "\n",
    "print(\"Total number of examples = {}\".format(m))\n",
    "\n",
    "# Step 4\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    for j in range(steps_per_epoch):\n",
    "        # Step 1\n",
    "        # Randomly sample a batch of examples\n",
    "        X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
    "\n",
    "        # Reset value of X and y Inputs\n",
    "        X.value = X_batch\n",
    "        y.value = y_batch\n",
    "\n",
    "        # Step 2\n",
    "        forward_and_backward(graph)\n",
    "\n",
    "        # Step 3\n",
    "        sgd_update(trainables)\n",
    "\n",
    "        loss += graph[-1].value\n",
    "\n",
    "    print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
