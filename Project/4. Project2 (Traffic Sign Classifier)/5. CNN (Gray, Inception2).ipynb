{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "from PIL import Image\n",
    "from skimage import exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def saveToPickle(data, file_name=None, folder_path=None):\n",
    "    if not os.path.isdir(folder_path):\n",
    "        #print(\"Create \\\"preprocessed-data\\\" folder\")\n",
    "        os.mkdir(folder_path)\n",
    "    else:\n",
    "        print(\"\\\"preprocessed-data\\\" folder already exist\")\n",
    "\n",
    "    file_name = folder_path + file_name\n",
    "    if not os.path.exists(file_name):\n",
    "        try:\n",
    "            with open(file_name, 'wb') as f:\n",
    "                pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "                print(\"Create\", file_name)\n",
    "        except Exception as e:\n",
    "            print('Error: unable to save data to', file_name, 'because', e)\n",
    "            \n",
    "def loadPickle(file_name=None, folder_path=None):\n",
    "    file = folder_path + file_name\n",
    "    #print('Load')\n",
    "    if os.path.exists(file):\n",
    "        try:\n",
    "            with open(file, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "                print(\"Open\", file)\n",
    "        except Exception as e:\n",
    "            print('Error: unable to open data to', file, 'because', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the loaded processed X train dataset: (86430, 32, 32)\n",
      "The shape of the loaded processed y train dataset: (86430,)\n",
      "The shape of the loaded processed X valid dataset: (10320, 32, 32)\n",
      "The shape of the loaded processed y valid dataset: (10320,)\n",
      "The shape of the loaded processed X test dataset: (32250, 32, 32)\n",
      "The shape of the loaded processed y test dataset: (32250,)\n"
     ]
    }
   ],
   "source": [
    "## Load\n",
    "augmented_X_train = loadPickle(file_name='augmented_X_train.p', folder_path='./preprocessed-data/')\n",
    "augmented_y_train = loadPickle(file_name='augmented_y_train.p', folder_path='./preprocessed-data/')\n",
    "\n",
    "augmented_X_valid = loadPickle(file_name='augmented_X_valid.p', folder_path='./preprocessed-data/')\n",
    "augmented_y_valid = loadPickle(file_name='augmented_y_valid.p', folder_path='./preprocessed-data/')\n",
    "\n",
    "augmented_X_test = loadPickle(file_name='augmented_X_test.p', folder_path='./preprocessed-data/')\n",
    "augmented_y_test = loadPickle(file_name='augmented_y_test.p', folder_path='./preprocessed-data/')\n",
    "\n",
    "print('The shape of the loaded processed X train dataset:', augmented_X_train.shape)\n",
    "print('The shape of the loaded processed y train dataset:', augmented_y_train.shape)\n",
    "\n",
    "print('The shape of the loaded processed X valid dataset:', augmented_X_valid.shape)\n",
    "print('The shape of the loaded processed y valid dataset:', augmented_y_valid.shape)\n",
    "\n",
    "print('The shape of the loaded processed X test dataset:', augmented_X_test.shape)\n",
    "print('The shape of the loaded processed y test dataset:', augmented_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    if batch_size > 0:\n",
    "        assert len(features) == len(labels)\n",
    "\n",
    "        output_batches = []\n",
    "        sample_size = len(features)\n",
    "\n",
    "        for start_i in range(0, sample_size, batch_size):\n",
    "            end_i = start_i + batch_size\n",
    "            batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "            output_batches.append(batch)\n",
    "    else:\n",
    "        assert len(features) == len(labels)\n",
    "        \n",
    "        output_batches = []\n",
    "        sample_size = len(features)\n",
    "        \n",
    "        for start in range(sample_size):\n",
    "            batch = [features[start], labels[start]]\n",
    "            output_batches.append(batch)\n",
    "            \n",
    "    return output_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def InceptionModule(x, inputDepth):\n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # 1x1 convolution\n",
    "    a_conv_W_1x1 = tf.Variable(tf.truncated_normal(shape=(1, 1, inputDepth, 16), mean = mu, stddev = sigma))\n",
    "    a_conv_b_1x1 = tf.Variable(tf.zeros(16))\n",
    "    a_conv_1x1   = tf.nn.conv2d(x, a_conv_W_1x1, strides=[1, 1, 1, 1], padding='SAME') + a_conv_b_1x1\n",
    "    print('a_conv_1x1:', np.shape(a_conv_1x1))\n",
    "    \n",
    "    # 3x3 convolution after 1x1 convolution\n",
    "    b_conv_W_3x3 = tf.Variable(tf.truncated_normal(shape=(3, 3, 16, 32), mean = mu, stddev = sigma))\n",
    "    b_conv_b_3x3 = tf.Variable(tf.zeros(32))\n",
    "    b_conv_3x3   = tf.nn.conv2d(a_conv_1x1, b_conv_W_3x3, strides=[1, 1, 1, 1], padding='SAME') + b_conv_b_3x3\n",
    "    print('b_conv_3x3:', np.shape(b_conv_3x3))\n",
    "    \n",
    "    # 5x5 convolution after 1x1 convolution\n",
    "    c_conv_W_5x5 = tf.Variable(tf.truncated_normal(shape=(5, 5, 16, 64), mean = mu, stddev = sigma))\n",
    "    c_conv_b_5x5 = tf.Variable(tf.zeros(64))\n",
    "    c_conv_5x5   = tf.nn.conv2d(a_conv_1x1, c_conv_W_5x5, strides=[1, 1, 1, 1], padding='SAME') + c_conv_b_5x5\n",
    "    print('c_conv_5x5:', np.shape(c_conv_5x5))\n",
    "    \n",
    "    # Pooling\n",
    "    d_pooling = tf.nn.max_pool(x, ksize=[1, 3, 3, 1], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    print('d_Pooling:', np.shape(d_pooling))\n",
    "    \n",
    "    # 1x1 convolution after Pooling\n",
    "    d_conv_W_1x1 = tf.Variable(tf.truncated_normal(shape=(1, 1, inputDepth, 12), mean = mu, stddev = sigma))\n",
    "    d_conv_b_1x1 = tf.Variable(tf.zeros(12))\n",
    "    d_conv_1x1   = tf.nn.conv2d(d_pooling, d_conv_W_1x1, strides=[1, 1, 1, 1], padding='SAME') + d_conv_b_1x1\n",
    "    print('d_conv_1x1:', np.shape(d_conv_1x1))\n",
    "    \n",
    "    inceptionOutput = tf.concat([a_conv_1x1, b_conv_3x3, c_conv_5x5, d_conv_1x1], 3)\n",
    "    #print('Inception Output:', np.shape(inceptionOutput))\n",
    "    return inceptionOutput\n",
    "\n",
    "def InceptionModule2(x, inputDepth):\n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # 1x1 convolution\n",
    "    a_conv_W_1x1 = tf.Variable(tf.truncated_normal(shape=(1, 1, inputDepth, 128), mean = mu, stddev = sigma))\n",
    "    a_conv_b_1x1 = tf.Variable(tf.zeros(128))\n",
    "    a_conv_1x1   = tf.nn.conv2d(x, a_conv_W_1x1, strides=[1, 1, 1, 1], padding='SAME') + a_conv_b_1x1\n",
    "    print('a_conv_1x1:', np.shape(a_conv_1x1))\n",
    "    \n",
    "    # 3x3 convolution after 1x1 convolution\n",
    "    b_conv_W_3x3 = tf.Variable(tf.truncated_normal(shape=(3, 3, 128, 230), mean = mu, stddev = sigma))\n",
    "    b_conv_b_3x3 = tf.Variable(tf.zeros(230))\n",
    "    b_conv_3x3   = tf.nn.conv2d(a_conv_1x1, b_conv_W_3x3, strides=[1, 1, 1, 1], padding='SAME') + b_conv_b_3x3\n",
    "    print('b_conv_3x3:', np.shape(b_conv_3x3))\n",
    "    \n",
    "    # 5x5 convolution after 1x1 convolution\n",
    "    c_conv_W_5x5 = tf.Variable(tf.truncated_normal(shape=(5, 5, 128, 230), mean = mu, stddev = sigma))\n",
    "    c_conv_b_5x5 = tf.Variable(tf.zeros(230))\n",
    "    c_conv_5x5   = tf.nn.conv2d(a_conv_1x1, c_conv_W_5x5, strides=[1, 1, 1, 1], padding='SAME') + c_conv_b_5x5\n",
    "    print('c_conv_5x5:', np.shape(c_conv_5x5))\n",
    "    \n",
    "    # Pooling\n",
    "    d_pooling = tf.nn.max_pool(x, ksize=[1, 3, 3, 1], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    print('d_Pooling:', np.shape(d_pooling))\n",
    "    \n",
    "    # 1x1 convolution after Pooling\n",
    "    d_conv_W_1x1 = tf.Variable(tf.truncated_normal(shape=(1, 1, inputDepth, 12), mean = mu, stddev = sigma))\n",
    "    d_conv_b_1x1 = tf.Variable(tf.zeros(12))\n",
    "    d_conv_1x1   = tf.nn.conv2d(d_pooling, d_conv_W_1x1, strides=[1, 1, 1, 1], padding='SAME') + d_conv_b_1x1\n",
    "    print('d_conv_1x1:', np.shape(d_conv_1x1))\n",
    "    \n",
    "    inceptionOutput = tf.concat([a_conv_1x1, b_conv_3x3, c_conv_5x5, d_conv_1x1], 3)\n",
    "    print('Inception Output:', np.shape(inceptionOutput))\n",
    "    return inceptionOutput\n",
    "\n",
    "def Model(x):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    ## Layer 1: Convolutional.##\n",
    "    # Input = 32x32x1. Output = output 30x30x6\n",
    "    L1_conv_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 1, 6), mean = mu, stddev = sigma))\n",
    "    L1_conv_b = tf.Variable(tf.zeros(6))\n",
    "    L1_conv   = tf.nn.conv2d(x, L1_conv_W, strides=[1, 1, 1, 1], padding='VALID') + L1_conv_b\n",
    "    # Activation.\n",
    "    L1_conv = tf.nn.relu(L1_conv)\n",
    "    # conv = tf.nn.dropout(conv, 0.7)\n",
    "    print('L1_conv:', np.shape(L1_conv))\n",
    "    \n",
    "    ## Layer 2 : Convolutional.##\n",
    "    # Input = 30x30x6. Output = 28x28x16.\n",
    "    L2_conv_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 6, 16), mean = mu, stddev = sigma))\n",
    "    L2_conv_b = tf.Variable(tf.zeros(16))\n",
    "    L2_conv   = tf.nn.conv2d(L1_conv, L2_conv_W, strides=[1, 1, 1, 1], padding='VALID') + L2_conv_b\n",
    "    # Activation.\n",
    "    L2_conv = tf.nn.relu(L2_conv)\n",
    "    print('L2_conv:', np.shape(L2_conv))\n",
    "    #conv1 = tf.nn.dropout(conv1, 0.7)\n",
    "    # Pooling. Input = 28x28x16. Output = 14x14x16.\n",
    "    L2_conv = tf.nn.max_pool(L2_conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    print('L2_pool:', np.shape(L2_conv))\n",
    "    L2_depth = np.shape(L2_conv)[3].value\n",
    "    #print('L2:', type(L2_depth))\n",
    "    \n",
    "    ## Layer 3: Inception.##\n",
    "    # Output = 10x10x64.\n",
    "    L3_inception = InceptionModule(L2_conv, L2_depth)\n",
    "    # Activation.\n",
    "    L3_inception = tf.nn.relu(L3_inception)\n",
    "    L3_inception = tf.nn.dropout(L3_inception, 0.5)\n",
    "    print('L3, Inception:', np.shape(L3_inception))\n",
    "    \n",
    "    ## Layer 4: 3x3 conv ##\n",
    "    # Input = 14x14x108. Output = 14x14x64.\n",
    "    L4_conv_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 124, 64), mean = mu, stddev = sigma))\n",
    "    L4_conv_b = tf.Variable(tf.zeros(64))\n",
    "    L4_conv   = tf.nn.conv2d(L3_inception, L4_conv_W, strides=[1, 1, 1, 1], padding='VALID') + L4_conv_b\n",
    "    L4_depth = np.shape(L4_conv)[3].value\n",
    "    print('L4, 3x3 conv:', np.shape(L4_conv), L4_depth)\n",
    "\n",
    "    ## Layer 5: Inception ##\n",
    "    # Input = 12x12x108. Output = 12x12x64.\n",
    "    L5_inception = InceptionModule2(L4_conv, L4_depth)\n",
    "    print('L5, Inception:', np.shape(L5_inception))\n",
    "    # Activation\n",
    "    L5_conv = tf.nn.relu(L5_inception)\n",
    "    L5_conv = tf.nn.dropout(L5_conv, 0.5)\n",
    "    # Pooling. Input = 12x12x320. Output = 3x3x320.\n",
    "    L5_conv = tf.nn.max_pool(L5_inception, ksize=[1, 3, 3, 1], strides=[1, 3, 3, 1], padding='VALID')\n",
    "    print('L5, pool:', np.shape(L5_conv))\n",
    "\n",
    "    ## Layer 6: 1x1 conv ##\n",
    "    # Input = 14x14x108. Output = 14x14x64.\n",
    "    L6_conv_W = tf.Variable(tf.truncated_normal(shape=(1, 1, 600, 130), mean = mu, stddev = sigma))\n",
    "    L6_conv_b = tf.Variable(tf.zeros(130))\n",
    "    L6_conv   = tf.nn.conv2d(L5_conv, L6_conv_W, strides=[1, 1, 1, 1], padding='SAME') + L6_conv_b\n",
    "    print('L6, 1x1 conv:', np.shape(L6_conv))\n",
    "    \n",
    "    # Flatten. Input = 3x3x320. Output = 2880.\n",
    "    fc0   = flatten(L6_conv)\n",
    "    \n",
    "    ## Layer 7: Fully Connected ## \n",
    "    # Input = 2880. Output = 1200.\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(2080, 1200), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(1200))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, 0.6)\n",
    "\n",
    "    ## Layer 8: Fully Connected##\n",
    "    # Input = 1200. Output = 430.\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(1200, 430), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(430))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "    fc2 = tf.nn.dropout(fc2, 0.5)\n",
    "\n",
    "    # Layer 9: Fully Connected. Input = 430. Output = 43.\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(430, 43), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(43))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train, Validate and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rate = 0.001\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Train your model here.\n",
    "### Calculate and report the accuracy on the training and validation set.\n",
    "### Once a final model architecture is selected, \n",
    "### the accuracy on the test set should be calculated and reported as well.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "## Features and Labels\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1_conv: (?, 30, 30, 6)\n",
      "L2_conv: (?, 28, 28, 16)\n",
      "L2_pool: (?, 14, 14, 16)\n",
      "a_conv_1x1: (?, 14, 14, 16)\n",
      "b_conv_3x3: (?, 14, 14, 32)\n",
      "c_conv_5x5: (?, 14, 14, 64)\n",
      "d_Pooling: (?, 14, 14, 16)\n",
      "d_conv_1x1: (?, 14, 14, 12)\n",
      "L3, Inception: (?, 14, 14, 124)\n",
      "L4, 3x3 conv: (?, 12, 12, 64) 64\n",
      "a_conv_1x1: (?, 12, 12, 128)\n",
      "b_conv_3x3: (?, 12, 12, 230)\n",
      "c_conv_5x5: (?, 12, 12, 230)\n",
      "d_Pooling: (?, 12, 12, 64)\n",
      "d_conv_1x1: (?, 12, 12, 12)\n",
      "Inception Output: (?, 12, 12, 600)\n",
      "L5, Inception: (?, 12, 12, 600)\n",
      "L5, pool: (?, 4, 4, 600)\n",
      "L6, 1x1 conv: (?, 4, 4, 130)\n"
     ]
    }
   ],
   "source": [
    "## Training Pipeline\n",
    "#logits =  LeNet(x)\n",
    "logits = Model(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Model Evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data, batch_size, sess):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "\n",
    "    for batch_x, batch_y in tqdm(batches(batch_size, X_data, y_data)):\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "## Reshape dataset\n",
    "augmented_X_train = np.reshape(augmented_X_train, (len(augmented_X_train), 32, 32, 1))\n",
    "augmented_X_valid = np.reshape(augmented_X_valid, (len(augmented_X_valid), 32, 32, 1))\n",
    "augmented_X_test = np.reshape(augmented_X_test, (len(augmented_X_test), 32, 32, 1))\n",
    "print(augmented_X_train[:128].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/676 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:24<00:00,  6.14it/s]\n",
      "100%|██████████| 676/676 [00:24<00:00, 28.08it/s]\n",
      "100%|██████████| 81/81 [00:03<00:00, 19.57it/s]\n",
      "100%|██████████| 252/252 [00:09<00:00, 17.02it/s]\n",
      "  0%|          | 1/676 [00:00<01:20,  8.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 ...\n",
      "Train Accuracy = 0.615\n",
      "Validation Accuracy = 0.533\n",
      "Test Accuracy = 0.247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:18<00:00,  8.61it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.70it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.74it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 28.49it/s]\n",
      "  0%|          | 1/676 [00:00<01:22,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 2 ...\n",
      "Train Accuracy = 0.822\n",
      "Validation Accuracy = 0.733\n",
      "Test Accuracy = 0.312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:17<00:00,  8.76it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.67it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.83it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 28.58it/s]\n",
      "  0%|          | 1/676 [00:00<01:19,  8.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 3 ...\n",
      "Train Accuracy = 0.895\n",
      "Validation Accuracy = 0.812\n",
      "Test Accuracy = 0.356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:17<00:00,  8.75it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.64it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.64it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 28.84it/s]\n",
      "  0%|          | 1/676 [00:00<01:16,  8.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 4 ...\n",
      "Train Accuracy = 0.919\n",
      "Validation Accuracy = 0.844\n",
      "Test Accuracy = 0.358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:17<00:00,  8.68it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.58it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.77it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 28.59it/s]\n",
      "  0%|          | 1/676 [00:00<01:17,  8.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 5 ...\n",
      "Train Accuracy = 0.914\n",
      "Validation Accuracy = 0.843\n",
      "Test Accuracy = 0.370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:17<00:00,  8.74it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.65it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.68it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 28.49it/s]\n",
      "  0%|          | 1/676 [00:00<01:16,  8.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 6 ...\n",
      "Train Accuracy = 0.939\n",
      "Validation Accuracy = 0.879\n",
      "Test Accuracy = 0.384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:18<00:00,  8.60it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.36it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.10it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 28.64it/s]\n",
      "  0%|          | 1/676 [00:00<01:18,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 7 ...\n",
      "Train Accuracy = 0.941\n",
      "Validation Accuracy = 0.875\n",
      "Test Accuracy = 0.385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:17<00:00,  8.71it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.27it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.46it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 28.51it/s]\n",
      "  0%|          | 1/676 [00:00<01:20,  8.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 8 ...\n",
      "Train Accuracy = 0.943\n",
      "Validation Accuracy = 0.880\n",
      "Test Accuracy = 0.388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:17<00:00,  8.69it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.65it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.50it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 28.40it/s]\n",
      "  0%|          | 1/676 [00:00<01:16,  8.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 9 ...\n",
      "Train Accuracy = 0.954\n",
      "Validation Accuracy = 0.895\n",
      "Test Accuracy = 0.402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:18<00:00,  8.63it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.35it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.65it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 28.48it/s]\n",
      "  0%|          | 1/676 [00:00<01:20,  8.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10 ...\n",
      "Train Accuracy = 0.936\n",
      "Validation Accuracy = 0.877\n",
      "Test Accuracy = 0.380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:18<00:00,  8.65it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.33it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.76it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 28.90it/s]\n",
      "  0%|          | 1/676 [00:00<01:16,  8.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 11 ...\n",
      "Train Accuracy = 0.951\n",
      "Validation Accuracy = 0.891\n",
      "Test Accuracy = 0.393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:18<00:00,  8.57it/s]\n",
      "100%|██████████| 676/676 [00:24<00:00, 27.65it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.36it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 27.36it/s]\n",
      "  0%|          | 1/676 [00:00<01:17,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 12 ...\n",
      "Train Accuracy = 0.941\n",
      "Validation Accuracy = 0.875\n",
      "Test Accuracy = 0.378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:17<00:00,  8.72it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.63it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.65it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 28.88it/s]\n",
      "  0%|          | 1/676 [00:00<01:18,  8.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 13 ...\n",
      "Train Accuracy = 0.958\n",
      "Validation Accuracy = 0.903\n",
      "Test Accuracy = 0.414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:17<00:00,  8.71it/s]\n",
      "100%|██████████| 676/676 [00:24<00:00, 28.12it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.26it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 28.41it/s]\n",
      "  0%|          | 1/676 [00:00<01:17,  8.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 14 ...\n",
      "Train Accuracy = 0.951\n",
      "Validation Accuracy = 0.892\n",
      "Test Accuracy = 0.388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:18<00:00,  8.59it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.58it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.70it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 28.31it/s]\n",
      "  0%|          | 1/676 [00:00<01:20,  8.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 15 ...\n",
      "Train Accuracy = 0.961\n",
      "Validation Accuracy = 0.908\n",
      "Test Accuracy = 0.412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:17<00:00,  8.68it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.60it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.62it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 28.35it/s]\n",
      "  0%|          | 1/676 [00:00<01:16,  8.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 16 ...\n",
      "Train Accuracy = 0.957\n",
      "Validation Accuracy = 0.903\n",
      "Test Accuracy = 0.407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:17<00:00,  8.73it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.65it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.82it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 28.88it/s]\n",
      "  0%|          | 1/676 [00:00<01:16,  8.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 17 ...\n",
      "Train Accuracy = 0.961\n",
      "Validation Accuracy = 0.905\n",
      "Test Accuracy = 0.405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:17<00:00,  8.75it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.46it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.02it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 28.80it/s]\n",
      "  0%|          | 1/676 [00:00<01:17,  8.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 18 ...\n",
      "Train Accuracy = 0.962\n",
      "Validation Accuracy = 0.909\n",
      "Test Accuracy = 0.413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:18<00:00,  8.57it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.37it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.52it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 28.54it/s]\n",
      "  0%|          | 1/676 [00:00<01:16,  8.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 19 ...\n",
      "Train Accuracy = 0.966\n",
      "Validation Accuracy = 0.912\n",
      "Test Accuracy = 0.412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:17<00:00,  8.69it/s]\n",
      "100%|██████████| 676/676 [00:24<00:00, 28.10it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.64it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 28.33it/s]\n",
      "  0%|          | 1/676 [00:00<01:20,  8.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 20 ...\n",
      "Train Accuracy = 0.956\n",
      "Validation Accuracy = 0.907\n",
      "Test Accuracy = 0.388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:17<00:00,  8.72it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.66it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.68it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 28.72it/s]\n",
      "  0%|          | 1/676 [00:00<01:16,  8.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 21 ...\n",
      "Train Accuracy = 0.957\n",
      "Validation Accuracy = 0.901\n",
      "Test Accuracy = 0.391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:17<00:00,  8.73it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.62it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.62it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 28.60it/s]\n",
      "  0%|          | 1/676 [00:00<01:16,  8.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 22 ...\n",
      "Train Accuracy = 0.960\n",
      "Validation Accuracy = 0.904\n",
      "Test Accuracy = 0.385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:17<00:00,  8.74it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.65it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.74it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 29.00it/s]\n",
      "  0%|          | 1/676 [00:00<01:18,  8.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 23 ...\n",
      "Train Accuracy = 0.959\n",
      "Validation Accuracy = 0.900\n",
      "Test Accuracy = 0.413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:17<00:00,  8.68it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.73it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.86it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 29.00it/s]\n",
      "  0%|          | 1/676 [00:00<01:17,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 24 ...\n",
      "Train Accuracy = 0.949\n",
      "Validation Accuracy = 0.894\n",
      "Test Accuracy = 0.385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:17<00:00,  8.76it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.68it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.86it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 28.94it/s]\n",
      "  0%|          | 1/676 [00:00<01:16,  8.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 25 ...\n",
      "Train Accuracy = 0.967\n",
      "Validation Accuracy = 0.909\n",
      "Test Accuracy = 0.412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:17<00:00,  8.76it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.72it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.87it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 29.01it/s]\n",
      "  0%|          | 1/676 [00:00<01:16,  8.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 26 ...\n",
      "Train Accuracy = 0.963\n",
      "Validation Accuracy = 0.905\n",
      "Test Accuracy = 0.402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:17<00:00,  8.75it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.71it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.85it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 28.59it/s]\n",
      "  0%|          | 1/676 [00:00<01:16,  8.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 27 ...\n",
      "Train Accuracy = 0.936\n",
      "Validation Accuracy = 0.875\n",
      "Test Accuracy = 0.365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:17<00:00,  8.76it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.70it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.77it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 28.80it/s]\n",
      "  0%|          | 1/676 [00:00<01:16,  8.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 28 ...\n",
      "Train Accuracy = 0.962\n",
      "Validation Accuracy = 0.915\n",
      "Test Accuracy = 0.411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:17<00:00,  8.76it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.71it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.75it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 28.88it/s]\n",
      "  0%|          | 1/676 [00:00<01:16,  8.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 29 ...\n",
      "Train Accuracy = 0.965\n",
      "Validation Accuracy = 0.919\n",
      "Test Accuracy = 0.415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 676/676 [01:17<00:00,  8.75it/s]\n",
      "100%|██████████| 676/676 [00:23<00:00, 28.69it/s]\n",
      "100%|██████████| 81/81 [00:02<00:00, 28.79it/s]\n",
      "100%|██████████| 252/252 [00:08<00:00, 29.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 30 ...\n",
      "Train Accuracy = 0.969\n",
      "Validation Accuracy = 0.924\n",
      "Test Accuracy = 0.419\n",
      "CPU times: user 40min 14s, sys: 6min 31s, total: 46min 45s\n",
      "Wall time: 56min 49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Train the Model\n",
    "\n",
    "#accuracyRate = []\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf. global_variables_initializer())\n",
    "    num_examples = len(augmented_X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    #print()\n",
    "    for i in range(EPOCHS):\n",
    "        shuffle_X, shuffle_y = shuffle(augmented_X_train, augmented_y_train)\n",
    "        for batch_x, batch_y in tqdm(batches(BATCH_SIZE, shuffle_X, shuffle_y)):\n",
    "            session.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        \n",
    "        train_accuracy = evaluate(augmented_X_train, augmented_y_train, BATCH_SIZE, session)\n",
    "        validation_accuracy = evaluate(augmented_X_valid, augmented_y_valid, BATCH_SIZE, session)\n",
    "        test_accuracy = evaluate(augmented_X_test, augmented_y_test, BATCH_SIZE, session)\n",
    "        \n",
    "        print(\"EPOCH {0} ...\".format(i+1))\n",
    "        print('Train Accuracy = {:.3f}'.format(train_accuracy))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver.save(session, './IC2')\n",
    "print(\"Model Saved\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
