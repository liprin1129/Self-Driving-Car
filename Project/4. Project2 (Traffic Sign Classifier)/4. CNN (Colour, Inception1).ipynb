{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "from PIL import Image\n",
    "from skimage import exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def saveToPickle(data, file_name=None, folder_path=None):\n",
    "    if not os.path.isdir(folder_path):\n",
    "        #print(\"Create \\\"preprocessed-data\\\" folder\")\n",
    "        os.mkdir(folder_path)\n",
    "    else:\n",
    "        print(\"\\\"preprocessed-data\\\" folder already exist\")\n",
    "\n",
    "    file_name = folder_path + file_name\n",
    "    if not os.path.exists(file_name):\n",
    "        try:\n",
    "            with open(file_name, 'wb') as f:\n",
    "                pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "                print(\"Create\", file_name)\n",
    "        except Exception as e:\n",
    "            print('Error: unable to save data to', file_name, 'because', e)\n",
    "            \n",
    "def loadPickle(file_name=None, folder_path=None):\n",
    "    file = folder_path + file_name\n",
    "    #print('Load')\n",
    "    if os.path.exists(file):\n",
    "        try:\n",
    "            with open(file, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "                print(\"Open\", file)\n",
    "        except Exception as e:\n",
    "            print('Error: unable to open data to', file, 'because', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Load\n",
    "augmented_X_train = loadPickle(file_name='augmented_X_train.p', folder_path='./preprocessed-data/')\n",
    "augmented_y_train = loadPickle(file_name='augmented_y_train.p', folder_path='./preprocessed-data/')\n",
    "\n",
    "augmented_X_valid = loadPickle(file_name='augmented_X_valid.p', folder_path='./preprocessed-data/')\n",
    "augmented_y_valid = loadPickle(file_name='augmented_y_valid.p', folder_path='./preprocessed-data/')\n",
    "\n",
    "augmented_X_test = loadPickle(file_name='augmented_X_test.p', folder_path='./preprocessed-data/')\n",
    "augmented_y_test = loadPickle(file_name='augmented_y_test.p', folder_path='./preprocessed-data/')\n",
    "\n",
    "print('The shape of the loaded processed X train dataset:', augmented_X_train.shape)\n",
    "print('The shape of the loaded processed y train dataset:', augmented_y_train.shape)\n",
    "\n",
    "print('The shape of the loaded processed X valid dataset:', augmented_X_valid.shape)\n",
    "print('The shape of the loaded processed y valid dataset:', augmented_y_valid.shape)\n",
    "\n",
    "print('The shape of the loaded processed X test dataset:', augmented_X_test.shape)\n",
    "print('The shape of the loaded processed y test dataset:', augmented_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(augmented_X_train[7806], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(augmented_X_valid[2306], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    if batch_size > 0:\n",
    "        assert len(features) == len(labels)\n",
    "\n",
    "        output_batches = []\n",
    "        sample_size = len(features)\n",
    "\n",
    "        for start_i in range(0, sample_size, batch_size):\n",
    "            end_i = start_i + batch_size\n",
    "            batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "            output_batches.append(batch)\n",
    "    else:\n",
    "        assert len(features) == len(labels)\n",
    "        \n",
    "        output_batches = []\n",
    "        sample_size = len(features)\n",
    "        \n",
    "        for start in range(sample_size):\n",
    "            batch = [features[start], labels[start]]\n",
    "            output_batches.append(batch)\n",
    "            \n",
    "    return output_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def InceptionModule(x, inputDepth):\n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # 1x1 convolution\n",
    "    a_conv_W_1x1 = tf.Variable(tf.truncated_normal(shape=(1, 1, inputDepth, 6), mean = mu, stddev = sigma))\n",
    "    a_conv_b_1x1 = tf.Variable(tf.zeros(6))\n",
    "    a_conv_1x1   = tf.nn.conv2d(x, a_conv_W_1x1, strides=[1, 1, 1, 1], padding='SAME') + a_conv_b_1x1\n",
    "    print('a_conv_1x1:', np.shape(a_conv_1x1))\n",
    "    \n",
    "    # 3x3 convolution after 1x1 convolution\n",
    "    b_conv_W_3x3 = tf.Variable(tf.truncated_normal(shape=(3, 3, 6, 32), mean = mu, stddev = sigma))\n",
    "    b_conv_b_3x3 = tf.Variable(tf.zeros(32))\n",
    "    b_conv_3x3   = tf.nn.conv2d(a_conv_1x1, b_conv_W_3x3, strides=[1, 1, 1, 1], padding='SAME') + b_conv_b_3x3\n",
    "    print('b_conv_3x3:', np.shape(b_conv_3x3))\n",
    "    \n",
    "    # 5x5 convolution after 1x1 convolution\n",
    "    c_conv_W_5x5 = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 43), mean = mu, stddev = sigma))\n",
    "    c_conv_b_5x5 = tf.Variable(tf.zeros(43))\n",
    "    c_conv_5x5   = tf.nn.conv2d(a_conv_1x1, c_conv_W_5x5, strides=[1, 1, 1, 1], padding='SAME') + c_conv_b_5x5\n",
    "    print('c_conv_5x5:', np.shape(c_conv_5x5))\n",
    "    \n",
    "    # Pooling\n",
    "    d_pooling = tf.nn.max_pool(x, ksize=[1, 3, 3, 1], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    print('d_Pooling:', np.shape(d_pooling))\n",
    "    \n",
    "    # 1x1 convolution after Pooling\n",
    "    d_conv_W_1x1 = tf.Variable(tf.truncated_normal(shape=(1, 1, inputDepth, 6), mean = mu, stddev = sigma))\n",
    "    d_conv_b_1x1 = tf.Variable(tf.zeros(6))\n",
    "    d_conv_1x1   = tf.nn.conv2d(d_pooling, d_conv_W_1x1, strides=[1, 1, 1, 1], padding='SAME') + d_conv_b_1x1\n",
    "    print('d_conv_1x1:', np.shape(d_conv_1x1))\n",
    "    \n",
    "    inceptionOutput = tf.concat([a_conv_1x1, b_conv_3x3, c_conv_5x5, d_conv_1x1], 3)\n",
    "    print('Inception Output:', np.shape(inceptionOutput))\n",
    "    return inceptionOutput\n",
    "\n",
    "def Model(x):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    ## Layer 1: ##\n",
    "    # Convolutional. Input = 32x32x1. Output = output 30x30x6\n",
    "    L1_conv_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 3, 6), mean = mu, stddev = sigma))\n",
    "    L1_conv_b = tf.Variable(tf.zeros(6))\n",
    "    L1_conv   = tf.nn.conv2d(x, L1_conv_W, strides=[1, 1, 1, 1], padding='VALID') + L1_conv_b\n",
    "    print('L1-1:', np.shape(L1_conv))\n",
    "    \n",
    "    # Activation.\n",
    "    L1_conv = tf.nn.relu(L1_conv)\n",
    "    # conv = tf.nn.dropout(conv, 0.7)\n",
    "    print('L1-2:', np.shape(L1_conv))\n",
    "    \n",
    "    ## Layer 2 ##\n",
    "    # Convolutional. Input = 30x30x6. Output = 28x28x16.\n",
    "    L2_conv_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 6, 16), mean = mu, stddev = sigma))\n",
    "    L2_conv_b = tf.Variable(tf.zeros(16))\n",
    "    L2_conv   = tf.nn.conv2d(L1_conv, L2_conv_W, strides=[1, 1, 1, 1], padding='VALID') + L2_conv_b\n",
    "    print('L2-1:', np.shape(L2_conv))\n",
    "    \n",
    "    # Activation.\n",
    "    L2_conv = tf.nn.relu(L2_conv)\n",
    "    #conv1 = tf.nn.dropout(conv1, 0.7)\n",
    "    print('L2-2:', np.shape(L2_conv))\n",
    "\n",
    "    # Pooling. Input = 28x28x16. Output = 14x14x16.\n",
    "    L2_conv = tf.nn.max_pool(L2_conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    L2_depth = np.shape(L2_conv)[3].value\n",
    "    print('L2-3:', np.shape(L2_conv))\n",
    "    \n",
    "    ## Layer 3: ##\n",
    "    # Inception. Output = 10x10x64.\n",
    "    L3_inception = InceptionModule(L2_conv, L2_depth)\n",
    "    print('L3-1:', np.shape(L3_inception))\n",
    "    \n",
    "    # Activation.\n",
    "    L3_inception = tf.nn.relu(L3_inception)\n",
    "    L3_inception = tf.nn.dropout(L3_inception, 0.5)\n",
    "    print('L3-2:', np.shape(L3_inception))\n",
    "    \n",
    "    ## Layer 4: ##\n",
    "    # Convolutional. Input = 14x14x108. Output = 12x12x320.\n",
    "    L4_conv_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 87, 230), mean = mu, stddev = sigma))\n",
    "    L4_conv_b = tf.Variable(tf.zeros(230))\n",
    "    L4_conv   = tf.nn.conv2d(L3_inception, L4_conv_W, strides=[1, 1, 1, 1], padding='VALID') + L4_conv_b\n",
    "    print('L4-1:', np.shape(L4_conv))\n",
    "    # Activation.\n",
    "    L4_conv = tf.nn.relu(L4_conv)\n",
    "    L4_conv = tf.nn.dropout(L4_conv, 0.7)\n",
    "\n",
    "    # Pooling. Input = 12x12x320. Output = 3x3x320.\n",
    "    L4_conv = tf.nn.max_pool(L4_conv, ksize=[1, 3, 3, 1], strides=[1, 3, 3, 1], padding='VALID')\n",
    "    print('L4-2:', np.shape(L4_conv))\n",
    "\n",
    "    # Flatten. Input = 3x3x320. Output = 2880.\n",
    "    fc0   = flatten(L4_conv)\n",
    "    print('fc0:', np.shape(fc0))\n",
    "    \n",
    "    ## Layer 5: ## \n",
    "    # Fully Connected. Input = 2880. Output = 1200.\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(2070, 1700), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(1700))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    print('fc1:', np.shape(fc0))\n",
    "    \n",
    "    # Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, 0.5)\n",
    "\n",
    "    ## Layer 5: ##\n",
    "    # Fully Connected. Input = 1200. Output = 430.\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(1700, 800), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(800))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "    fc2 = tf.nn.dropout(fc2, 0.5)\n",
    "    print('fc2:', np.shape(fc0))\n",
    "    \n",
    "    # Layer 6: Fully Connected. Input = 430. Output = 43.\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(800, 43), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(43))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train, Validate and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rate = 0.001\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Train your model here.\n",
    "### Calculate and report the accuracy on the training and validation set.\n",
    "### Once a final model architecture is selected, \n",
    "### the accuracy on the test set should be calculated and reported as well.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "## Features and Labels\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Training Pipeline\n",
    "#logits =  LeNet(x)\n",
    "logits = Model(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Model Evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data, batch_size, sess):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "\n",
    "    for batch_x, batch_y in tqdm(batches(batch_size, X_data, y_data)):\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Train the Model\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf. global_variables_initializer())\n",
    "    num_examples = len(augmented_X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    #print()\n",
    "    for i in range(EPOCHS):\n",
    "        shuffle_X, shuffle_y = shuffle(augmented_X_train, augmented_y_train)\n",
    "        for batch_x, batch_y in tqdm(batches(BATCH_SIZE, shuffle_X, shuffle_y)):\n",
    "            session.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        \n",
    "        train_accuracy = evaluate(augmented_X_train, augmented_y_train, BATCH_SIZE, session)\n",
    "        validation_accuracy = evaluate(augmented_X_valid, augmented_y_valid, BATCH_SIZE, session)\n",
    "        test_accuracy = evaluate(augmented_X_test, augmented_y_test, BATCH_SIZE, session)\n",
    "        \n",
    "        print(\"EPOCH {0} ...\".format(i+1))\n",
    "        print('Train Accuracy = {:.3f}'.format(train_accuracy))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print(\"Test Accuracy = {:.3f}\".format(test_accuracy))\n",
    "    \n",
    "    #saver.save(session, './CNN3')\n",
    "    print(\"Model Saved\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
