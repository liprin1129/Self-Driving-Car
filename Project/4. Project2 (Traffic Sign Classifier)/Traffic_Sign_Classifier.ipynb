{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project: Build a Traffic Sign Recognition Classifier\n",
    "\n",
    "In this notebook, a template is provided for you to implement your functionality in stages, which is required to successfully complete this project. If additional code is required that cannot be included in the notebook, be sure that the Python code is successfully imported and included in your submission if necessary. \n",
    "\n",
    "> **Note**: Once you have completed all of the code implementations, you need to finalize your work by exporting the iPython Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission. \n",
    "\n",
    "In addition to implementing code, there is a writeup to complete. The writeup should be completed in a separate file, which can be either a markdown file or a pdf document. There is a [write up template](https://github.com/udacity/CarND-Traffic-Sign-Classifier-Project/blob/master/writeup_template.md) that can be used to guide the writing process. Completing the code template and writeup template will cover all of the [rubric points](https://review.udacity.com/#!/rubrics/481/view) for this project.\n",
    "\n",
    "The [rubric](https://review.udacity.com/#!/rubrics/481/view) contains \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. The stand out suggestions are optional. If you decide to pursue the \"stand out suggestions\", you can include the code in this Ipython notebook and also discuss the results in the writeup file.\n",
    "\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "## Step 0: Load The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import copy\n",
    "from PIL import Image\n",
    "from skimage import exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "filePath = 'traffic-signs-data/'\n",
    "training_file = filePath+'train.p'\n",
    "validation_file= filePath+'valid.p'\n",
    "testing_file = filePath+'test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Summary & Exploration\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the original width and height the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. **THESE COORDINATES ASSUME THE ORIGINAL IMAGE. THE PICKLED DATA CONTAINS RESIZED VERSIONS (32 by 32) OF THESE IMAGES**\n",
    "\n",
    "Complete the basic data summary below. Use python, numpy and/or pandas methods to calculate the data summary rather than hard coding the results. For example, the [pandas shape method](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.shape.html) might be useful for calculating some of the summary results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Provide a Basic Summary of the Data Set Using Python, Numpy and/or Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: x=34799, y=34799\n",
      "Number of validation examples: x=4410, y=4410\n",
      "Number of testing examples:  x=12630, y=12630\n",
      "Image data shape = (32, 32)\n",
      "Number of classes = 43\n"
     ]
    }
   ],
   "source": [
    "### Replace each question mark with the appropriate value. \n",
    "### Use python, pandas or numpy methods rather than hard coding the results\n",
    "\n",
    "# TODO: Number of training examples\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "# TODO: Number of validation examples\n",
    "n_validation = X_valid.shape[0]\n",
    "\n",
    "# TODO: Number of testing examples.\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "# TODO: What's the shape of an traffic sign image?\n",
    "image_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "# TODO: How many unique classes/labels there are in the dataset.\n",
    "labels = pd.read_csv('signnames.csv')\n",
    "n_classes = len(labels)\n",
    "\n",
    "print(\"Number of training examples: x={0}, y={1}\".format(n_train, y_train.shape[0]))\n",
    "print(\"Number of validation examples: x={0}, y={1}\".format(n_validation, y_valid.shape[0]))\n",
    "print(\"Number of testing examples:  x={0}, y={1}\".format(n_test, y_test.shape[0]))\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Include an exploratory visualization of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Visualize the German Traffic Signs Dataset using the pickled file(s). This is open ended, suggestions include: plotting traffic sign images, plotting the count of each sign, etc. \n",
    "\n",
    "The [Matplotlib](http://matplotlib.org/) [examples](http://matplotlib.org/examples/index.html) and [gallery](http://matplotlib.org/gallery.html) pages are a great resource for doing visualizations in Python.\n",
    "\n",
    "**NOTE:** It's recommended you start with something simple first. If you wish to do more, come back to it after you've completed the rest of the sections. It can be interesting to look at the distribution of classes in the training, validation and test set. Is the distribution the same? Are there more examples of some classes than others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_train_df = pd.DataFrame(data=y_train, columns=['ClassId'])\n",
    "y_valid_df = pd.DataFrame(data=y_valid, columns=['ClassId'])\n",
    "y_test_df = pd.DataFrame(data=y_test, columns=['ClassId'])\n",
    "\n",
    "grouped_train_index = y_train_df.groupby('ClassId')\n",
    "grouped_valid_index = y_valid_df.groupby('ClassId')\n",
    "grouped_test_index = y_test_df.groupby('ClassId')\n",
    "\n",
    "## Get the first index images in each class\n",
    "firstTrainDataForEachClass = grouped_train_index.head(1).sort_values('ClassId').reset_index()\n",
    "#print(firstTrainDataForEachClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Plot class ID from index 0 to 9\n",
    "numOfRows = 5\n",
    "numOfColumns = 2\n",
    "\n",
    "ClassIdIndex = 0\n",
    "\n",
    "fig, ax = plt.subplots(numOfRows, numOfColumns, figsize=(20, 20))\n",
    "sns.set_style(style='white')\n",
    "\n",
    "for row in range(numOfRows):\n",
    "    for column in range(numOfColumns):\n",
    "        ax[row][column].imshow(X_train[firstTrainDataForEachClass.iloc[ClassIdIndex].values[0]])\n",
    "        ax[row][column].set_title(labels.loc[ClassIdIndex][1], fontsize=20)\n",
    "        ClassIdIndex += 1\n",
    "        \n",
    "        if ClassIdIndex >= 10:\n",
    "            break\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Plot classes from index 10 to 19\n",
    "numOfRows = 5\n",
    "numOfColumns = 2\n",
    "ClassIdIndex = 10\n",
    "\n",
    "fig, ax = plt.subplots(numOfRows, numOfColumns, figsize=(20, 20))\n",
    "sns.set_style(style='white')\n",
    "\n",
    "for row in range(numOfRows):\n",
    "    for column in range(numOfColumns):\n",
    "        ax[row][column].imshow(X_train[firstTrainDataForEachClass.iloc[ClassIdIndex].values[0]])\n",
    "        ax[row][column].set_title(labels.loc[ClassIdIndex][1], fontsize=20)\n",
    "        ClassIdIndex += 1\n",
    "        \n",
    "        if ClassIdIndex >= 20:\n",
    "            break\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'firstTrainDataForEachClass' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-3e74d6a4c68e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumOfRows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumOfColumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfirstTrainDataForEachClass\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mClassIdIndex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mClassIdIndex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mClassIdIndex\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'firstTrainDataForEachClass' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIYAAARhCAYAAABXro46AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15bd2780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plot classes from index 20 to 29\n",
    "numOfRows = 5\n",
    "numOfColumns = 2\n",
    "ClassIdIndex = 20\n",
    "\n",
    "fig, ax = plt.subplots(numOfRows, numOfColumns, figsize=(20, 20))\n",
    "sns.set_style(style='white')\n",
    "\n",
    "for row in range(numOfRows):\n",
    "    for column in range(numOfColumns):\n",
    "        ax[row][column].imshow(X_train[firstTrainDataForEachClass.iloc[ClassIdIndex].values[0]])\n",
    "        ax[row][column].set_title(labels.loc[ClassIdIndex][1], fontsize=20)\n",
    "        ClassIdIndex += 1\n",
    "        \n",
    "        if ClassIdIndex >= 30:\n",
    "            break\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'firstTrainDataForEachClass' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7b6ffa6a58af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumOfRows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumOfColumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfirstTrainDataForEachClass\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mClassIdIndex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mClassIdIndex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mClassIdIndex\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'firstTrainDataForEachClass' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIYAAARhCAYAAABXro46AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3VFo3fX9P/5XTJra9sQWafDKFFoab7xoU2+GhJVtQWZl\nUDN3YmfqRUF2NRhhzJuGXmiarV4MOh04mG4FbUrxoinUQWylEBTaaCphqKPrcrEbO9ZiczIbw/n8\nL3SHX//9LselPSfv5f143H3O56TnBU8+57x4+jnHlqIoigAAAAAgO/es9AAAAAAArAzFEAAAAECm\nFEMAAAAAmVIMAQAAAGRKMQQAAACQKcUQAAAAQKa+UTF06dKlGBwcvO3xs2fPRn9/f5TL5Thx4sRd\nHw4AIGd2MACg0drqPeF3v/tdnDp1KtatW3fL419++WUcPnw4Tp48GevWrYunn346vvOd78TmzZsb\nNiwAQC7sYABAM9S9Y6irqyuOHj162+OXL1+Orq6u2LhxY7S3t8euXbviwoULDRkSACA3djAAoBnq\nFkOPPfZYtLXdfmPR3NxcdHR01I43bNgQc3Nzd3c6AIBM2cEAgGZY9o9Pl0qlqFQqteNKpXLLkvKf\nFEWx3JcEAMieHQwAuJvq/sbQf7Jt27aYnZ2N69evx/r16+PixYtx4MCBun/X0tISV6/eWO7L0iCd\nnR1ySYxM0iSX9MgkPZ2d9UsKls8Otnp4/0qTXNIjkzTJJT3L3cH+62JofHw85ufno1wux/PPPx8H\nDhyIoiiiv78/HnjggWUNAQDA0uxgAEAjtBQrcF+xVjE92t70yCRNckmPTNLjjqF0uVbS4v0rTXJJ\nj0zSJJf0LHcHW/ZvDAEAAADwv00xBAAAAJApxRAAAABAphRDAAAAAJlSDAEAAABkSjEEAAAAkCnF\nEAAAAECmFEMAAAAAmVIMAQAAAGRKMQQAAACQKcUQAAAAQKYUQwAAAACZUgwBAAAAZEoxBAAAAJAp\nxRAAAABAphRDAAAAAJlSDAEAAABkSjEEAAAAkCnFEAAAAECmFEMAAAAAmVIMAQAAAGRKMQQAAACQ\nKcUQAAAAQKYUQwAAAACZUgwBAAAAZEoxBAAAAJApxRAAAABAphRDAAAAAJlSDAEAAABkSjEEAAAA\nkKm6xVC1Wo3h4eEol8sxODgYs7Ozt5w/depU7N27N/r7++ONN95o2KAAALmwfwEAzdJW7wkTExOx\nsLAQY2NjMT09HaOjo/Hb3/62dv5Xv/pVnD59OtavXx979uyJPXv2xMaNGxs6NADAamb/AgCapW4x\nNDU1Fb29vRERsWPHjpiZmbnl/EMPPRQ3btyItra2KIoiWlpaGjMpAEAm7F8AQLPULYbm5uaiVCrV\njltbW2NxcTHa2r760+3bt0d/f3+sW7cu+vr64r777qv7op2dHXcwMo0il/TIJE1ySY9MWG0asX9F\nuFZSJJM0ySU9MkmTXFaHusVQqVSKSqVSO65Wq7Wl5OOPP45333033nnnnVi/fn38/Oc/jzNnzsT3\nv//9Jf/Nq1dv3OHY3G2dnR1ySYxM0iSX9MgkPZbEO9eI/SvCDpYa719pkkt6ZJImuaRnuTtY3R+f\n7unpifPnz0dExPT0dHR3d9fOdXR0xL333htr166N1tbWuP/+++Pzzz9f1iAAAHzF/gUANEvdO4b6\n+vpicnIyBgYGoiiKGBkZifHx8Zifn49yuRzlcjn27dsXa9asia6urti7d28z5gYAWLXsXwBAs7QU\nRVE0+0XdbpYetwGmRyZpkkt6ZJIeXyVLl2slLd6/0iSX9MgkTXJJT8O+SgYAAADA6qQYAgAAAMiU\nYggAAAAgU4ohAAAAgEwphgAAAAAypRgCAAAAyJRiCAAAACBTiiEAAACATCmGAAAAADKlGAIAAADI\nlGIIAAAAIFOKIQAAAIBMKYYAAAAAMqUYAgAAAMiUYggAAAAgU4ohAAAAgEwphgAAAAAypRgCAAAA\nyJRiCAAAACBTiiEAAACATCmGAAAAADKlGAIAAADIlGIIAAAAIFOKIQAAAIBMKYYAAAAAMqUYAgAA\nAMiUYggAAAAgU4ohAAAAgEwphgAAAAAy1VbvCdVqNQ4dOhSffPJJtLe3xwsvvBBbtmypnf/oo49i\ndHQ0iqKIzs7OOHLkSKxdu7ahQwMArGb2LwCgWereMTQxMRELCwsxNjYWQ0NDMTo6WjtXFEUcPHgw\nDh8+HG+++Wb09vbG3//+94YODACw2tm/AIBmqXvH0NTUVPT29kZExI4dO2JmZqZ27sqVK7Fp06Z4\n/fXX4y9/+Ut8+9vfjq1btzZuWgCADNi/AIBmqXvH0NzcXJRKpdpxa2trLC4uRkTEtWvX4sMPP4xn\nnnkmXnvttXj//ffjvffea9y0AAAZsH8BAM1S946hUqkUlUqldlytVqOt7as/27RpU2zZsiW2bdsW\nERG9vb0xMzMT3/rWt5b8Nzs7O+5kZhpELumRSZrkkh6ZsNo0Yv+KcK2kSCZpkkt6ZJImuawOdYuh\nnp6eOHfuXDz++OMxPT0d3d3dtXMPPvhgVCqVmJ2djS1btsTFixfjhz/8Yd0XvXr1xp1NzV3X2dkh\nl8TIJE1ySY9M0mNJvHON2L8i7GCp8f6VJrmkRyZpkkt6lruD1S2G+vr6YnJyMgYGBqIoihgZGYnx\n8fGYn5+PcrkcL774YgwNDUVRFLFz587YvXv3sgYBAOAr9i8AoFlaiqIomv2iWsX0aHvTI5M0ySU9\nMkmPO4bS5VpJi/evNMklPTJJk1zSs9wdrO6PTwMAAACwOimGAAAAADKlGAIAAADIlGIIAAAAIFOK\nIQAAAIBMKYYAAAAAMqUYAgAAAMiUYggAAAAgU4ohAAAAgEwphgAAAAAypRgCAAAAyJRiCAAAACBT\niiEAAACATCmGAAAAADKlGAIAAADIlGIIAAAAIFOKIQAAAIBMKYYAAAAAMqUYAgAAAMiUYggAAAAg\nU4ohAAAAgEwphgAAAAAypRgCAAAAyJRiCAAAACBTiiEAAACATCmGAAAAADKlGAIAAADIlGIIAAAA\nIFOKIQAAAIBMKYYAAAAAMlW3GKpWqzE8PBzlcjkGBwdjdnb2/3zewYMH46WXXrrrAwIA5Mb+BQA0\nS91iaGJiIhYWFmJsbCyGhoZidHT0tuccP348Pv3004YMCACQG/sXANAsdYuhqamp6O3tjYiIHTt2\nxMzMzC3nP/jgg7h06VKUy+XGTAgAkBn7FwDQLG31njA3NxelUql23NraGouLi9HW1hafffZZvPzy\ny/Gb3/wmzpw5841ftLOzY3nT0lBySY9M0iSX9MiE1aYR+1eEayVFMkmTXNIjkzTJZXWoWwyVSqWo\nVCq142q1Gm1tX/3Z22+/HdeuXYvnnnsurl69Gl988UVs3bo1nnzyySX/zatXb9zh2NxtnZ0dckmM\nTNIkl/TIJD2WxDvXiP0rwg6WGu9faZJLemSSJrmkZ7k7WN1iqKenJ86dOxePP/54TE9PR3d3d+3c\n/v37Y//+/RER8dZbb8Vf//rXb7SUAADwn9m/AIBmqVsM9fX1xeTkZAwMDERRFDEyMhLj4+MxPz/v\ne+0AAA1g/wIAmqWlKIqi2S/qdrP0uA0wPTJJk1zSI5P0+CpZulwrafH+lSa5pEcmaZJLepa7g9X9\nv5IBAAAAsDophgAAAAAypRgCAAAAyJRiCAAAACBTiiEAAACATCmGAAAAADKlGAIAAADIlGIIAAAA\nIFOKIQAAAIBMKYYAAAAAMqUYAgAAAMiUYggAAAAgU4ohAAAAgEwphgAAAAAypRgCAAAAyJRiCAAA\nACBTiiEAAACATCmGAAAAADKlGAIAAADIlGIIAAAAIFOKIQAAAIBMKYYAAAAAMqUYAgAAAMiUYggA\nAAAgU4ohAAAAgEwphgAAAAAypRgCAAAAyJRiCAAAACBTiiEAAACATLXVe0K1Wo1Dhw7FJ598Eu3t\n7fHCCy/Eli1baudPnz4df/jDH6K1tTW6u7vj0KFDcc89+iYAgOWyfwEAzVJ3g5iYmIiFhYUYGxuL\noaGhGB0drZ374osv4te//nX88Y9/jOPHj8fc3FycO3euoQMDAKx29i8AoFnqFkNTU1PR29sbERE7\nduyImZmZ2rn29vY4fvx4rFu3LiIiFhcXY+3atQ0aFQAgD/YvAKBZ6hZDc3NzUSqVasetra2xuLj4\n1R/fc09s3rw5IiKOHTsW8/Pz8eijjzZoVACAPNi/AIBmqfsbQ6VSKSqVSu24Wq1GW1vbLcdHjhyJ\nK1euxNGjR6OlpaXui3Z2dixzXBpJLumRSZrkkh6ZsNo0Yv+KcK2kSCZpkkt6ZJImuawOdYuhnp6e\nOHfuXDz++OMxPT0d3d3dt5wfHh6O9vb2eOWVV77xjx5evXpjedPSMJ2dHXJJjEzSJJf0yCQ9lsQ7\n14j9K8IOlhrvX2mSS3pkkia5pGe5O1jdYqivry8mJydjYGAgiqKIkZGRGB8fj/n5+Xj44Yfj5MmT\n8cgjj8Szzz4bERH79++Pvr6+ZQ0DAID9CwBonpaiKIpmv6hWMT3a3vTIJE1ySY9M0uOOoXS5VtLi\n/StNckmPTNIkl/Qsdwf75vceAwAAALCqKIYAAAAAMqUYAgAAAMiUYggAAAAgU4ohAAAAgEwphgAA\nAAAypRgCAAAAyJRiCAAAACBTiiEAAACATCmGAAAAADKlGAIAAADIlGIIAAAAIFOKIQAAAIBMKYYA\nAAAAMqUYAgAAAMiUYggAAAAgU4ohAAAAgEwphgAAAAAypRgCAAAAyJRiCAAAACBTiiEAAACATCmG\nAAAAADKlGAIAAADIlGIIAAAAIFOKIQAAAIBMKYYAAAAAMqUYAgAAAMiUYggAAAAgU4ohAAAAgEwp\nhgAAAAAyVbcYqlarMTw8HOVyOQYHB2N2dvaW82fPno3+/v4ol8tx4sSJhg0KAJAL+xcA0Cx1i6GJ\niYlYWFiIsbGxGBoaitHR0dq5L7/8Mg4fPhy///3v49ixYzE2Nhb/+Mc/GjowAMBqZ/8CAJqlbjE0\nNTUVvb29ERGxY8eOmJmZqZ27fPlydHV1xcaNG6O9vT127doVFy5caNy0AAAZsH8BAM3SVu8Jc3Nz\nUSqVasetra2xuLgYbW1tMTc3Fx0dHbVzGzZsiLm5ubov2tnZUfc5NJ9c0iOTNMklPTJhtWnE/hXh\nWkmRTNIkl/TIJE1yWR3q3jFUKpWiUqnUjqvVarS1tf2f5yqVyi2LCgAA/z37FwDQLHWLoZ6enjh/\n/nxERExPT0d3d3ft3LZt22J2djauX78eCwsLcfHixdi5c2fjpgUAyID9CwBolpaiKIqlnlCtVuPQ\noUPx6aefRlEUMTIyEn/+859jfn4+yuVynD17Nl5++eUoiiL6+/vjxz/+cbNmBwBYlexfAECz1C2G\nAAAAAFid6n6VDAAAAIDVSTEEAAAAkKmGFUPVajWGh4ejXC7H4OBgzM7O3nL+7Nmz0d/fH+VyOU6c\nONGoMfh/1Mvk9OnT8dRTT8XAwEAMDw9HtVpdoUnzUS+Tfzt48GC89NJLTZ4uX/Vy+eijj2Lfvn3x\n9NNPx09/+tO4efPmCk2aj3qZnDp1Kvbu3Rv9/f3xxhtvrNCU+bp06VIMDg7e9rjP+uazf6XJDpYe\nO1ia7GDpsYOl667uX0WD/OlPfyp+8YtfFEVRFB9++GHxk5/8pHZuYWGh+N73vldcv369uHnzZvHk\nk08WV69ebdQofG2pTP71r38V3/3ud4v5+fmiKIriZz/7WTExMbEic+ZkqUz+7c033yx+9KMfFUeO\nHGn2eNlaKpdqtVr84Ac/KP72t78VRVEUJ06cKC5fvrwic+ak3rXy6KOPFteuXStu3rxZ+3yhOV59\n9dXiiSeeKJ566qlbHvdZvzLsX2myg6XHDpYmO1h67GBputv7V8PuGJqamore3t6IiNixY0fMzMzU\nzl2+fDm6urpi48aN0d7eHrt27YoLFy40ahS+tlQm7e3tcfz48Vi3bl1ERCwuLsbatWtXZM6cLJVJ\nRMQHH3wQly5dinK5vBLjZWupXK5cuRKbNm2K119/PZ555pm4fv16bN26daVGzUa9a+Whhx6KGzdu\nxMLCQhRFES0tLSsxZpa6urri6NGjtz3us35l2L/SZAdLjx0sTXaw9NjB0nS396+GFUNzc3NRKpVq\nx62trbG4uFg719HRUTu3YcOGmJuba9QofG2pTO65557YvHlzREQcO3Ys5ufn49FHH12ROXOyVCaf\nffZZvPzyyzE8PLxS42VrqVyuXbsWH374YTzzzDPx2muvxfvvvx/vvffeSo2ajaUyiYjYvn179Pf3\nx549e2L37t1x3333rcSYWXrssceira3ttsd91q8M+1ea7GDpsYOlyQ6WHjtYmu72/tWwYqhUKkWl\nUqkdV6vV2uD//3OVSuWW4WmMpTL59/Evf/nLmJycjKNHj2p7m2CpTN5+++24du1aPPfcc/Hqq6/G\n6dOn46233lqpUbOyVC6bNm2KLVu2xLZt22LNmjXR29t723854e5bKpOPP/443n333XjnnXfi7Nmz\n8c9//jPOnDmzUqPyNZ/1K8P+lSY7WHrsYGmyg6XHDva/Zbmf9Q0rhnp6euL8+fMRETE9PR3d3d21\nc9u2bYvZ2dm4fv16LCwsxMWLF2Pnzp2NGoWvLZVJRMTw8HDcvHkzXnnlldrtzDTWUpns378/3nrr\nrTh27Fg899xz8cQTT8STTz65UqNmZalcHnzwwahUKrUf3rt48WJs3759RebMyVKZdHR0xL333htr\n166N1tbWuP/+++Pzzz9fqVH5ms/6lWH/SpMdLD12sDTZwdJjB/vfstzP+tvvPbpL+vr6YnJyMgYG\nBqIoihgZGYnx8fGYn5+Pcrkczz//fBw4cCCKooj+/v544IEHGjUKX1sqk4cffjhOnjwZjzzySDz7\n7LMR8dWHYl9f3wpPvbrVu05YGfVyefHFF2NoaCiKooidO3fG7t27V3rkVa9eJuVyOfbt2xdr1qyJ\nrq6u2Lt370qPnC2f9SvL/pUmO1h67GBpsoOlxw72v+FOP+tbiqIomjAnAAAAAIlp2FfJAAAAAEib\nYggAAAAgU4ohAAAAgEwphgAAAAAypRgCAAAAyJRiCAAAACBTiiEAAACATCmGAAAAADKlGAIAAADI\nlGIIAAAAIFOKIQAAAIBMKYYAAAAAMqUYAgAAAMiUYggAAAAgU4ohAAAAgEwphgAAAAAypRgCAAAA\nyJRiCAAAACBTiiEAAACATH2jYujSpUsxODh42+Nnz56N/v7+KJfLceLEibs+HABAzuxgAECjtdV7\nwu9+97s4depUrFu37pbHv/zyyzh8+HCcPHky1q1bF08//XR85zvfic2bNzdsWACAXNjBAIBmqHvH\nUFdXVxw9evS2xy9fvhxdXV2xcePGaG9vj127dsWFCxcaMiQAQG7sYABAM9Qthh577LFoa7v9xqK5\nubno6OioHW/YsCHm5ubu7nQAAJmygwEAzbDsH58ulUpRqVRqx5VK5ZYl5T8pimK5LwkAkD07GABw\nN9X9jaH/ZNu2bTE7OxvXr1+P9evXx8WLF+PAgQN1/66lpSWuXr2x3JelQTo7O+SSGJmkSS7pkUl6\nOjvrlxQsnx1s9fD+lSa5pEcmaZJLepa7g/3XxdD4+HjMz89HuVyO559/Pg4cOBBFUUR/f3888MAD\nyxoCAICl2cEAgEZoKVbgvmKtYnq0vemRSZrkkh6ZpMcdQ+lyraTF+1ea5JIemaRJLulZ7g627N8Y\nAgAAAOB/m2IIAAAAIFOKIQAAAIBMKYYAAAAAMqUYAgAAAMiUYggAAAAgU4ohAAAAgEwphgAAAAAy\npRgCAAAAyJRiCAAAACBTiiEAAACATCmGAAAAADKlGAIAAADIlGIIAAAAIFOKIQAAAIBMKYYAAAAA\nMqUYAgAAAMiUYggAAAAgU4ohAAAAgEwphgAAAAAypRgCAAAAyJRiCAAAACBTiiEAAACATCmGAAAA\nADKlGAIAAADIlGIIAAAAIFOKIQAAAIBMKYYAAAAAMqUYAgAAAMiUYggAAAAgU3WLoWq1GsPDw1Eu\nl2NwcDBmZ2dvOX/q1KnYu3dv9Pf3xxtvvNGwQQEAcmH/AgCapa3eEyYmJmJhYSHGxsZieno6RkdH\n47e//W3t/K9+9as4ffp0rF+/Pvbs2RN79uyJjRs3NnRoAIDVzP4FADRL3WJoamoqent7IyJix44d\nMTMzc8v5hx56KG7cuBFtbW1RFEW0tLQ0ZlIAgEzYvwCAZqlbDM3NzUWpVKodt7a2xuLiYrS1ffWn\n27dvj/7+/li3bl309fXFfffdV/dFOzs77mBkGkUu6ZFJmuSSHpmw2jRi/4pwraRIJmmSS3pkkia5\nrA51i6FSqRSVSqV2XK1Wa0vJxx9/HO+++2688847sX79+vj5z38eZ86cie9///tL/ptXr964w7G5\n2zo7O+SSGJmkSS7pkUl6LIl3rhH7V4QdLDXev9Ikl/TIJE1ySc9yd7C6Pz7d09MT58+fj4iI6enp\n6O7urp3r6OiIe++9N9auXRutra1x//33x+eff76sQQAA+Ir9CwBolrp3DPX19cXk5GQMDAxEURQx\nMjIS4+PjMT8/H+VyOcrlcuzbty/WrFkTXV1dsXfv3mbMDQCwatm/AIBmaSmKomj2i7rdLD1uA0yP\nTNIkl/TIJD2+SpYu10pavH+lSS7pkUma5JKehn2VDAAAAIDVSTEEAAAAkCnFEAAAAECmFEMAAAAA\nmVIMAQAAAGRKMQQAAACQKcUQAAAAQKYUQwAAAACZUgwBAAAAZEoxBAAAAJApxRAAAABAphRDAAAA\nAJlSDAEAAABkSjEEAAAAkCnFEAAAAECmFEMAAAAAmVIMAQAAAGRKMQQAAACQKcUQAAAAQKYUQwAA\nAACZUgwBAAAAZEoxBAAAAJApxRAAAABAphRDAAAAAJlSDAEAAABkSjEEAAAAkCnFEAAAAECmFEMA\nAAAAmVIMAQAAAGSqrd4TqtVqHDp0KD755JNob2+PF154IbZs2VI7/9FHH8Xo6GgURRGdnZ1x5MiR\nWLt2bUOHBgBYzexfAECz1L1jaGJiIhYWFmJsbCyGhoZidHS0dq4oijh48GAcPnw43nzzzejt7Y2/\n//3vDR0YAGC1s38BAM1S946hqamp6O3tjYiIHTt2xMzMTO3clStXYtOmTfH666/HX/7yl/j2t78d\nW7dubdy0AAAZsH8BAM1S946hubm5KJVKtePW1tZYXFyMiIhr167Fhx9+GM8880y89tpr8f7778d7\n773XuGkBADJg/wIAmqXuHUOlUikqlUrtuFqtRlvbV3+2adOm2LJlS2zbti0iInp7e2NmZia+9a1v\nLflvdnZ23MnMNIhc0iOTNMklPTJhtWnE/hXhWkmRTNIkl/TIJE1yWR3qFkM9PT1x7ty5ePzxx2N6\nejq6u7tr5x588MGoVCoxOzsbW7ZsiYsXL8YPf/jDui969eqNO5uau66zs0MuiZFJmuSSHpmkx5J4\n5xqxf0XYwVLj/StNckmPTNIkl/QsdwerWwz19fXF5ORkDAwMRFEUMTIyEuPj4zE/Px/lcjlefPHF\nGBoaiqIoYufOnbF79+5lDQIAwFfsXwBAs7QURVE0+0W1iunR9qZHJmmSS3pkkh53DKXLtZIW719p\nkkt6ZJImuaRnuTtY3R+fBgAAAGB1UgwBAAAAZEoxBAAAAJApxRAAAABAphRDAAAAAJlSDAEAAABk\nSjEEAAAAkCnFEAAAAECmFEMAAAAAmVIMAQAAAGRKMQQAAACQKcUQAAAAQKYUQwAAAACZUgwBAAAA\nZEoxBAAAAJApxRAAAABAphRDAAAAAJlSDAEAAABkSjEEAAAAkCnFEAAAAECmFEMAAAAAmVIMAQAA\nAGRKMQQAAACQKcUQAAAAQKYUQwAAAACZUgwBAAAAZEoxBAAAAJApxRAAAABAphRDAAAAAJlSDAEA\nAABkqm4xVK1WY3h4OMrlcgwODsbs7Oz/+byDBw/GSy+9dNcHBADIjf0LAGiWusXQxMRELCwsxNjY\nWAwNDcXo6Ohtzzl+/Hh8+umnDRkQACA39i8AoFnqFkNTU1PR29sbERE7duyImZmZW85/8MEHcenS\npSiXy42ZEAAgM/YvAKBZ2uo9YW5uLkqlUu24tbU1FhcXo62tLT777LN4+eWX4ze/+U2cOXPmG79o\nZ2fH8qaloeSSHpmkSS7pkQmrTSP2rwjXSopkkia5pEcmaZLL6lC3GCqVSlGpVGrH1Wo12tq++rO3\n3347rl27Fs8991xcvXo1vvjii9i6dWs8+eSTS/6bV6/euMOxuds6OzvkkhiZpEku6ZFJeiyJd64R\n+1eEHSw13r/SJJf0yCRNcknPcnewusVQT09PnDt3Lh5//PGYnp6O7u7u2rn9+/fH/v37IyLirbfe\nir/+9a/faCkBAOA/s38BAM1Stxjq6+uLycnJGBgYiKIoYmRkJMbHx2N+ft732gEAGsD+BQA0S0tR\nFEWzX9TtZulxG2B6ZJImuaRHJunxVbJ0uVbS4v0rTXJJj0zSJJf0LHcHq/t/JQMAAABgdVIMAQAA\nAGRKMQQAAACQKcUQAAAAQKYUQwAAAACZUgwBAAAAZEoxBAAAAJApxRAAAABAphRDAAAAAJlSDAEA\nAABkSjEEAAAAkCnFEAAAAECmFEMAAAAAmVIMAQAAAGRKMQQAAACQKcUQAAAAQKYUQwAAAACZUgwB\nAAAAZEoxBAAAAJApxRAAAABAphRDAAAAAJlSDAEAAABkSjEEAAAAkCnFEAAAAECmFEMAAAAAmVIM\nAQAAAGRKMQQAAACQKcUQAAAAQKYUQwAAAACZaqv3hGq1GocOHYpPPvkk2tvb44UXXogtW7bUzp8+\nfTr+8Ic/RGtra3R3d8ehQ4finnv0TQAAy2X/AgCape4GMTExEQsLCzE2NhZDQ0MxOjpaO/fFF1/E\nr3/96/jjH/8Yx48fj7m5uTh37lxDBwYAWO3sXwBAs9QthqampqK3tzciInbs2BEzMzO1c+3t7XH8\n+PFYt27U0EtMAAAgAElEQVRdREQsLi7G2rVrGzQqAEAe7F8AQLPU/SrZ3NxclEql2nFra2ssLi5G\nW1tb3HPPPbF58+aIiDh27FjMz8/Ho48+WvdFOzs77mBkGkUu6ZFJmuSSHpmw2jRi/4pwraRIJmmS\nS3pkkia5rA51i6FSqRSVSqV2XK1Wo62t7ZbjI0eOxJUrV+Lo0aPR0tJS90WvXr2xzHFplM7ODrkk\nRiZpkkt6ZJIeS+Kda8T+FWEHS433rzTJJT0ySZNc0rPcHazuV8l6enri/PnzERExPT0d3d3dt5wf\nHh6OmzdvxiuvvFK7pRkAgOWzfwEAzVL3jqG+vr6YnJyMgYGBKIoiRkZGYnx8PObn5+Phhx+OkydP\nxiOPPBLPPvtsRETs378/+vr6Gj44AMBqZf8CAJqlpSiKotkv6naz9LgNMD0ySZNc0iOT9PgqWbpc\nK2nx/pUmuaRHJmmSS3oa9lUyAAAAAFYnxRAAAABAphRDAAAAAJlSDAEAAABkSjEEAAAAkCnFEAAA\nAECmFEMAAAAAmVIMAQAAAGRKMQQAAACQKcUQAAAAQKYUQwAAAACZUgwBAAAAZEoxBAAAAJApxRAA\nAABAphRDAAAAAJlSDAEAAABkSjEEAAAAkCnFEAAAAECmFEMAAAAAmVIMAQAAAGRKMQQAAACQKcUQ\nAAAAQKYUQwAAAACZUgwBAAAAZEoxBAAAAJApxRAAAABAphRDAAAAAJlSDAEAAABkSjEEAAAAkCnF\nEAAAAECm6hZD1Wo1hoeHo1wux+DgYMzOzt5y/uzZs9Hf3x/lcjlOnDjRsEEBAHJh/wIAmqVuMTQx\nMRELCwsxNjYWQ0NDMTo6Wjv35ZdfxuHDh+P3v/99HDt2LMbGxuIf//hHQwcGAFjt7F8AQLPULYam\npqait7c3IiJ27NgRMzMztXOXL1+Orq6u2LhxY7S3t8euXbviwoULjZsWACAD9i8AoFna6j1hbm4u\nSqVS7bi1tTUWFxejra0t5ubmoqOjo3Zuw4YNMTc3V/dFOzs76j6H5pNLemSSJrmkRyasNo3YvyJc\nKymSSZrkkh6ZpEkuq0PdO4ZKpVJUKpXacbVajba2tv/zXKVSuWVRAQDgv2f/AgCapW4x1NPTE+fP\nn4+IiOnp6eju7q6d27ZtW8zOzsb169djYWEhLl68GDt37mzctAAAGbB/AQDN0lIURbHUE6rVahw6\ndCg+/fTTKIoiRkZG4s9//nPMz89HuVyOs2fPxssvvxxFUUR/f3/8+Mc/btbsAACrkv0LAGiWusUQ\nAAAAAKtT3a+SAQAAALA6KYYAAAAAMtWwYqharcbw8HCUy+UYHByM2dnZW86fPXs2+vv7o1wux4kT\nJxo1Bv+PepmcPn06nnrqqRgYGIjh4eGoVqsrNGk+6mXybwcPHoyXXnqpydPlq14uH330Uezbty+e\nfvrp+OlPfxo3b95coUnzUS+TU6dOxd69e6O/vz/eeOONFZoyX5cuXYrBwcHbHvdZ33z2rzTZwdJj\nB0uTHSw9drB03dX9q2iQP/3pT8UvfvGLoiiK4sMPPyx+8pOf1M4tLCwU3/ve94rr168XN2/eLJ58\n8sni6tWrjRqFry2Vyb/+9a/iu9/9bjE/P18URVH87Gc/KyYmJlZkzpwslcm/vfnmm8WPfvSj4siR\nI80eL1tL5VKtVosf/OAHxd/+9reiKIrixIkTxeXLl1dkzpzUu1YeffTR4tq1a8XNmzdrny80x6uv\nvlo88cQTxVNPPXXL4z7rV4b9K012sPTYwdJkB0uPHSxNd3v/atgdQ1NTU9Hb2xsRETt27IiZmZna\nucuXL0dXV1ds3Lgx2tvbY9euXXHhwoVGjcLXlsqkvb09jh8/HuvWrYuIiMXFxVi7du2KzJmTpTKJ\niPjggw/i0qVLUS6XV2K8bC2Vy5UrV2LTpk3x+uuvxzPPPBPXr1+PrVu3rtSo2ah3rTz00ENx48aN\nWFhYiKIooqWlZSXGzFJXV1ccPXr0tsd91q8M+1ea7GDpsYOlyQ6WHjtYmu72/tWwYmhubi5KpVLt\nuLW1NRYXF2vnOjo6auc2bNgQc3NzjRqFry2VyT333BObN2+OiIhjx47F/Px8PProoysyZ06WyuSz\nzz6Ll19+OYaHh1dqvGwtlcu1a9fiww8/jGeeeSZee+21eP/99+O9995bqVGzsVQmERHbt2+P/v7+\n2LNnT+zevTvuu+++lRgzS4899li0tbXd9rjP+pVh/0qTHSw9drA02cHSYwdL093evxpWDJVKpahU\nKrXjarVaG/z/f65SqdwyPI2xVCb/Pv7lL38Zk5OTcfToUW1vEyyVydtvvx3Xrl2L5557Ll599dU4\nffp0vPXWWys1alaWymXTpk2xZcuW2LZtW6xZsyZ6e3tv+y8n3H1LZfLxxx/Hu+++G++8806cPXs2\n/vnPf8aZM2dWalS+5rN+Zdi/0mQHS48dLE12sPTYwf63LPezvmHFUE9PT5w/fz4iIqanp6O7u7t2\nbtu2bTE7OxvXr1+PhYWFuHjxYuzcubNRo/C1pTKJiBgeHo6bN2/GK6+8UrudmcZaKpP9+/fHW2+9\nFceOHYvnnnsunnjiiXjyySdXatSsLJXLgw8+GJVKpfbDexcvXozt27evyJw5WSqTjo6OuPfee2Pt\n2rXR2toa999/f3z++ecrNSpf81m/MuxfabKDpccOliY7WHrsYP9blvtZf/u9R3dJX19fTE5OxsDA\nQBRFESMjIzE+Ph7z8/NRLpfj+eefjwMHDkRRFNHf3x8PPPBAo0bha0tl8vDDD8fJkyfjkUceiWef\nfTYivvpQ7OvrW+GpV7d61wkro14uL774YgwNDUVRFLFz587YvXv3So+86tXLpFwux759+2LNmjXR\n1dUVe/fuXemRs+WzfmXZv9JkB0uPHSxNdrD02MH+N9zpZ31LURRFE+YEAAAAIDEN+yoZAAAAAGlT\nDAEAAABkSjEEAAAAkCnFEAAAAECmFEMAAAAAmVIMAQAAAGRKMQQAAACQKcUQAAAAQKYUQwAAAACZ\nUgwBAAAAZEoxBAAAAJApxRAAAABAphRDAAAAAJlSDAEAAABkSjEEAAAAkCnFEAAAAECmFEMAAAAA\nmVIMAQAAAGRKMQQAAACQqW9UDF26dCkGBwdve/zs2bPR398f5XI5Tpw4cdeHAwDImR0MAGi0tnpP\n+N3vfhenTp2KdevW3fL4l19+GYcPH46TJ0/GunXr4umnn47vfOc7sXnz5oYNCwCQCzsYANAMde8Y\n6urqiqNHj972+OXLl6Orqys2btwY7e3tsWvXrrhw4UJDhgQAyI0dDABohrrF0GOPPRZtbbffWDQ3\nNxcdHR214w0bNsTc3FzdFyyK4r8cEQAgP3YwAKAZ6n6V7D8plUpRqVRqx5VK5ZYl5T9paWmJq1dv\nLPdlaZDOzg65JEYmaZJLemSSns7O+vsAy2cHWz28f6VJLumRSZrkkp7l7mDL/r+Sbdu2LWZnZ+P6\n9euxsLAQFy9ejJ07dy73nwMA4BuwgwEAd9N/fcfQ+Ph4zM/PR7lcjueffz4OHDgQRVFEf39/PPDA\nA42YEQAge3YwAKARWooV+MK5283S4zbA9MgkTXJJj0zS46tk6XKtpMX7V5rkkh6ZpEku6Wn6V8kA\nAAAA+N+mGAIAAADIlGIIAAAAIFOKIQAAAIBMKYYAAAAAMqUYAgAAAMiUYggAAAAgU4ohAAAAgEwp\nhgAAAAAypRgCAAAAyJRiCAAAACBTiiEAAACATCmGAAAAADKlGAIAAADIlGIIAAAAIFOKIQAAAIBM\nKYYAAAAAMqUYAgAAAMiUYggAAAAgU4ohAAAAgEwphgAAAAAypRgCAAAAyJRiCAAAACBTiiEAAACA\nTCmGAAAAADKlGAIAAADIlGIIAAAAIFOKIQAAAIBMKYYAAAAAMqUYAgAAAMhU3WKoWq3G8PBwlMvl\nGBwcjNnZ2VvOnzp1Kvbu3Rv9/f3xxhtvNGxQAIBc2L8AgGZpq/eEiYmJWFhYiLGxsZieno7R0dH4\n7W9/Wzv/q1/9Kk6fPh3r16+PPXv2xJ49e2Ljxo0NHRoAYDWzfwEAzVK3GJqamore3t6IiNixY0fM\nzMzccv6hhx6KGzduRFtbWxRFES0tLY2ZFAAgE/YvAKBZ6hZDc3NzUSqVasetra2xuLgYbW1f/en2\n7dujv78/1q1bF319fXHffffVfdHOzo47GJlGkUt6ZJImuaRHJqw2jdi/IlwrKZJJmuSSHpmkSS6r\nQ91iqFQqRaVSqR1Xq9XaUvLxxx/Hu+++G++8806sX78+fv7zn8eZM2fi+9///pL/5tWrN+5wbO62\nzs4OuSRGJmmSS3pkkh5L4p1rxP4VYQdLjfevNMklPTJJk1zSs9wdrO6PT/f09MT58+cjImJ6ejq6\nu7tr5zo6OuLee++NtWvXRmtra9x///3x+eefL2sQAAC+Yv8CAJql7h1DfX19MTk5GQMDA1EURYyM\njMT4+HjMz89HuVyOcrkc+/btizVr1kRXV1fs3bu3GXMDAKxa9i8AoFlaiqIomv2ibjdLj9sA0yOT\nNMklPTJJj6+Spcu1khbvX2mSS3pkkia5pKdhXyUDAAAAYHVSDAEAAABkSjEEAAAAkCnFEAAAAECm\nFEMAAAAAmVIMAQAAAGRKMQQAAACQKcUQAAAAQKYUQwAAAACZUgwBAAAAZEoxBAAAAJApxRAAAABA\nphRDAAAAAJlSDAEAAABkSjEEAAAAkCnFEAAAAECmFEMAAAAAmVIMAQAAAGRKMQQAAACQKcUQAAAA\nQKYUQwAAAACZUgwBAAAAZEoxBAAAAJApxRAAAABAphRDAAAAAJlSDAEAAABkSjEEAAAAkCnFEAAA\nAECmFEMAAAAAmWqr94RqtRqHDh2KTz75JNrb2+OFF16ILVu21M5/9NFHMTo6GkVRRGdnZxw5ciTW\nrl3b0KEBAFYz+xcA0Cx17xiamJiIhYWFGBsbi6GhoRgdHa2dK4oiDh48GIcPH44333wzent74+9/\n/3tDBwYAWO3sXwBAs9S9Y2hqaip6e3sjImLHjh0xMzNTO3flypXYtGlTvP766/GXv/wlvv3tb8fW\nrVsbNy0AQAbsXwBAs9Qthubm5qJUKtWOW1tbY3FxMdra2uLatWvx4YcfxvDwcHR1dcVPfvKTePjh\nh+Nb3/rWkv9mZ2fHnU/OXSeX9MgkTXJJj0xYbRqxf0W4VlIkkzTJJT0ySZNcVoe6xVCpVIpKpVI7\nrlar0db21Z9t2rQptmzZEtu2bYuIiN7e3piZmam7mFy9euNOZqYBOjs75JIYmaRJLumRSXosiXeu\nEftXhB0sNd6/0iSX9MgkTXJJz3J3sLq/MdTT0xPnz5+PiIjp6eno7u6unXvwwQejUqnE7OxsRERc\nvHgxtm/fvqxBAAD4iv0LAGiWuncM9fX1xeTkZAwMDERRFDEyMhLj4+MxPz8f5XI5XnzxxRgaGoqi\nKGLnzp2xe/fuJowNALB62b8AgGZpKYqiaPaLut0sPW4DTI9M0iSX9MgkPb5Kli7XSlq8f6VJLumR\nSZrkkp6GfZUMAAAAgNVJMQQAAACQKcUQAAAAQKYUQwAAAACZUgwBAAAAZEoxBAAAAJApxRAAAABA\nphRDAAAAAJlSDAEAAABkSjEEAAAAkCnFEAAAAECmFEMAAAAA/x97dxRad33+D/xJc0ytPbFFGrwy\nhZbGm16kqTejhMm2IFMZtMGd6Ey9EIpXgxHGvGnohabZ6sWgvzpwsLkVtCmlF6agg9hKoWzQRlMJ\nQx1dl4vd2LEWe5LZGM73f6E7/Pp3y9kv7Tn5LJ/X6+6bz2nOA2/OOQ9vvydmSjEEAAAAkCnFEAAA\nAECmFEMAAAAAmVIMAQAAAGRKMQQAAACQKcUQAAAAQKYUQwAAAACZUgwBAAAAZEoxBAAAAJApxRAA\nAABAphRDAAAAAJlSDAEAAABkSjEEAAAAkCnFEAAAAECmFEMAAAAAmVIMAQAAAGRKMQQAAACQKcUQ\nAAAAQKYaFkO1Wi1GR0ejUqnE8PBwzM3N/cvHHTx4MF555ZW7PiAAQG7sXwBAqzQshqampmJxcTEm\nJiZiZGQkxsfHv/aYEydOxCeffNKUAQEAcmP/AgBapWExND09Hf39/RER0dvbG7Ozs7edv//++3H5\n8uWoVCrNmRAAIDP2LwCgVUqNHlCtVqNcLtev29vbY2lpKUqlUnz66adx7Nix+J//+Z94++23/+Mn\n7erqXNm0NJVc0iOTNMklPTJhrWnG/hXhtZIimaRJLumRSZrksjY0LIbK5XLMz8/Xr2u1WpRKX/6z\nd955J65fvx4HDhyIa9euxeeffx7btm2Lffv2Lfs7r127eYdjc7d1dXXKJTEySZNc0iOT9FgS71wz\n9q8IO1hqvH+lSS7pkUma5JKele5gDYuhvr6+OHfuXDz++OMxMzMTPT099bP9+/fH/v37IyLi9OnT\n8ec///k/WkoAAPj37F8AQKs0LIYGBgbiwoULMTQ0FEVRxNjYWExOTsbCwoLvtQMANIH9CwBolbai\nKIpWP6nbzdLjNsD0yCRNckmPTNLjq2Tp8lpJi/evNMklPTJJk1zSs9IdrOH/lQwAAACAtUkxBAAA\nAJApxRAAAABAphRDAAAAAJlSDAEAAABkSjEEAAAAkCnFEAAAAECmFEMAAAAAmVIMAQAAAGRKMQQA\nAACQKcUQAAAAQKYUQwAAAACZUgwBAAAAZEoxBAAAAJApxRAAAABAphRDAAAAAJlSDAEAAABkSjEE\nAAAAkCnFEAAAAECmFEMAAAAAmVIMAQAAAGRKMQQAAACQKcUQAAAAQKYUQwAAAACZUgwBAAAAZEox\nBAAAAJApxRAAAABAphRDAAAAAJlSDAEAAABkqtToAbVaLQ4dOhQff/xxdHR0xEsvvRRbt26tn585\ncyZ+85vfRHt7e/T09MShQ4di3Tp9EwDAStm/AIBWabhBTE1NxeLiYkxMTMTIyEiMj4/Xzz7//PP4\n+c9/Hr/97W/jxIkTUa1W49y5c00dGABgrbN/AQCt0rAYmp6ejv7+/oiI6O3tjdnZ2fpZR0dHnDhx\nIjZs2BAREUtLS7F+/fomjQoAkAf7FwDQKg2/SlatVqNcLtev29vbY2lpKUqlUqxbty62bNkSERHH\njx+PhYWF2LNnT8Mn7erqvIORaRa5pEcmaZJLemTCWtOM/SvCayVFMkmTXNIjkzTJZW1oWAyVy+WY\nn5+vX9dqtSiVSrddHzlyJK5evRpHjx6Ntra2hk967drNFY5Ls3R1dcolMTJJk1zSI5P0WBLvXDP2\nrwg7WGq8f6VJLumRSZrkkp6V7mANv0rW19cX58+fj4iImZmZ6Onpue18dHQ0bt26Fa+++mr9lmYA\nAFbO/gUAtErDO4YGBgbiwoULMTQ0FEVRxNjYWExOTsbCwkLs3LkzTp06FY888kg899xzERGxf//+\nGBgYaPrgAABrlf0LAGiVtqIoilY/qdvN0uM2wPTIJE1ySY9M0uOrZOnyWkmL9680ySU9MkmTXNLT\ntK+SAQAAALA2KYYAAAAAMqUYAgAAAMiUYggAAAAgU4ohAAAAgEwphgAAAAAypRgCAAAAyJRiCAAA\nACBTiiEAAACATCmGAAAAADKlGAIAAADIlGIIAAAAIFOKIQAAAIBMKYYAAAAAMqUYAgAAAMiUYggA\nAAAgU4ohAAAAgEwphgAAAAAypRgCAAAAyJRiCAAAACBTiiEAAACATCmGAAAAADKlGAIAAADIlGII\nAAAAIFOKIQAAAIBMKYYAAAAAMqUYAgAAAMiUYggAAAAgU4ohAAAAgEwphgAAAAAy1bAYqtVqMTo6\nGpVKJYaHh2Nubu6287Nnz8bg4GBUKpU4efJk0wYFAMiF/QsAaJWGxdDU1FQsLi7GxMREjIyMxPj4\neP3siy++iMOHD8evfvWrOH78eExMTMTf/va3pg4MALDW2b8AgFZpWAxNT09Hf39/RET09vbG7Oxs\n/ezKlSvR3d0dmzZtio6Ojti9e3dcvHixedMCAGTA/gUAtEqp0QOq1WqUy+X6dXt7eywtLUWpVIpq\ntRqdnZ31s40bN0a1Wm34pF1dnQ0fQ+vJJT0ySZNc0iMT1ppm7F8RXispkkma5JIemaRJLmtDwzuG\nyuVyzM/P169rtVqUSqV/eTY/P3/bogIAwP+d/QsAaJWGxVBfX1+cP38+IiJmZmaip6enfrZ9+/aY\nm5uLGzduxOLiYly6dCl27drVvGkBADJg/wIAWqWtKIpiuQfUarU4dOhQfPLJJ1EURYyNjcUf//jH\nWFhYiEqlEmfPno1jx45FURQxODgYP/jBD1o1OwDAmmT/AgBapWExBAAAAMDa1PCrZAAAAACsTYoh\nAAAAgEw1rRiq1WoxOjoalUolhoeHY25u7rbzs2fPxuDgYFQqlTh58mSzxuB/aZTJmTNn4qmnnoqh\noaEYHR2NWq22SpPmo1Em/3Tw4MF45ZVXWjxdvhrl8uGHH8YzzzwTTz/9dPzwhz+MW7durdKk+WiU\nyVtvvRV79+6NwcHBeOONN1Zpynxdvnw5hoeHv/Zzn/WtZ/9Kkx0sPXawNNnB0mMHS9dd3b+KJvnd\n735X/OQnPymKoig++OCD4oUXXqifLS4uFt/5zneKGzduFLdu3Sr27dtXXLt2rVmj8JXlMvnHP/5R\nfPvb3y4WFhaKoiiKH/3oR8XU1NSqzJmT5TL5pzfffLP4/ve/Xxw5cqTV42VruVxqtVrxve99r/jL\nX/5SFEVRnDx5srhy5cqqzJmTRq+VPXv2FNevXy9u3bpV/3yhNV577bXiySefLJ566qnbfu6zfnXY\nv9JkB0uPHSxNdrD02MHSdLf3r6bdMTQ9PR39/f0REdHb2xuzs7P1sytXrkR3d3ds2rQpOjo6Yvfu\n3XHx4sVmjcJXlsuko6MjTpw4ERs2bIiIiKWlpVi/fv2qzJmT5TKJiHj//ffj8uXLUalUVmO8bC2X\ny9WrV2Pz5s3x+uuvx7PPPhs3btyIbdu2rdao2Wj0Wnn44Yfj5s2bsbi4GEVRRFtb22qMmaXu7u44\nevTo137us3512L/SZAdLjx0sTXaw9NjB0nS396+mFUPVajXK5XL9ur29PZaWlupnnZ2d9bONGzdG\ntVpt1ih8ZblM1q1bF1u2bImIiOPHj8fCwkLs2bNnVebMyXKZfPrpp3Hs2LEYHR1drfGytVwu169f\njw8++CCeffbZ+PWvfx1/+MMf4ve///1qjZqN5TKJiNixY0cMDg7GE088EY8++mjcf//9qzFmlh57\n7LEolUpf+7nP+tVh/0qTHSw9drA02cHSYwdL093ev5pWDJXL5Zifn69f12q1+uD//9n8/Pxtw9Mc\ny2Xyz+uf/vSnceHChTh69Ki2twWWy+Sdd96J69evx4EDB+K1116LM2fOxOnTp1dr1Kwsl8vmzZtj\n69atsX379rjnnnuiv7//a//lhLtvuUw++uijeO+99+Ldd9+Ns2fPxt///vd4++23V2tUvuKzfnXY\nv9JkB0uPHSxNdrD02MH+u6z0s75pxVBfX1+cP38+IiJmZmaip6enfrZ9+/aYm5uLGzduxOLiYly6\ndCl27drVrFH4ynKZRESMjo7GrVu34tVXX63fzkxzLZfJ/v374/Tp03H8+PE4cOBAPPnkk7Fv377V\nGjUry+Xy0EMPxfz8fP0P7126dCl27NixKnPmZLlMOjs74957743169dHe3t7PPDAA/HZZ5+t1qh8\nxWf96rB/pckOlh47WJrsYOmxg/13Weln/dfvPbpLBgYG4sKFCzE0NBRFUcTY2FhMTk7GwsJCVCqV\nePHFF+P555+PoihicHAwHnzwwWaNwleWy2Tnzp1x6tSpeOSRR+K5556LiC8/FAcGBlZ56rWt0euE\n1dEol5dffjlGRkaiKIrYtWtXPProo6s98prXKJNKpRLPPPNM3HPPPdHd3R179+5d7ZGz5bN+ddm/\n0mQHS48dLE12sPTYwf473OlnfVtRFEUL5gQAAAAgMU37KhkAAAAAaVMMAQAAAGRKMQQAAACQKcUQ\nAAAAQKYUQwAAAACZUgwBAAAAZEoxBAAAAJApxRAAAABAphRDAAAAAJlSDAEAAABkSjEEAAAAkCnF\nEAAAAECmFEMAAAAAmVIMAQAAAGRKMQQAAACQKcUQAAAAQKYUQwAAAACZUgwBAAAAZEoxBAAAAJCp\n/6gYunz5cgwPD3/t52fPno3BwcGoVCpx8uTJuz4cAEDO7GAAQLOVGj3gl7/8Zbz11luxYcOG237+\nxRdfxOHDh+PUqVOxYcOGePrpp+Nb3/pWbNmypWnDAgDkwg4GALRCwzuGuru74+jRo1/7+ZUrV6K7\nuzs2bdoUHR0dsXv37rh48WJThgQAyI0dDABohYbF0GOPPRal0tdvLKpWq9HZ2Vm/3rhxY1Sr1YZP\nWBTF/3FEAID82MEAgFZo+FWyf6dcLsf8/Hz9en5+/rYl5d9pa2uLa9durvRpaZKurk65JEYmaZJL\nemSSnq6uxvsAK2cHWzu8f6VJLumRSZrkkp6V7mAr/r+Sbd++Pebm5uLGjRuxuLgYly5dil27dq30\n1wEA8B+wgwEAd9P/+Y6hycnJWFhYiEqlEi+++GI8//zzURRFDA4OxoMPPtiMGQEAsmcHAwCaoa1Y\nhS+cu90sPW4DTI9M0iSX9MgkPb5Kli6vlbR4/0qTXNIjkzTJJT0t/yoZAAAAAP/dFEMAAAAAmVIM\nAQAAAGRKMQQAAACQKcUQAAAAQKYUQwAAAACZUgwBAAAAZEoxBAAAAJApxRAAAABAphRDAAAAAJlS\nDAEAAABkSjEEAAAAkCnFEAAAAECmFEMAAAAAmVIMAQAAAGRKMQQAAACQKcUQAAAAQKYUQwAAAACZ\nUgwBAAAAZEoxBAAAAJApxRAAAABAphRDAAAAAJlSDAEAAABkSjEEAAAAkCnFEAAAAECmFEMAAAAA\nmVlHHOMAACAASURBVFIMAQAAAGRKMQQAAACQKcUQAAAAQKYUQwAAAACZalgM1Wq1GB0djUqlEsPD\nwzE3N3fb+VtvvRV79+6NwcHBeOONN5o2KABALuxfAECrlBo9YGpqKhYXF2NiYiJmZmZifHw8fvGL\nX9TPf/azn8WZM2fivvvuiyeeeCKeeOKJ2LRpU1OHBgBYy+xfAECrNCyGpqeno7+/PyIient7Y3Z2\n9rbzhx9+OG7evBmlUimKooi2trbmTAoAkAn7FwDQKg2LoWq1GuVyuX7d3t4eS0tLUSp9+U937NgR\ng4ODsWHDhhgYGIj777+/4ZN2dXXewcg0i1zSI5M0ySU9MmGtacb+FeG1kiKZpEku6ZFJmuSyNjQs\nhsrlcszPz9eva7VafSn56KOP4r333ot333037rvvvvjxj38cb7/9dnz3u99d9ndeu3bzDsfmbuvq\n6pRLYmSSJrmkRybpsSTeuWbsXxF2sNR4/0qTXNIjkzTJJT0r3cEa/vHpvr6+OH/+fEREzMzMRE9P\nT/2ss7Mz7r333li/fn20t7fHAw88EJ999tmKBgEA4Ev2LwCgVRreMTQwMBAXLlyIoaGhKIoixsbG\nYnJyMhYWFqJSqUSlUolnnnkm7rnnnuju7o69e/e2Ym4AgDXL/gUAtEpbURRFq5/U7WbpcRtgemSS\nJrmkRybp8VWydHmtpMX7V5rkkh6ZpEku6WnaV8kAAAAAWJsUQwAAAACZUgwBAAAAZEoxBAAAAJAp\nxRAAAABAphRDAAAAAJlSDAEAAABkSjEEAAAAkCnFEAAAAECmFEMAAAAAmVIMAQAAAGRKMQQAAACQ\nKcUQAAAAQKYUQwAAAACZUgwBAAAAZEoxBAAAAJApxRAAAABAphRDAAAAAJlSDAEAAABkSjEEAAAA\nkCnFEAAAAECmFEMAAAAAmVIMAQAAAGRKMQQAAACQKcUQAAAAQKYUQwAAAACZUgwBAAAAZEoxBAAA\nAJApxRAAAABApkqNHlCr1eLQoUPx8ccfR0dHR7z00kuxdevW+vmHH34Y4+PjURRFdHV1xZEjR2L9\n+vVNHRoAYC2zfwEArdLwjqGpqalYXFyMiYmJGBkZifHx8fpZURRx8ODBOHz4cLz55pvR398ff/3r\nX5s6MADAWmf/AgBapeEdQ9PT09Hf3x8REb29vTE7O1s/u3r1amzevDlef/31+NOf/hTf/OY3Y9u2\nbc2bFgAgA/YvAKBVGhZD1Wo1yuVy/bq9vT2WlpaiVCrF9evX44MPPojR0dHo7u6OF154IXbu3Bnf\n+MY3lv2dXV2ddz45d51c0iOTNMklPTJhrWnG/hXhtZIimaRJLumRSZrksjY0LIbK5XLMz8/Xr2u1\nWpRKX/6zzZs3x9atW2P79u0REdHf3x+zs7MNF5Nr127eycw0QVdXp1wSI5M0ySU9MkmPJfHONWP/\nirCDpcb7V5rkkh6ZpEku6VnpDtbwbwz19fXF+fPnIyJiZmYmenp66mcPPfRQzM/Px9zcXEREXLp0\nKXbs2LGiQQAA+JL9CwBolYZ3DA0MDMSFCxdiaGgoiqKIsbGxmJycjIWFhahUKvHyyy/HyMhIFEUR\nu3btikcffbQFYwMArF32LwCgVdqKoiha/aRuN0uP2wDTI5M0ySU9MkmPr5Kly2slLd6/0iSX9Mgk\nTXJJT9O+SgYAAADA2qQYAgAAAMiUYggAAAAgU4ohAAAAgEwphgAAAAAypRgCAAAAyJRiCAAAACBT\niiEAAACATCmGAAAAADKlGAIAAADIlGIIAAAAIFOKIQAAAIBMKYYAAAAAMqUYAgAAAMiUYggAAAAg\nU4ohAAAAgEwphgAAAAAypRgCAAAAyJRiCAAAACBTiiEAAACATCmGAAAAADKlGAIAAADIlGIIAAAA\nIFOKIQAAAIBMKYYAAAAAMqUYAgAAAMiUYggAAAAgU4ohAAAAgEwphgAAAAAy1bAYqtVqMTo6GpVK\nJYaHh2Nubu5fPu7gwYPxyiuv3PUBAQByY/8CAFqlYTE0NTUVi4uLMTExESMjIzE+Pv61x5w4cSI+\n+eSTpgwIAJAb+xcA0CoNi6Hp6eno7++PiIje3t6YnZ297fz999+Py5cvR6VSac6EAACZsX8BAK3S\nsBiqVqtRLpfr1+3t7bG0tBQREZ9++mkcO3YsRkdHmzchAEBm7F8AQKuUGj2gXC7H/Px8/bpWq0Wp\n9OU/e+edd+L69etx4MCBuHbtWnz++eexbdu22Ldv37K/s6ur8w7Hphnkkh6ZpEku6ZEJa00z9q8I\nr5UUySRNckmPTNIkl7WhYTHU19cX586di8cffzxmZmaip6enfrZ///7Yv39/REScPn06/vznP/9H\nS8m1azfvYGSaoaurUy6JkUma5JIemaTHknjnmrF/RdjBUuP9K01ySY9M0iSX9Kx0B2tYDA0MDMSF\nCxdiaGgoiqKIsbGxmJycjIWFBd9rBwBoAvsXANAqbUVRFK1+Uq1ierS96ZFJmuSSHpmkxx1D6fJa\nSYv3rzTJJT0ySZNc0rPSHazhH58GAAAAYG1SDAEAAABkSjEEAAAAkCnFEAAAAECmFEMAAAAAmVIM\nAQAAAGRKMQQAAACQKcUQAAAAQKYUQwAAAACZUgwBAAAAZEoxBAAAAJApxRAAAABAphRDAAAAAJlS\nDAEAAABkSjEEAAAAkCnFEAAAAECmFEMAAAAAmVIMAQAAAGRKMQQAAACQKcUQAAAAQKYUQwAAAACZ\nUgwBAAAAZEoxBAAAAJApxRAAAABAphRDAAAAAJlSDAEAAABkSjEEAAAAkCnFEAAAAECmFEMAAAAA\nmVIMAQAAAGSq1OgBtVotDh06FB9//HF0dHTESy+9FFu3bq2fnzlzJn7zm99Ee3t79PT0xKFDh2Ld\nOn0TAMBK2b8AgFZpuEFMTU3F4uJiTExMxMjISIyPj9fPPv/88/j5z38ev/3tb+PEiRNRrVbj3Llz\nTR0YAGCts38BAK3SsBianp6O/v7+iIjo7e2N2dnZ+llHR0ecOHEiNmzYEBERS0tLsX79+iaNCgCQ\nB/sXANAqDb9KVq1Wo1wu16/b29tjaWkpSqVSrFu3LrZs2RIREcePH4+FhYXYs2dPwyft6uq8g5Fp\nFrmkRyZpkkt6ZMJa04z9K8JrJUUySZNc0iOTNMllbWhYDJXL5Zifn69f12q1KJVKt10fOXIkrl69\nGkePHo22traGT3rt2s0VjkuzdHV1yiUxMkmTXNIjk/RYEu9cM/avCDtYarx/pUku6ZFJmuSSnpXu\nYA2/StbX1xfnz5+PiIiZmZno6em57Xx0dDRu3boVr776av2WZgAAVs7+BQC0SsM7hgYGBuLChQsx\nNDQURVHE2NhYTE5OxsLCQuzcuTNOnToVjzzySDz33HMREbF///4YGBho+uAAAGuV/QsAaJW2oiiK\nVj+p283S4zbA9MgkTXJJj0zS46tk6fJaSYv3rzTJJT0ySZNc0tO0r5IBAAAAsDYphgAAAAAypRgC\nAAAAyJRiCAAAACBTiiEAAACATCmGAAAAADKlGAIAAADIlGIIAAAAIFOKIQAAAIBMKYYAAAAAMqUY\nAgAAAMiUYggAAAAgU4ohAAAAgEwphgAAAAAypRgCAAAAyJRiCAAAACBTiiEAAACATCmGAAAAADKl\nGAIAAADIlGIIAAAAIFOKIQAAAIBMKYYAAAAAMqUYAgAAAMiUYggAAAAgU4ohAAAAgEwphgAAAAAy\npRgCAAAAyJRiCAAAACBTiiEAAACATDUshmq1WoyOjkalUonh4eGYm5u77fzs2bMxODgYlUolTp48\n2bRBAQByYf8CAFqlYTE0NTUVi4uLMTExESMjIzE+Pl4/++KLL+Lw4cPxq1/9Ko4fPx4TExPxt7/9\nrakDAwCsdfYvAKBVGhZD09PT0d/fHxERvb29MTs7Wz+7cuVKdHd3x6ZNm6KjoyN2794dFy9ebN60\nAAAZsH8BAK3SsBiqVqtRLpfr1+3t7bG0tFQ/6+zsrJ9t3LgxqtVqE8YEAMiH/QsAaJVSoweUy+WY\nn5+vX9dqtSiVSv/ybH5+/rZF5d/p6mr8GFpPLumRSZrkkh6ZsNY0Y/+K8FpJkUzSJJf0yCRNclkb\nGt4x1NfXF+fPn4+IiJmZmejp6amfbd++Pebm5uLGjRuxuLgYly5dil27djVvWgCADNi/AIBWaSuK\noljuAbVaLQ4dOhSffPJJFEURY2Nj8cc//jEWFhaiUqnE2bNn49ixY1EURQwODsYPfvCDVs0OALAm\n2b8AgFZpWAwBAAAAsDY1/CoZAAAAAGuTYggAAAAgU4ohAAAAgEw1rRiq1WoxOjoalUolhoeHY25u\n7rbzs2fPxuDgYFQqlTh58mSzxuB/aZTJmTNn4qmnnoqhoaEYHR2NWq22SpPmo1Em/3Tw4MF45ZVX\nWjxdvhrl8uGHH8YzzzwTTz/9dPzwhz+MW7durdKk+WiUyVtvvRV79+6NwcHBeOONN1Zpynxdvnw5\nhoeHv/Zzn/WtZ/9Kkx0sPXawNNnB0mMHS9dd3b+KJvnd735X/OQnPymKoig++OCD4oUXXqifLS4u\nFt/5zneKGzduFLdu3Sr27dtXXLt2rVmj8JXlMvnHP/5RfPvb3y4WFhaKoiiKH/3oR8XU1NSqzJmT\n5TL5pzfffLP4/ve/Xxw5cqTV42VruVxqtVrxve99r/jLX/5SFEVRnDx5srhy5cqqzJmTRq+VPXv2\nFNevXy9u3bpV/3yhNV577bXiySefLJ566qnbfu6zfnXYv9JkB0uPHSxNdrD02MHSdLf3r6bdMTQ9\nPR39/f0REdHb2xuzs7P1sytXrkR3d3ds2rQpOjo6Yvfu3XHx4sVmjcJXlsuko6MjTpw4ERs2bIiI\niKWlpVi/fv2qzJmT5TKJiHj//ffj8uXLUalUVmO8bC2Xy9WrV2Pz5s3x+uuvx7PPPhs3btyIbdu2\nrdao2Wj0Wnn44Yfj5s2bsbi4GEVRRFtb22qMmaXu7u44evTo137us3512L/SZAdLjx0sTXaw9NjB\n0nS396+mFUPVajXK5XL9ur29PZaWlupnnZ2d9bONGzdGtVpt1ih8ZblM1q1bF1u2bImIiOPHj8fC\nwkLs2bNnVebMyXKZfPrpp3Hs2LEYHR1drfGytVwu169fjw8++CCeffbZ+PWvfx1/+MMf4ve///1q\njZqN5TKJiNixY0cMDg7GE088EY8++mjcf//9qzFmlh577LEolUpf+7nP+tVh/0qTHSw9drA02cHS\nYwdL093ev5pWDJXL5Zifn69f12q1+uD//9n8/Pxtw9Mcy2Xyz+uf/vSnceHChTh69Ki2twWWy+Sd\nd96J69evx4EDB+K1116LM2fOxOnTp1dr1Kwsl8vmzZtj69atsX379rjnnnuiv7//a//lhLtvuUw+\n+uijeO+99+Ldd9+Ns2fPxt///vd4++23V2tUvuKzfnXYv9JkB0uPHSxNdrD02MH+u6z0s75pxVBf\nX1+cP38+IiJmZmaip6enfrZ9+/aYm5uLGzduxOLiYly6dCl27drVrFH4ynKZRESMjo7GrVu34tVX\nX63fzkxzLZfJ/v374/Tp03H8+PE4cOBAPPnkk7Fv377VGjUry+Xy0EMPxfz8fP0P7126dCl27Nix\nKnPmZLlMOjs74957743169dHe3t7PPDAA/HZZ5+t1qh8xWf96rB/pckOlh47WJrsYOmxg/13Weln\n/dfvPbpLBgYG4sKFCzE0NBRFUcTY2FhMTk7GwsJCVCqVePHFF+P555+PoihicHAwHnzwwWaNwleW\ny2Tnzp1x6tSpeOSRR+K5556LiC8/FAcGBlZ56rWt0euE1dEol5dffjlGRkaiKIrYtWtXPProo6s9\n8prXKJNKpRLPPPNM3HPPPdHd3R179+5d7ZGz5bN+ddm/0mQHS48dLE12sPTYwf473OlnfVtRFEUL\n5gQAAAAgMU37KhkAAAAAaVMMAQAAAGRKMQQAAACQKcUQAAAAQKYUQwAAAACZUgwBAAAAZEoxBAAA\nAJApxRAAAABAphRDAAAAAJlSDAEAAABkSjEEAAAAkCnFEAAAAECmFEMAAAAAmVIMAQAAAGRKMQQA\nAACQKcUQAAAAQKYUQwAAAACZUgwBAAAAZEoxBAAAAJCp/6gYunz5cgwPD3/t52fPno3BwcGoVCpx\n8uTJuz4cAEDO7GAAQLOVGj3gl7/8Zbz11luxYcOG237+xRdfxOHDh+PUqVOxYcOGePrpp+Nb3/pW\nbNmypWnDAgDkwg4GALRCwzuGuru74+jRo1/7+ZUrV6K7uzs2bdoUHR0dsXv37rh48WJThgQAyI0d\nDABohYbF0GOPPRal0tdvLKpWq9HZ2Vm/3rhxY1Sr1YZPWBTF/3FEAID82MEAgFZo+FWyf6dcLsf8\n/Hz9en5+/rYl5d9pa2uLa9durvRpaZKurk65JEYmaZJLemSSnq6uxvsAK2cHWzu8f6VJLumRSZrk\nkp6V7mAr/r+Sbd++Pebm5uLGjRuxuLgYly5dil27dq301wEA8B+wgwEAd9P/+Y6hycnJWFhYiEql\nEi+++GI8//zzURRFDA4OxoMPPtiMGQEAsmcHAwCaoa1YhS+cu90sPW4DTI9M0iSX9MgkPb5Kli6v\nlbR4/0qTXNIjkzTJJT0t/yoZAAAAAP/dFEMAAAAAmVIMAQAAAGRKMQQAAACQKcUQAAAAQKYUQwAA\nAACZUgwBAAAAZEoxBAAAAJApxRAAAABAphRDAAAAAJlSDAEAAABkSjEEAAAAkCnFEAAAAECmFEMA\nAAAAmVIMAQAAAGRKMQQAAACQKcUQAAAAQKYUQwAAAACZUgwBAAAAZEoxBAAAAJApxRAAAABAphRD\nAAAAAJlSDAEAAABkSjEEAAAAkCnFEAAAAECmFEMAAAAAmVIMAQAAAGRKMQQAAACQKcUQAAAAQKYa\nFkO1Wi1GR0ejUqnE8PBwzM3N3Xb+1ltvxd69e2NwcDDeeOONpg0KAJAL+xcA0CqlRg+YmpqKxcXF\nmJiYiJmZmRgfH49f/OIX9fOf/exncebMmbjvvvviiSeeiCeeeCI2bdrU1KEBANYy+xcA0CoNi6Hp\n6eno7++PiIje3t6YnZ297fzhhx+OmzdvRqlUiqIooq2trTmTAgBkwv4FALRKw2KoWq1GuVyuX7e3\nt8fS0lKUSl/+0x07dsTg4GBs2LAhBgYG4v7772/etAAAGbB/AQCt0rAYKpfLMT8/X7+u1Wr1peSj\njz6K9957L959992477774sc//nG8/fbb8d3vfnfZ39nV1XmHY9MMckmPTNIkl/TIhLWmGftXhNdK\nimSSJrmkRyZpksva0LAY6uvri3PnzsXjjz8eMzMz0dPTUz/r7OyMe++9N9avXx/t7e3xwAMPxGef\nfdbwSa9du3lnU3PXdXV1yiUxMkmTXNIjk/RYEu9cM/avCDtYarx/pUku6ZFJmuSSnpXuYA2LoYGB\ngbhw4UIMDQ1FURQxNjYWk5OTsbCwEJVKJSqVSjzzzDNxzz33RHd3d+zdu3dFgwAA8CX7FwDQKm1F\nURStflKtYnq0vemRSZrkkh6ZpMcdQ+nyWkmL9680ySU9MkmTXNKz0h1s3V2eAwAAAID/EoohAAAA\ngEwphgAAAAAypRgCAAAAyJRiCAAAACBTiiEAAACATCmGAAAAADKlGAIAAADIlGIIAAAAIFOKIQAA\nAIBMKYYAAAAAMqUYAgAAAMiUYggAAAAgU4ohAAAAgEwphgAAAAAypRgCAAAAyJRiCAAAACBTiiEA\nAACATCmGAAAAADKlGAIAAADIlGIIAAAAIFOKIQAAAIBMKYYAAAAAMqUYAgAAAMiUYggAAAAgU4oh\nAAAAgEwphgAAAAAypRgCAAAAyJRiCAAAACBTiiEAAACATJUaPaBWq8WhQ4fi448/jo6OjnjppZdi\n69at9fMPP/wwxsfHoyiK6OrqiiNHjsT69eubOjQAwFpm/wIAWqXhHUNTU1OxuLgYExMTMTIyEuPj\n4/Wzoiji4MGDcfjw4XjzzTejv78//vrXvzZ1YACAtc7+BQC0SsM7hqanp6O/vz8iInp7e2N2drZ+\ndvXq1di8eXO8/vrr8ac//Sm++c1vxrZt25o3LQBABuxfAECrNCyGqtVqlMvl+nV7e3ssLS1FqVSK\n69evxwcffBCjo6PR3d0dL7zwQuzcuTO+8Y1vLPs7u7o673xy7jq5pEcmaZJLemTCWtOM/SvCayVF\nMkmTXNIjkzTJZW1oWAyVy+WYn5+vX9dqtSiVvvxnmzdvjq1bt8b27dsjIqK/vz9mZ2cbLibXrt28\nk5lpgq6uTrkkRiZpkkt6ZJIeS+Kda8b+FWEHS433rzTJJT0ySZNc0rPSHazh3xjq6+uL8+fPR0TE\nzMxM9PT01M8eeuihmJ+fj7m5uYiIuHTpUuzYsWNFgwAA8CX7FwDQKg3vGBoYGIgLFy7E0NBQFEUR\nY2NjMTk5GQsLC1GpVOLll1+OkZGRKIoidu3aFY8++mgLxgYAWLvsXwBAq7QVRVG0+kndbpYetwGm\nRyZpkkt6ZJIeXyVLl9dKWrx/pUku6ZFJmuSSnqZ9lQwAAACAtUkxBAAAAJApxRAAAABAphRDAAAA\nAJlSDAEAAABkSjEEAAAAkCnFEAAAAECmFEMAAAAAmVIMAQAAAGRKMQQAAACQKcUQAAAAQKYUQwAA\nAACZUgwBAAAAZEoxBAAAAJApxRAAAABAphRDAAAAAJlSDAEAAABkSjEEAAAAkCnFEAAAAECmFEMA\nAAAAmVIMAQAAAGRKMQQAAACQKcUQAAAAQKYUQwAAAACZUgwBAAAAZEoxBAAAAJApxRAAAABAphRD\nAAAAAJlSDAEAAABkqmExVKvVYnR0NCqVSgwPD8fc3Ny/fNzBgwfjlVdeuesDAgDkxv4FALRKw2Jo\namoqFhcXY2JiIkZGRmJ8fPxrjzlx4kR88sknTRkQACA39i8AoFUaFkPT09PR398fERG9vb0xOzt7\n2/n7778fly9fjkql0pwJAQAyY/8CAFqlYTFUrVajXC7Xr9vb22NpaSkiIj799NM4duxYjI6ONm9C\nAIDM2L8AgFYpNXpAuVyO+fn5+nWtVotS6ct/9s4778T169fjwIEDce3atfj8889j27ZtsW/fvmV/\nZ1dX5x2OTTPIJT0ySZNc0iMT1ppm7F8RXispkkma5JIemaRJLmtDw2Kor68vzp07F48//njMzMxE\nT09P/Wz//v2xf//+iIg4ffp0/PnPf/6PlpJr127ewcg0Q1dXp1wSI5M0ySU9MkmPJfHONWP/irCD\npcb7V5rkkh6ZpEku6VnpDtawGBoYGIgLFy7E0NBQFEURY2NjMTk5GQsLC77XDgDQBPYvAKBV2oqi\nKFr9pFrF9Gh70yOTNMklPTJJjzuG0uW1khbvX2mSS3pkkia5pGelO1jDPz4NAAAAwNqkGAIAAADI\nlGIIAAAAIFOKIQAAAIBMKYYAAAAAMqUYAgAAAMiUYggAAAAgU4ohAAAAgEwphgAAAAAypRgCAAAA\nyJRiCAAAACBTiiEAAACATCmGAAAAADKlGAIAAADIlGIIAAAAIFOKIQAAAIBMKYYAAAAAMqUYAgAA\nAMiUYggAAAAgU4ohAAAAgEwphgAAAAAypRgCAAAAyJRiCAAAACBTiiEAAACATCmGAAAAADKlGAIA\nAADIlGIIAAAAIFOKIQAAAIBMKYYAAAAAMqUYAgAAAMhUqdEDarVaHDp0KD7++OPo6OiIl156KbZu\n3Vo/P3PmTPzmN7+J9vb26OnpiUOHDsW6dfomAICVsn8BAK3ScIOYmpqKxcXFmJiYiJGRkRgfH6+f\nff755/Hzn/88fvvb38aJEyeiWq3GuXPnmjowAMBaZ/8CAFqlYTE0PT0d/f39ERHR29sbs7Oz9bOO\njo44ceJEbNiwISIilpaWYv369U0aFQAgD/YvAKBVGn6VrFqtRrlcrl+3t7fH0tJSlEqlWLduXWzZ\nsiUiIo4fPx4LCwuxZ8+ehk/a1dV5ByPTLHJJj0zSJJf0yIS1phn7V4TXSopkkia5pEcmaZLL2tCw\nGCqXyzE/P1+/rtVqUSqVbrs+cuRIXL16NY4ePRptbW0Nn/TatZsrHJdm6erqlEtiZJImuaRHJumx\nJN65ZuxfEXaw1Hj/SpNc0iOTNMklPSvdwRp+layvry/Onz8fEREzMzPR09Nz2/no6GjcunUrXn31\n1fotzQAArJz9CwBolYZ3DA0MDMSFCxdiaGgoiqKIsbGxmJycjIWFhdi5c2ecOnUqHnnkkXjuueci\nImL//v0xMDDQ9MEBANYq+xcA0CptRVEUrX5St5ulx22A6ZFJmuSSHpmkx1fJ0uW1khbvX2mSS3pk\nkia5pKdpXyUDAAAAYG1SDAEAAABkSjEEAAAAkCnFEAAAAECmFEMAAAAAmVIMAQAAAGRKMQQAAACQ\nKcUQAAAAQKYUQwAAAACZUgwBAAAAZEoxBAAAAJApxRAAAABAphRDAAAAAJlSDAEAAABkSjEEAAAA\nkCnFEAAAAECmFEMAAAAAmVIMAQAAAGRKMQQAAACQKcUQAAAAQKYUQwAAAACZUgwBAAAAZEoxBAAA\nAJApxRAAAABAphRDAAAAAJlSDAEAAABkSjEEAAAAkCnFEAAAAECmFEMAAAAAmWpYDNVqtRgdHY1K\npRLDw8MxNzd32/nZs2djcHAwKpVKnDx5smmDAgDkwv4FALRKw2JoamoqFhcXY2JiIkZGRmJ8fLx+\n9sUXX8Thw4fjV7/6VRw/fjwmJibib3/7W1MHBgBY6+xfAECrNCyGpqeno7+/PyIient7Y3Z2tn52\n5cqV6O7ujk2bNkVHR0fs3r07Ll682LxpAQAyYP8CAFqlYTFUrVajXC7Xr9vb22Npaal+1tnZWT/b\nuHFjVKvVJowJAJAP+xcA0CqlRg8ol8sxPz9fv67ValEqlf7l2fz8/G2Lyr/T1dX4MbSeXNIjMW45\nqAAABcFJREFUkzTJJT0yYa1pxv4V4bWSIpmkSS7pkUma5LI2NLxjqK+vL86fPx8RETMzM9HT01M/\n2759e8zNzcWNGzdicXExLl26FLt27WretAAAGbB/AQCt0lYURbHcA2q1Whw6dCg++eSTKIoixsbG\n4o9//GMsLCxEpVKJs2fPxrFjx6IoihgcHIwf/OAHrZodAGBNsn8BAK3SsBgCAAAAYG1q+FUyAAAA\nANYmxRAAAABAphRDAAAAAJlqWjFUq9VidHQ0KpVKDA8Px9zc3G3nZ8/+v3buICTqLY7i+DFTi7Qk\nglZNkFibFlntRBBKWiSBDjYmVotAWgXRojbNLkFqJ7VwUyBUSLgooYKSCKQgy4wWbaTaFjRDORMz\nyPxavGl4PeE/0PPvvXW/n938/5sDh+EerlPTSiaTSqVSmpiYiCsG/qVaJ1NTU+rr61N/f7/S6bRK\npZKjpOGo1slPFy9e1JUrV1Y5Xbiq9fLmzRsNDAzo2LFjOnPmjAqFgqOk4ajWyd27d9XT06NkMqmb\nN286Shmu+fl5HT9+fNlzzvrVx/7yExvMP2wwP7HB/MMG89eK7i+LycOHD+38+fNmZjY3N2enT5+u\nvCsWi3bw4EHLZrNWKBSst7fXPn/+HFcUlEV18v37dztw4IDl83kzMzt79qw9evTISc6QRHXy061b\nt+zo0aN2+fLl1Y4XrKheSqWSHTlyxD58+GBmZhMTE7awsOAkZ0iqfVfa29stk8lYoVConC9YHWNj\nY9bd3W19fX2/POesd4P95Sc2mH/YYH5ig/mHDeanld5fsf1i6OXLl+ro6JAk7dmzR2/fvq28W1hY\nUCKR0KZNm1RfX699+/bpxYsXcUVBWVQn9fX1un37ttavXy9JWlpaUkNDg5OcIYnqRJJevXql+fl5\npVIpF/GCFdXL+/fv1dzcrBs3bmhwcFDZbFY7duxwFTUY1b4ru3bt0rdv31QsFmVmqqmpcREzSIlE\nQqOjo8uec9a7wf7yExvMP2wwP7HB/MMG89NK76/YLoYWFxfV2NhY+VxbW6ulpaXKu6ampsq7DRs2\naHFxMa4oKIvqZM2aNdqyZYskaXx8XPl8Xu3t7U5yhiSqk0+fPunq1atKp9Ou4gUrqpdMJqO5uTkN\nDg7q+vXrev78uZ49e+YqajCiOpGk1tZWJZNJHT58WJ2dndq4caOLmEE6dOiQ1q5du+w5Z70b7C8/\nscH8wwbzExvMP2wwP630/ortYqixsVG5XK7yuVQqVYL/910ul/slPOIR1cnPzyMjI5qZmdHo6Ci3\nvasgqpMHDx4ok8loaGhIY2Njmpqa0uTkpKuoQYnqpbm5Wdu3b1dLS4vq6urU0dGx7C8nWHlRnbx7\n905PnjzR48ePNT09rS9fvuj+/fuuoqKMs94N9pef2GD+YYP5iQ3mHzbYn+V3z/rYLob27t2rp0+f\nSpJev36tnTt3Vt61tLTo48ePymazKhaLmp2dVVtbW1xRUBbViSSl02kVCgVdu3at8nNmxCuqkxMn\nTmhyclLj4+MaGhpSd3e3ent7XUUNSlQv27ZtUy6Xq/zHe7Ozs2ptbXWSMyRRnTQ1NWndunVqaGhQ\nbW2tNm/erK9fv7qKijLOejfYX35ig/mHDeYnNph/2GB/lt8965f/9miFdHV1aWZmRv39/TIzDQ8P\n6969e8rn80qlUrpw4YJOnTolM1MymdTWrVvjioKyqE52796tO3fuaP/+/Tp58qSkfw7Frq4ux6n/\nbtW+J3CjWi+XLl3SuXPnZGZqa2tTZ2en68h/vWqdpFIpDQwMqK6uTolEQj09Pa4jB4uz3i32l5/Y\nYP5hg/mJDeYfNtif4f+e9TVmZquQEwAAAAAAAJ6J7Z+SAQAAAAAAwG9cDAEAAAAAAASKiyEAAAAA\nAIBAcTEEAAAAAAAQKC6GAAAAAAAAAsXFEAAAAAAAQKC4GAIAAAAAAAgUF0MAAAAAAACB+gEeQmkM\no6SNKgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf9a50b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plot classes from index 30 to 39\n",
    "numOfRows = 5\n",
    "numOfColumns = 2\n",
    "ClassIdIndex = 30\n",
    "\n",
    "fig, ax = plt.subplots(numOfRows, numOfColumns, figsize=(20, 20))\n",
    "sns.set_style(style='white')\n",
    "\n",
    "for row in range(numOfRows):\n",
    "    for column in range(numOfColumns):\n",
    "        ax[row][column].imshow(X_train[firstTrainDataForEachClass.iloc[ClassIdIndex].values[0]])\n",
    "        ax[row][column].set_title(labels.loc[ClassIdIndex][1], fontsize=20)\n",
    "        ClassIdIndex += 1\n",
    "        \n",
    "        if ClassIdIndex >= 40:\n",
    "            break\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Plot classes from index 40 to 42\n",
    "numOfRows = 2\n",
    "numOfColumns = 2\n",
    "\n",
    "fig, ax = plt.subplots(numOfRows, numOfColumns, figsize=(10, 10))\n",
    "sns.set_style(style='white')\n",
    "ClassIdIndex = 40\n",
    "for row in range(numOfRows):\n",
    "    for column in range(numOfColumns):\n",
    "        ax[row][column].imshow(X_train[firstTrainDataForEachClass.iloc[ClassIdIndex].values[0]])\n",
    "        ax[row][column].set_title(labels.loc[ClassIdIndex][1], fontsize=20)\n",
    "        ClassIdIndex += 1\n",
    "        \n",
    "        if ClassIdIndex >= 43:\n",
    "            break\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "numOfSamples = pd.DataFrame()\n",
    "numOfSamples['Label'] = labels['SignName']\n",
    "\n",
    "numOfSamples['Number of Samples'] = grouped_train_index.size()\n",
    "#print(numOfSamples['Number of Samples'])\n",
    "fig, ax = plt.subplots(figsize=(10, 15))\n",
    "sns.barplot(x='Number of Samples', y='Label', data=numOfSamples)\n",
    "ax.set(xlim=(0, 2100), ylabel=\"Classes\", xlabel=\"Number of Samples\")\n",
    "ax.set_title('Number of samples in each class', size=20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#print(labels)\n",
    "#print(grouped_train_index.groups[15])\n",
    "aaa = grouped_train_index.groups[15]\n",
    "#print(len(aaa))\n",
    "fig, ax = plt.subplots(1, 5, figsize=[10, 10])\n",
    "\n",
    "ggg = 220 # 0, 5\n",
    "\n",
    "ax[0].imshow(enhanceContrastOfImage(X_train[aaa[ggg+1]]))\n",
    "ax[1].imshow(enhanceContrastOfImage(X_train[aaa[ggg+2]]))\n",
    "ax[2].imshow(enhanceContrastOfImage(X_train[aaa[ggg+3]]))\n",
    "ax[3].imshow(enhanceContrastOfImage(X_train[aaa[ggg+4]]))\n",
    "ax[4].imshow(enhanceContrastOfImage(X_train[aaa[ggg+5]]))\n",
    "\n",
    "print(aaa.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n",
    "\n",
    "Design and implement a deep learning model that learns to recognize traffic signs. Train and test your model on the [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset).\n",
    "\n",
    "The LeNet-5 implementation shown in the [classroom](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/601ae704-1035-4287-8b11-e2c2716217ad/concepts/d4aca031-508f-4e0b-b493-e7b706120f81) at the end of the CNN lesson is a solid starting point. You'll have to change the number of classes and possibly the preprocessing, but aside from that it's plug and play! \n",
    "\n",
    "With the LeNet-5 solution from the lecture, you should expect a validation set accuracy of about 0.89. To meet specifications, the validation set accuracy will need to be at least 0.93. It is possible to get an even higher accuracy, but 0.93 is the minimum for a successful project submission. \n",
    "\n",
    "There are various aspects to consider when thinking about this problem:\n",
    "\n",
    "- Neural network architecture (is the network over or underfitting?)\n",
    "- Play around preprocessing techniques (normalization, rgb to grayscale, etc)\n",
    "- Number of examples per label (some have more than others).\n",
    "- Generate fake data.\n",
    "\n",
    "Here is an example of a [published baseline model on this problem](http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf). It's not required to be familiar with the approach used in the paper but, it's good practice to try to read papers like these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Pre-process the Data Set (normalization, grayscale, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Minimally, the image data should be normalized so that the data has mean zero and equal variance. For image data, `(pixel - 128)/ 128` is a quick way to approximately normalize the data and can be used in this project. \n",
    "\n",
    "Other pre-processing steps are optional. You can try different techniques to see if it improves performance. \n",
    "\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Preprocess the data here. It is required to normalize the data. Other preprocessing steps could include \n",
    "### converting to grayscale, etc.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "## Ganussian Filter\n",
    "def gaussianFilter(img):\n",
    "    # Gaussian filter\n",
    "    filter_size = 3\n",
    "    filter_sigma = 0.2\n",
    "    gaussian_filter = np.dot(cv2.getGaussianKernel(filter_size, \n",
    "                                                   filter_sigma, \n",
    "                                                   cv2.CV_64F),\n",
    "                            (cv2.getGaussianKernel(filter_size, \n",
    "                                                   filter_sigma, \n",
    "                                                   cv2.CV_64F)).T)\n",
    "\n",
    "    img_filtered = cv2.filter2D(img, -1, \n",
    "                                gaussian_filter, \n",
    "                                borderType=cv2.BORDER_REFLECT_101) # reflect across edge filter\n",
    "    \n",
    "    return img_filtered\n",
    "\n",
    "## Change to gray scale\n",
    "def colourToGrey(images):\n",
    "    gray = images[:,:,:,0]\n",
    "    #gray = np.reshape(gray, (len(images), 32, 32, 1))\n",
    "    for idx in tqdm(range(len(images))):\n",
    "        gray[idx] = cv2.cvtColor(images[idx], cv2.COLOR_RGB2GRAY)\n",
    "        #gray[idx] = np.reshape(gray[idx], (32,32,1))\n",
    "    return gray\n",
    "\n",
    "## Normalisation\n",
    "def normaliseColourImages(images):\n",
    "    images = images.astype(np.float32)\n",
    "    for idx, img in tqdm(enumerate(images)):\n",
    "        images[idx] = cv2.normalize(img, dst=img, alpha=0, beta=0.999999, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "    return images\n",
    "\n",
    "## Enhance Contrast using histogram equalization\n",
    "def enhanceContrastOfImage(img):\n",
    "    return cv2.equalizeHist(img)\n",
    "    '''\n",
    "    yuv_img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "    yuv_img[:,:,0] = cv2.equalizeHist(yuv_img[:,:,0])\n",
    "    return cv2.cvtColor(yuv_img, cv2.COLOR_YUV2RGB)\n",
    "    '''\n",
    "    '''\n",
    "    if np.mean(img) < 50:\n",
    "        brightImage = img.copy()\n",
    "        brightImage[:,:,0] = cv2.equalizeHist(img[:,:,0])\n",
    "        brightImage[:,:,1] = cv2.equalizeHist(img[:,:,1])\n",
    "        brightImage[:,:,2] = cv2.equalizeHist(img[:,:,2])\n",
    "        return brightImage\n",
    "    return img\n",
    "    '''\n",
    "\n",
    "\n",
    "def sharpenImage(img):\n",
    "    kernelOfSharpener = np.array([[-1,-1,-1,-1,-1], \n",
    "                                 [-1,2,2,2,-1], \n",
    "                                 [-1,2,8,2,-1],\n",
    "                                 [-1,2,2,2,-1],\n",
    "                                 [-1,-1,-1,-1,-1]]) / 8.0\n",
    "    return cv2.filter2D(img, -1, kernelOfSharpener)\n",
    "\n",
    "def affineTransformation(img):\n",
    "    row, column = X_train[8000].shape[:2]\n",
    "    sourcePoints = np.float32([[0, 0], [column-1, 0], [0, row-1]])\n",
    "\n",
    "    destinationPoints = np.float32([[np.random.uniform(low=0., high=0.2, size=1)[0]*(row-1), np.random.uniform(low=0., high=0.2, size=1)[0]*(column-1)], \n",
    "                                    [np.random.uniform(low=0.7, high=1, size=1)[0]*(row-1), np.random.uniform(low=0., high=0.2, size=1)[0]*(column-1)], \n",
    "                                    [np.random.uniform(low=0., high=0.2, size=1)[0]*(row-1), np.random.uniform(low=0.7, high=1, size=1)[0]*(column-1)]])\n",
    "\n",
    "    affine_matrix = cv2.getAffineTransform(sourcePoints, destinationPoints)\n",
    "    #img_output = cv2.warpAffine(X_train[8000], affine_matrix, (row, column))\n",
    "    \n",
    "    return cv2.warpAffine(img, affine_matrix, (row, column))\n",
    "\n",
    "def translateImageWithRandomDistance(img):\n",
    "    row, column = img.shape[:2]\n",
    "    matrixForTranslation = np.array([ [1, 0, np.random.randint(low=-5, high=5)], [0, 1, np.random.randint(low=-5, high=5)] ], dtype=float)\n",
    "    translatedImage = cv2.warpAffine(img, matrixForTranslation, (row, column))\n",
    "    return translatedImage\n",
    "\n",
    "def rotateImageWithRandomAngle(img):\n",
    "    row, column = img.shape[:2]\n",
    "    matrixForRotation = cv2.getRotationMatrix2D((column/2, row/2), np.random.randint(low=-35, high=35), 1)\n",
    "    rotatedImage = cv2.warpAffine(img, matrixForRotation, (row, column))\n",
    "    return rotatedImage\n",
    "\n",
    "def makeBallanceBetweenClasses(grouped_y_index=None, xData=None, yData=None):\n",
    "    ## balance the number of samples in classes\n",
    "    maxSampleNumbersAmongClasses = max(grouped_y_index.size())\n",
    "    argmaxSampleNumbersAmongClasses = np.argmax(grouped_y_index.size())\n",
    "\n",
    "    for class_idx, value in tqdm(grouped_y_index.groups.items()):\n",
    "        '''\n",
    "        if class_idx>1:\n",
    "            break\n",
    "        '''\n",
    "        necessaryNum = maxSampleNumbersAmongClasses - len(value)\n",
    "        randomImgInAClass = xData[np.random.choice(grouped_y_index.groups[class_idx].values, necessaryNum)]\n",
    "\n",
    "        augmented_y = np.ones(necessaryNum) * class_idx\n",
    "\n",
    "        for idx, img in enumerate(randomImgInAClass):\n",
    "            randomImgInAClass[idx] = transAndRotate(img)\n",
    "            #randomImgInAClass[idx] = transAndRotate(colourToGrey(img))\n",
    "        xData = np.vstack([xData, randomImgInAClass])\n",
    "        yData = np.append(yData, augmented_y)\n",
    "        \n",
    "    return xData, yData\n",
    "\n",
    "'''\n",
    "def transAndRotate(pre_grouped_y_index=None, grouped_y_index=None, xData=None):\n",
    "    #maxSampleNumbersAmongClasses = max(pre_grouped_y_index.size())\n",
    "    start = pre_grouped_y_index.size()\n",
    "    \n",
    "    for class_idx, value in tqdm(grouped_y_index.groups.items()):\n",
    "        for iteration, position in enumerate(value):\n",
    "            #print(iteration)\n",
    "            if iteration >= start[class_idx]:\n",
    "                switch = np.random.choice(('affine', 'rotate', 'translate', 'rotate+translate'), size=1)[0] # choose one among three strings\n",
    "                #print(position)\n",
    "                if switch == 'affine':\n",
    "                    xData[iteration] = affineTransformation(xData[iteration])\n",
    "                if switch == 'rotate':\n",
    "                    xData[iteration] = rotateImageWithRandomAngle(xData[iteration])\n",
    "                if switch == 'translate':\n",
    "                    xData[iteration] = translateImageWithRandomDistance(xData[iteration])\n",
    "                if switch == 'rotate+translate':\n",
    "                    xData[iteration] = translateImageWithRandomDistance(rotateImageWithRandomAngle(xData[iteration]))\n",
    "'''\n",
    "def transAndRotate(img):\n",
    "    switch = np.random.choice(('affine', 'rotate', 'translate', 'rotate+translate'), size=1)[0] # choose one among three strings\n",
    "    #print(switch)\n",
    "    if switch == 'affine':\n",
    "        return affineTransformation(img)\n",
    "    if switch == 'rotate':\n",
    "        return rotateImageWithRandomAngle(img)\n",
    "    if switch == 'translate':\n",
    "        return translateImageWithRandomDistance(img)\n",
    "    if switch == 'rotate+translate':\n",
    "        return translateImageWithRandomDistance(img)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "plt.imshow(sharpenImage(enhanceContrastOfImage(colourToGrey(X_train[34000]))), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def saveToPickle(data, file_name=None, folder_path=None):\n",
    "    if not os.path.isdir(folder_path):\n",
    "        #print(\"Create \\\"preprocessed-data\\\" folder\")\n",
    "        os.mkdir(folder_path)\n",
    "    else:\n",
    "        print(\"\\\"preprocessed-data\\\" folder already exist\")\n",
    "\n",
    "    file_name = folder_path + file_name\n",
    "    if not os.path.exists(file_name):\n",
    "        try:\n",
    "            with open(file_name, 'wb') as f:\n",
    "                pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "                print(\"Create\", file_name)\n",
    "        except Exception as e:\n",
    "            print('Error: unable to save data to', file_name, 'because', e)\n",
    "            \n",
    "def loadPickle(file_name=None, folder_path=None):\n",
    "    file = folder_path + file_name\n",
    "    #print('Load')\n",
    "    if os.path.exists(file):\n",
    "        try:\n",
    "            with open(file, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "                print(\"Open\", file)\n",
    "        except Exception as e:\n",
    "            print('Error: unable to open data to', file, 'because', e)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "xData = copy.deepcopy(X_train)\n",
    "yData = copy.deepcopy(y_train)\n",
    "\n",
    "maxSampleNumbersAmongClasses = max(grouped_train_index.size())\n",
    "#argmaxSampleNumbersAmongClasses = np.argmax(grouped_train_index.size())\n",
    "\n",
    "necessaryNum = maxSampleNumbersAmongClasses - len(grouped_train_index.groups[15])\n",
    "print(len(grouped_train_index.groups[15]), necessaryNum)\n",
    "\n",
    "ccc = np.random.choice(grouped_train_index.groups[15], necessaryNum)\n",
    "randomImgInAClass = X_train[ccc]\n",
    "\n",
    "augmented_y = np.ones(necessaryNum) * 15\n",
    "\n",
    "for idx, img in enumerate(randomImgInAClass):\n",
    "    randomImgInAClass[idx] = transAndRotate(img)\n",
    "xData = np.vstack([xData, randomImgInAClass])\n",
    "yData = np.append(yData, augmented_y)\n",
    "\n",
    "print(labels[labels['ClassId']==15])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "print(ccc[1402])\n",
    "plt.imshow(X_train[8000])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "print(len(xData), len(ccc))\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=[10, 10])\n",
    "\n",
    "ggg = 36200 # 0, 5 -> 34799 -> 1401\n",
    "\n",
    "ax[0].imshow(enhanceContrastOfImage(xData[ggg+1]))\n",
    "ax[1].imshow(enhanceContrastOfImage(xData[ggg+2]))\n",
    "ax[2].imshow(enhanceContrastOfImage(xData[ggg+3]))\n",
    "ax[3].imshow(enhanceContrastOfImage(xData[ggg+4]))\n",
    "ax[4].imshow(enhanceContrastOfImage(xData[ggg+5]))\n",
    "\n",
    "print(labels[labels['ClassId']==yData[ggg]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "y_index = pd.DataFrame(data = augmented_y_train, columns=['ClassId'])\n",
    "grouped_y_index = y_index.groupby('ClassId')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#print(labels)\n",
    "print(len(grouped_train_index.groups[15]))\n",
    "print(len(grouped_y_index.groups[15]))\n",
    "\n",
    "aaa = grouped_y_index.groups[15]\n",
    "#print(len(aaa))\n",
    "fig, ax = plt.subplots(1, 5, figsize=[10, 10])\n",
    "\n",
    "ggg = 710 # 0, 5\n",
    "\n",
    "ax[0].imshow(enhanceContrastOfImage(augmented_X_train[aaa[ggg+1]]))\n",
    "ax[1].imshow(enhanceContrastOfImage(augmented_X_train[aaa[ggg+2]]))\n",
    "ax[2].imshow(enhanceContrastOfImage(augmented_X_train[aaa[ggg+3]]))\n",
    "ax[3].imshow(enhanceContrastOfImage(augmented_X_train[aaa[ggg+4]]))\n",
    "ax[4].imshow(enhanceContrastOfImage(augmented_X_train[aaa[ggg+5]]))\n",
    "\n",
    "print(aaa.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Using exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 34799/34799 [00:00<00:00, 209632.67it/s]\n",
      "100%|██████████████████████████████████| 4410/4410 [00:00<00:00, 200453.87it/s]\n",
      "100%|████████████████████████████████| 12630/12630 [00:00<00:00, 221578.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# colour to gray\n",
    "augmented_X_train = colourToGrey(X_train)\n",
    "augmented_X_valid = colourToGrey(X_valid)\n",
    "augmented_X_test = colourToGrey(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# normalisation\n",
    "augmented_X_train = normaliseColourImages(augmented_X_train)\n",
    "augmented_X_valid = normaliseColourImages(augmented_X_valid)\n",
    "augmented_X_test = normaliseColourImages(augmented_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#print(type(augmented_X_train[0, 0, 0]))\n",
    "print(np.max(augmented_X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Apply localized histogram localization  \n",
    "for i in tqdm(range(augmented_X_train.shape[0])):\n",
    "    augmented_X_train[i] = exposure.equalize_adapthist(augmented_X_train[i])\n",
    "    \n",
    "for i in tqdm(range(augmented_X_valid.shape[0])):\n",
    "    augmented_X_valid[i] = exposure.equalize_adapthist(augmented_X_valid[i])\n",
    "    \n",
    "for i in tqdm(range(augmented_X_test.shape[0])):\n",
    "    augmented_X_test[i] = exposure.equalize_adapthist(augmented_X_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "augmented_X_train = sharpenImage(augmented_X_train)\n",
    "augmented_X_valid = sharpenImage(augmented_X_valid)\n",
    "augmented_X_test = sharpenImage(augmented_X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Save\n",
    "saveToPickle(augmented_X_train, file_name='contrast_X_train.p', folder_path=\"./preprocessed-data/\")\n",
    "\n",
    "saveToPickle(augmented_X_valid, file_name='contrast_X_valid.p', folder_path=\"./preprocessed-data/\")\n",
    "\n",
    "saveToPickle(augmented_X_test, file_name='contrast_X_test.p', folder_path=\"./preprocessed-data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "augmented_X_train, augmented_y_train = makeBallanceBetweenClasses(grouped_train_index, augmented_X_train, y_train)\n",
    "augmented_X_valid, augmented_y_valid = makeBallanceBetweenClasses(grouped_valid_index, augmented_X_valid, y_valid)\n",
    "augmented_X_test, augmented_y_test = makeBallanceBetweenClasses(grouped_test_index, augmented_X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "saveToPickle(augmented_X_train, file_name='augmented_X_train.p', folder_path=\"./preprocessed-data/\")\n",
    "saveToPickle(augmented_y_train, file_name='augmented_y_train.p', folder_path=\"./preprocessed-data/\")\n",
    "\n",
    "saveToPickle(augmented_X_valid, file_name='augmented_X_valid.p', folder_path=\"./preprocessed-data/\")\n",
    "saveToPickle(augmented_y_valid, file_name='augmented_y_valid.p', folder_path=\"./preprocessed-data/\")\n",
    "\n",
    "saveToPickle(augmented_X_test, file_name='augmented_X_test.p', folder_path=\"./preprocessed-data/\")\n",
    "saveToPickle(augmented_y_test, file_name='augmented_y_test.p', folder_path=\"./preprocessed-data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the loaded processed X train dataset: (86430, 32, 32)\n",
      "The shape of the loaded processed y train dataset: (86430,)\n",
      "The shape of the loaded processed X valid dataset: (10320, 32, 32)\n",
      "The shape of the loaded processed y valid dataset: (10320,)\n",
      "The shape of the loaded processed X test dataset: (32250, 32, 32)\n",
      "The shape of the loaded processed y test dataset: (32250,)\n"
     ]
    }
   ],
   "source": [
    "## Load\n",
    "augmented_X_train = loadPickle(file_name='augmented_X_train.p', folder_path='./preprocessed-data/')\n",
    "augmented_y_train = loadPickle(file_name='augmented_y_train.p', folder_path='./preprocessed-data/')\n",
    "\n",
    "augmented_X_valid = loadPickle(file_name='augmented_X_valid.p', folder_path='./preprocessed-data/')\n",
    "augmented_y_valid = loadPickle(file_name='augmented_y_valid.p', folder_path='./preprocessed-data/')\n",
    "\n",
    "augmented_X_test = loadPickle(file_name='augmented_X_test.p', folder_path='./preprocessed-data/')\n",
    "augmented_y_test = loadPickle(file_name='augmented_y_test.p', folder_path='./preprocessed-data/')\n",
    "\n",
    "print('The shape of the loaded processed X train dataset:', augmented_X_train.shape)\n",
    "print('The shape of the loaded processed y train dataset:', augmented_y_train.shape)\n",
    "\n",
    "print('The shape of the loaded processed X valid dataset:', augmented_X_valid.shape)\n",
    "print('The shape of the loaded processed y valid dataset:', augmented_y_valid.shape)\n",
    "\n",
    "print('The shape of the loaded processed X test dataset:', augmented_X_test.shape)\n",
    "print('The shape of the loaded processed y test dataset:', augmented_y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Plot histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.hist(X_train[34000].ravel(), bins=256, histtype='step', color='black')\n",
    "plt.ticklabel_format(axis='y', style='scientific', scilimits=(0, 0))\n",
    "cdf = plt.twinx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.hist(augmented_X_train[34000].ravel(), bins=256, histtype='step', color='black')\n",
    "plt.ticklabel_format(axis='y', style='scientific', scilimits=(0, 0))\n",
    "cdf = plt.twinx()\n",
    "img_cdf, bins = exposure.cumulative_distribution(augmented_X_train[0], 256)\n",
    "cdf.plot(bins, img_cdf, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Conver colour image to gray scale image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train = colourToGrey(X_train)\n",
    "X_valid = colourToGrey(X_valid)\n",
    "X_test = colourToGrey(X_test)\n",
    "'''\n",
    "augmented_X_train = colourToGrey(augmented_X_train)\n",
    "augmented_X_valid = colourToGrey(augmented_X_valid)\n",
    "augmented_X_test = colourToGrey(augmented_X_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "np.shape(X_train.squeeze())\n",
    "#np.shape(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Augmentation with Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "augmented_X_train, augmented_y_train = makeBallanceBetweenClasses(grouped_train_index, X_train, y_train)\n",
    "augmented_X_valid, augmented_y_valid = makeBallanceBetweenClasses(grouped_valid_index, X_valid, y_valid)\n",
    "augmented_X_test, augmented_y_test = makeBallanceBetweenClasses(grouped_test_index, X_test, y_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "saveToPickle(augmented_X_train, file_name='augmented_X_train.p', folder_path=\"./preprocessed-data/\")\n",
    "saveToPickle(augmented_y_train, file_name='augmented_y_train.p', folder_path=\"./preprocessed-data/\")\n",
    "\n",
    "saveToPickle(augmented_X_valid, file_name='augmented_X_valid.p', folder_path=\"./preprocessed-data/\")\n",
    "saveToPickle(augmented_y_valid, file_name='augmented_y_valid.p', folder_path=\"./preprocessed-data/\")\n",
    "\n",
    "saveToPickle(augmented_X_test, file_name='augmented_X_test.p', folder_path=\"./preprocessed-data/\")\n",
    "saveToPickle(augmented_y_test, file_name='augmented_y_test.p', folder_path=\"./preprocessed-data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Load\n",
    "#augmented_X_train = loadPickle(file_name='augmented_X_train.p', folder_path='./preprocessed-data/')\n",
    "augmented_y_train = loadPickle(file_name='augmented_y_train.p', folder_path='./preprocessed-data/')\n",
    "\n",
    "#augmented_X_valid = loadPickle(file_name='augmented_X_valid.p', folder_path='./preprocessed-data/')\n",
    "augmented_y_valid = loadPickle(file_name='augmented_y_valid.p', folder_path='./preprocessed-data/')\n",
    "\n",
    "#augmented_X_test = loadPickle(file_name='augmented_X_test.p', folder_path='./preprocessed-data/')\n",
    "augmented_y_test = loadPickle(file_name='augmented_y_test.p', folder_path='./preprocessed-data/')\n",
    "\n",
    "#print('The shape of the loaded processed X train dataset:', augmented_X_train.shape)\n",
    "print('The shape of the loaded processed y train dataset:', augmented_y_train.shape)\n",
    "\n",
    "#print('The shape of the loaded processed X valid dataset:', augmented_X_valid.shape)\n",
    "print('The shape of the loaded processed y valid dataset:', augmented_y_valid.shape)\n",
    "\n",
    "#print('The shape of the loaded processed X test dataset:', augmented_X_test.shape)\n",
    "print('The shape of the loaded processed y test dataset:', augmented_y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Sharpen and Brigthen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#sharpenAndBright_Train = np.zeros(augmented_X_train.shape)\n",
    "#sharpenAndBright_Valid = np.zeros(augmented_X_valid.shape)\n",
    "#sharpenAndBright_Test = np.zeros(augmented_X_test.shape)\n",
    "\n",
    "for idx in tqdm(range(augmented_X_train.shape[0])):\n",
    "    augmented_X_train[idx] = sharpenImage(enhanceContrastOfImage(augmented_X_train[idx]))\n",
    "\n",
    "for idx in tqdm(range(augmented_X_valid.shape[0])):\n",
    "    augmented_X_valid[idx] = sharpenImage(enhanceContrastOfImage(augmented_X_valid[idx]))\n",
    "\n",
    "for idx in tqdm(range(augmented_X_test.shape[0])):\n",
    "    augmented_X_test[idx] = sharpenImage(enhanceContrastOfImage(augmented_X_test[idx]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Save\n",
    "saveToPickle(augmented_X_train, file_name='sharpenAndBright_Train.p', folder_path=\"./preprocessed-data/\")\n",
    "saveToPickle(augmented_X_valid, file_name='sharpenAndBright_Valid.p', folder_path=\"./preprocessed-data/\")\n",
    "saveToPickle(augmented_X_test, file_name='sharpenAndBright_Test.p', folder_path=\"./preprocessed-data/\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load\n",
    "augmented_X_train = loadPickle(file_name='sharpenAndBright_Train.p', folder_path='./preprocessed-data/')\n",
    "augmented_X_valid = loadPickle(file_name='sharpenAndBright_Valid.p', folder_path='./preprocessed-data/')\n",
    "augmented_X_test = loadPickle(file_name='sharpenAndBright_Test.p', folder_path='./preprocessed-data/')\n",
    "\n",
    "print('The shape of the loaded processed X train dataset:', augmented_X_train.shape)\n",
    "print('The shape of the loaded processed X valid dataset:', augmented_X_valid.shape)\n",
    "print('The shape of the loaded processed X test dataset:', augmented_X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "augmented_X_train = normaliseColourImages(augmented_X_train)\n",
    "augmented_X_valid = normaliseColourImages(augmented_X_valid)\n",
    "augmented_X_test = normaliseColourImages(augmented_X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Save\n",
    "saveToPickle(augmented_X_train, file_name='normalisation_Train.p', folder_path=\"./preprocessed-data/\")\n",
    "saveToPickle(augmented_X_valid, file_name='normalisation_Valid.p', folder_path=\"./preprocessed-data/\")\n",
    "saveToPickle(augmented_X_test, file_name='normalisation_Test.p', folder_path=\"./preprocessed-data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Load\n",
    "augmented_X_train = loadPickle(file_name='normalisation_Train.p', folder_path='./preprocessed-data/')\n",
    "augmented_X_valid = loadPickle(file_name='normalisation_Valid.p', folder_path='./preprocessed-data/')\n",
    "augmented_X_test = loadPickle(file_name='normalisation_Test.p', folder_path='./preprocessed-data/')\n",
    "\n",
    "print('The shape of the loaded processed X train dataset:', augmented_X_train.shape)\n",
    "print('The shape of the loaded processed X valid dataset:', augmented_X_valid.shape)\n",
    "print('The shape of the loaded processed X test dataset:', augmented_X_test.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Check whether image has a correct label 34799 -> 36629\n",
    "'''\n",
    "check = 45000\n",
    "plt.imshow(augmented_X_train[check])\n",
    "print(labels[labels['ClassId']==augmented_y_train[check]])\n",
    "'''\n",
    "'''\n",
    "fig, ax = plt.subplots(1, 5, figsize=[10, 10])\n",
    "\n",
    "checker = 77000 # 0, 5\n",
    "\n",
    "ax[0].imshow(augmented_X_train[checker+1])\n",
    "ax[1].imshow(augmented_X_train[checker+2])\n",
    "ax[2].imshow(augmented_X_train[checker+3])\n",
    "ax[3].imshow(augmented_X_train[checker+4])\n",
    "ax[4].imshow(augmented_X_train[checker+5])\n",
    "'''\n",
    "print(labels[labels['ClassId']==augmented_y_train[checker]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Plot class ID from index 0 to 9\n",
    "numOfRows = 5\n",
    "numOfColumns = 2\n",
    "\n",
    "ClassIdIndex = 0\n",
    "\n",
    "fig, ax = plt.subplots(numOfRows, numOfColumns, figsize=(10, 10))\n",
    "sns.set_style(style='white')\n",
    "\n",
    "for row in range(numOfRows):\n",
    "    for column in range(numOfColumns):\n",
    "        ax[row][column].imshow(augmented_X_train[firstTrainDataForEachClass.iloc[ClassIdIndex].values[0]])\n",
    "        ax[row][column].set_title(labels.loc[ClassIdIndex][1], fontsize=10)\n",
    "        ClassIdIndex += 1\n",
    "        \n",
    "        if ClassIdIndex >= 10:\n",
    "            break\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Plot classes from index 10 to 19\n",
    "numOfRows = 5\n",
    "numOfColumns = 2\n",
    "ClassIdIndex = 10\n",
    "\n",
    "fig, ax = plt.subplots(numOfRows, numOfColumns, figsize=(10, 10))\n",
    "sns.set_style(style='white')\n",
    "\n",
    "for row in range(numOfRows):\n",
    "    for column in range(numOfColumns):\n",
    "        ax[row][column].imshow(augmented_X_train[firstTrainDataForEachClass.iloc[ClassIdIndex].values[0]])\n",
    "        ax[row][column].set_title(labels.loc[ClassIdIndex][1], fontsize=10)\n",
    "        ClassIdIndex += 1\n",
    "        \n",
    "        if ClassIdIndex >= 20:\n",
    "            break\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Plot classes from index 20 to 29\n",
    "numOfRows = 5\n",
    "numOfColumns = 2\n",
    "ClassIdIndex = 20\n",
    "\n",
    "fig, ax = plt.subplots(numOfRows, numOfColumns, figsize=(10, 10))\n",
    "sns.set_style(style='white')\n",
    "\n",
    "for row in range(numOfRows):\n",
    "    for column in range(numOfColumns):\n",
    "        ax[row][column].imshow(augmented_X_train[firstTrainDataForEachClass.iloc[ClassIdIndex].values[0]])\n",
    "        ax[row][column].set_title(labels.loc[ClassIdIndex][1], fontsize=10)\n",
    "        ClassIdIndex += 1\n",
    "        \n",
    "        if ClassIdIndex >= 30:\n",
    "            break\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Plot classes from index 30 to 39\n",
    "numOfRows = 5\n",
    "numOfColumns = 2\n",
    "ClassIdIndex = 30\n",
    "\n",
    "fig, ax = plt.subplots(numOfRows, numOfColumns, figsize=(10, 10))\n",
    "sns.set_style(style='white')\n",
    "\n",
    "for row in range(numOfRows):\n",
    "    for column in range(numOfColumns):\n",
    "        ax[row][column].imshow(augmented_X_train[firstTrainDataForEachClass.iloc[ClassIdIndex].values[0]])\n",
    "        ax[row][column].set_title(labels.loc[ClassIdIndex][1], fontsize=10)\n",
    "        ClassIdIndex += 1\n",
    "        \n",
    "        if ClassIdIndex >= 40:\n",
    "            break\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Plot classes from index 40 to 42\n",
    "numOfRows = 2\n",
    "numOfColumns = 2\n",
    "\n",
    "fig, ax = plt.subplots(numOfRows, numOfColumns, figsize=(5, 5))\n",
    "sns.set_style(style='white')\n",
    "ClassIdIndex = 40\n",
    "for row in range(numOfRows):\n",
    "    for column in range(numOfColumns):\n",
    "        ax[row][column].imshow(augmented_X_train[firstTrainDataForEachClass.iloc[ClassIdIndex].values[0]])\n",
    "        ax[row][column].set_title(labels.loc[ClassIdIndex][1], fontsize=10)\n",
    "        ClassIdIndex += 1\n",
    "        \n",
    "        if ClassIdIndex >= 43:\n",
    "            break\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    if batch_size > 0:\n",
    "        assert len(features) == len(labels)\n",
    "\n",
    "        output_batches = []\n",
    "        sample_size = len(features)\n",
    "\n",
    "        for start_i in range(0, sample_size, batch_size):\n",
    "            end_i = start_i + batch_size\n",
    "            batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "            output_batches.append(batch)\n",
    "    else:\n",
    "        assert len(features) == len(labels)\n",
    "        \n",
    "        output_batches = []\n",
    "        sample_size = len(features)\n",
    "        \n",
    "        for start in range(sample_size):\n",
    "            batch = [features[start], labels[start]]\n",
    "            output_batches.append(batch)\n",
    "            \n",
    "    return output_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rate = 0.001\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 128\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x, dropout):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    '''\n",
    "    # SOLUTION: Layer 1: Convolutional. Input = 32x32x3. Output = 28x28x32.\n",
    "    conv_W = tf.Variable(tf.truncated_normal(shape=(1, 1, 3, 1), mean = mu, stddev = sigma))\n",
    "    conv_b = tf.Variable(tf.zeros(1))\n",
    "    conv   = tf.nn.conv2d(x, conv_W, strides=[1, 1, 1, 1], padding='VALID') + conv_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    conv = tf.nn.relu(conv)\n",
    "    conv = tf.nn.dropout(conv, dropout)\n",
    "    '''\n",
    "    # SOLUTION: Layer 1: Convolutional. Input = 32x32x3. Output = 28x28x32.\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 16), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(16))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "    #conv1   = tf.nn.conv2d(conv, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    #conv1 = tf.nn.dropout(conv1, dropout)\n",
    "    conv1 = tf.nn.dropout(conv1, 0.9)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 28x28x32. Output = 14x14x32.\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Layer 2: Convolutional. Output = 10x10x64.\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 16, 32), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(32))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2 = tf.nn.dropout(conv2, 0.8)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 10x10x64. Output = 5x5x64.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Flatten. Input = 5x5x64. Output = 1600.\n",
    "    fc0   = flatten(conv2)\n",
    "    \n",
    "    # SOLUTION: Layer 3: Fully Connected. Input = 1600. Output = 1200.\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(800, 400), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(400))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, 0.7)\n",
    "\n",
    "    # SOLUTION: Layer 4: Fully Connected. Input = 1200. Output = 840.\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(400, 200), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(200))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "    fc2 = tf.nn.dropout(fc2, 0.6)\n",
    "\n",
    "    # SOLUTION: Layer 5: Fully Connected. Input = 840. Output = 43.\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(200, 43), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(43))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x, dropout):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # SOLUTION: Layer 1: Convolutional. Input = 32x32x3. Output = 28x28x32.\n",
    "    conv_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 32), mean = mu, stddev = sigma))\n",
    "    conv_b = tf.Variable(tf.zeros(32))\n",
    "    conv   = tf.nn.conv2d(x, conv_W, strides=[1, 1, 1, 1], padding='VALID') + conv_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    conv = tf.nn.relu(conv)\n",
    "    conv = tf.nn.dropout(conv, 0.9)\n",
    "    \n",
    "    #conv = tf.nn.max_pool(conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    # SOLUTION: Layer 1: Convolutional. Input = 32x32x3. Output = 28x28x32.\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 32, 64), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(64))\n",
    "    conv1   = tf.nn.conv2d(conv, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "    #conv1   = tf.nn.conv2d(conv, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "    # SOLUTION: Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    #conv1 = tf.nn.dropout(conv1, dropout)\n",
    "    conv1 = tf.nn.dropout(conv1, 0.8)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 28x28x32. Output = 14x14x32.\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Layer 2: Convolutional. Output = 10x10x64.\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 64, 128), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(128))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2 = tf.nn.dropout(conv2, 0.7)\n",
    "\n",
    "    # SOLUTION: Pooling. Input = 10x10x64. Output = 5x5x64.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # SOLUTION: Flatten. Input = 5x5x64. Output = 1600.\n",
    "    fc0   = flatten(conv2)\n",
    "    \n",
    "    # SOLUTION: Layer 3: Fully Connected. Input = 1600. Output = 1200.\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(2048, 1024), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(1024))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, 0.6)\n",
    "\n",
    "    # SOLUTION: Layer 4: Fully Connected. Input = 1200. Output = 840.\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(1024, 512), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(512))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # SOLUTION: Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "    fc2 = tf.nn.dropout(fc2, 0.5)\n",
    "\n",
    "    # SOLUTION: Layer 5: Fully Connected. Input = 840. Output = 43.\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(512, 43), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(43))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    ## Layer 1\n",
    "    # Convolutional. Input = 32x32x3. Output = 32x32x32.\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(1, 1, 1, 32), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(32))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "    \n",
    "    # Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1 = tf.nn.dropout(conv1, 0.6)\n",
    "    \n",
    "    # Pooling. Input = 32x32x32. Output = 16x16x32.\n",
    "    conv1 = tf.nn.avg_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    ## Layer 2\n",
    "    # Convolutional. Input = 16x16x32. Output = 14x14x96.\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 32, 96), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(96))\n",
    "    conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "\n",
    "    # Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2 = tf.nn.dropout(conv2, 0.6)\n",
    "\n",
    "    # Pooling. Input = 14x14x96. Output = 7x7x96.\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    ## Layer 3 \n",
    "    #Convolutional. Input = 7x7x96. Output = 5x5x288.\n",
    "    conv3_W = tf.Variable(tf.truncated_normal(shape=(3, 3, 96, 288), mean = mu, stddev = sigma))\n",
    "    conv3_b = tf.Variable(tf.zeros(288))\n",
    "    conv3   = tf.nn.conv2d(conv2, conv3_W, strides=[1, 1, 1, 1], padding='VALID') + conv3_b\n",
    "    \n",
    "    # Activation.\n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "    conv3 = tf.nn.dropout(conv3, 0.6)\n",
    "\n",
    "    # Pooling. Input = 5x5x288. Output = 3x3x288.\n",
    "    conv3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # Flatten. Input = 3x3x288. Output = 1152.\n",
    "    fc0   = flatten(conv3)\n",
    "    \n",
    "    # Layer 3: Fully Connected. Input = 1152. Output = 1152.\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(1152, 1152), mean = mu, stddev = sigma))\n",
    "    fc1_b = tf.Variable(tf.zeros(1152))\n",
    "    fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # Activation.\n",
    "    fc1    = tf.nn.relu(fc1)\n",
    "    # fc1 = tf.nn.dropout(fc1, 0.6)\n",
    "\n",
    "    # Layer 4: Fully Connected. Input = 1152. Output = 576.\n",
    "    fc2_W  = tf.Variable(tf.truncated_normal(shape=(1152, 576), mean = mu, stddev = sigma))\n",
    "    fc2_b  = tf.Variable(tf.zeros(576))\n",
    "    fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # Activation.\n",
    "    fc2    = tf.nn.relu(fc2)\n",
    "    fc2 = tf.nn.dropout(fc2, 0.6)\n",
    "\n",
    "    # Layer 5: Fully Connected. Input = 576. Output = 43.\n",
    "    fc3_W  = tf.Variable(tf.truncated_normal(shape=(576, 43), mean = mu, stddev = sigma))\n",
    "    fc3_b  = tf.Variable(tf.zeros(43))\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train, Validate and Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A validation set can be used to assess how well the model is performing. A low accuracy on the training and validation\n",
    "sets imply underfitting. A high accuracy on the training set but low accuracy on the validation set implies overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Train your model here.\n",
    "### Calculate and report the accuracy on the training and validation set.\n",
    "### Once a final model architecture is selected, \n",
    "### the accuracy on the test set should be calculated and reported as well.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "## Features and Labels\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "one_hot_y = tf.one_hot(y, 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Training Pipeline\n",
    "logits =  LeNet(x, keep_prob)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Model Evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data, batch_size, sess):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "\n",
    "    for batch_x, batch_y in tqdm(batches(batch_size, X_data, y_data)):\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "## Reshape dataset\n",
    "augmented_X_train = np.reshape(augmented_X_train, (len(augmented_X_train), 32, 32, 1))\n",
    "augmented_X_valid = np.reshape(augmented_X_valid, (len(augmented_X_valid), 32, 32, 1))\n",
    "augmented_X_test = np.reshape(augmented_X_test, (len(augmented_X_test), 32, 32, 1))\n",
    "print(augmented_X_train[:128].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:41<00:00, 16.17it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 46.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.024\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:40<00:00, 16.62it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 48.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 2 ...\n",
      "Validation Accuracy = 0.022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:41<00:00, 16.45it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 47.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 3 ...\n",
      "Validation Accuracy = 0.022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:40<00:00, 16.58it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 47.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 4 ...\n",
      "Validation Accuracy = 0.023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:40<00:00, 16.58it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 48.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 5 ...\n",
      "Validation Accuracy = 0.022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:40<00:00, 16.52it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 47.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 6 ...\n",
      "Validation Accuracy = 0.023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:40<00:00, 16.59it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 47.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 7 ...\n",
      "Validation Accuracy = 0.023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:41<00:00, 16.48it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 47.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 8 ...\n",
      "Validation Accuracy = 0.025\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:41<00:00, 16.42it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 47.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 9 ...\n",
      "Validation Accuracy = 0.023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:40<00:00, 16.50it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 47.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 10 ...\n",
      "Validation Accuracy = 0.023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:41<00:00, 16.47it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 47.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 11 ...\n",
      "Validation Accuracy = 0.023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:40<00:00, 16.61it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 47.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 12 ...\n",
      "Validation Accuracy = 0.023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:40<00:00, 16.67it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 47.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 13 ...\n",
      "Validation Accuracy = 0.023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:40<00:00, 16.65it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 48.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 14 ...\n",
      "Validation Accuracy = 0.023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:40<00:00, 16.73it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 48.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 15 ...\n",
      "Validation Accuracy = 0.023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:41<00:00, 16.38it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 46.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 16 ...\n",
      "Validation Accuracy = 0.023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:41<00:00, 16.17it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 46.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 17 ...\n",
      "Validation Accuracy = 0.023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:41<00:00, 16.18it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 46.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 18 ...\n",
      "Validation Accuracy = 0.023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:41<00:00, 16.16it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 46.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 19 ...\n",
      "Validation Accuracy = 0.023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:41<00:00, 16.16it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 46.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 20 ...\n",
      "Validation Accuracy = 0.023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:41<00:00, 16.15it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 46.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 21 ...\n",
      "Validation Accuracy = 0.023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:41<00:00, 16.15it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 46.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 22 ...\n",
      "Validation Accuracy = 0.023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:41<00:00, 16.14it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 46.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 23 ...\n",
      "Validation Accuracy = 0.023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:41<00:00, 16.18it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 46.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 24 ...\n",
      "Validation Accuracy = 0.023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:41<00:00, 16.19it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 46.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 25 ...\n",
      "Validation Accuracy = 0.023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:41<00:00, 16.16it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 46.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 26 ...\n",
      "Validation Accuracy = 0.023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:41<00:00, 16.16it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 46.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 27 ...\n",
      "Validation Accuracy = 0.023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:41<00:00, 16.15it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 46.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 28 ...\n",
      "Validation Accuracy = 0.023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:41<00:00, 16.16it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 46.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 29 ...\n",
      "Validation Accuracy = 0.023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 676/676 [00:41<00:00, 16.17it/s]\n",
      "100%|██████████████████████████████████████████| 81/81 [00:01<00:00, 46.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 30 ...\n",
      "Validation Accuracy = 0.023\n",
      "\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "## Train the Model\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf. global_variables_initializer())\n",
    "    num_examples = len(augmented_X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    #print()\n",
    "    for i in range(EPOCHS):\n",
    "        shuffle_X, shuffle_y = shuffle(augmented_X_train, augmented_y_train)\n",
    "        for batch_x, batch_y in tqdm(batches(BATCH_SIZE, shuffle_X, shuffle_y)):\n",
    "            session.run(training_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: DROPOUT})\n",
    "        \n",
    "        validation_accuracy = evaluate(augmented_X_valid, augmented_y_valid, BATCH_SIZE, session)\n",
    "        print(\"EPOCH {0} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "    \n",
    "    saver.save(session, './weights')\n",
    "    print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## Step 3: Test a Model on New Images\n",
    "\n",
    "To give yourself more insight into how your model is working, download at least five pictures of German traffic signs from the web and use your model to predict the traffic sign type.\n",
    "\n",
    "You may find `signnames.csv` useful as it contains mappings from the class id (integer) to the actual sign name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load and Output the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "#saver = tf.train.Saver()\n",
    "with tf.Session() as session:\n",
    "    saver.restore(session, tf.train.latest_checkpoint('./'))\n",
    "    '''\n",
    "    new_saver = tf.train.import_meta_graph(\"weights.meta\")\n",
    "    new_saver.restore(session, tf.train.latest_checkpoint('./'))\n",
    "    all_vars = tf.get_collection('vars')\n",
    "    for v in all_vars:\n",
    "        v_ = session.run(v)\n",
    "        print(v_)\n",
    "    '''\n",
    "    saver.restore(session, './weights')\n",
    "    accuracy = evaluate(augmented_X_test, augmented_y_test, BATCH_SIZE, session)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "randomIndex = np.random.choice(X_test.shape[0], size=5)\n",
    "# newTestImages = X_test[randomIndex] # Colour\n",
    "newTestImages = X_test[randomIndex]\n",
    "#print(np.shape(newTestImages))\n",
    "\n",
    "for idx, img in enumerate(newTestImages):\n",
    "    newTestImages[idx] = sharpenImage(enhanceContrastOfImage((img)))\n",
    "print(np.shape(newTestImages))\n",
    "\n",
    "newTestLabels = labels.ix[y_test[randomIndex]]['SignName'].values\n",
    "sns.set_style(style='white')\n",
    "#print(newLabels)\n",
    "## show 5 new images\n",
    "for idx, image in enumerate(newTestImages):\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(image)\n",
    "    plt.title(newTestLabels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Predict the Sign Type for Each Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "newTestImages = X_test[randomIndex] # copy X_test for 5 images\n",
    "\n",
    "''' Colour\n",
    "img1 = cv2.cvtColor(cv2.imread('Test_images/no passing.jpg'), cv2.COLOR_BGR2RGB)\n",
    "img2 = cv2.cvtColor(cv2.imread('Test_images/right of way at the next intersection.jpg'), cv2.COLOR_BGR2RGB)\n",
    "img3 = cv2.cvtColor(cv2.imread('Test_images/speed limit 80km.jpg'), cv2.COLOR_BGR2RGB)\n",
    "img4 = cv2.cvtColor(cv2.imread('Test_images/yield.jpg'), cv2.COLOR_BGR2RGB)\n",
    "img5 = cv2.cvtColor(cv2.imread('Test_images/general causion.png'), cv2.COLOR_BGR2RGB)\n",
    "'''\n",
    "\n",
    "## Gray\n",
    "img1 = cv2.cvtColor(cv2.imread('./Images/Test_images/no passing.jpg'), cv2.COLOR_BGR2GRAY)\n",
    "img2 = cv2.cvtColor(cv2.imread('./Images/Test_images/right of way at the next intersection.jpg'), cv2.COLOR_BGR2GRAY)\n",
    "img3 = cv2.cvtColor(cv2.imread('./Images/Test_images/speed limit 80km.jpg'), cv2.COLOR_BGR2GRAY)\n",
    "img4 = cv2.cvtColor(cv2.imread('./Images/Test_images/yield.jpg'), cv2.COLOR_BGR2GRAY)\n",
    "img5 = cv2.cvtColor(cv2.imread('./Images/Test_images/general causion.png'), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "newTestImages[0] = cv2.resize(img1, (32, 32))\n",
    "newTestImages[1] = cv2.resize(img2, (32, 32)) \n",
    "newTestImages[2] = cv2.resize(img3, (32, 32)) \n",
    "newTestImages[3] = cv2.resize(img4, (32, 32)) \n",
    "newTestImages[4] = cv2.resize(img5, (32, 32)) \n",
    "\n",
    "fig, ax = plt.subplots(5, 1, figsize=[10, 10])\n",
    "\n",
    "newTestImages = np.reshape(newTestImages, (len(newTestImages), 32, 32, 1)) # Gray\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf. global_variables_initializer())\n",
    "    saver.restore(session, './weights')\n",
    "\n",
    "    output = session.run(logits, feed_dict={x: newTestImages, keep_prob: 1})\n",
    "\n",
    "correct_index = [9, 11, 5, 13, 18]\n",
    "correctLabels = labels.ix[correct_index]['SignName'].values\n",
    "\n",
    "predict_index = output.argmax(axis=1)\n",
    "predictLabels = labels.ix[predict_index]['SignName'].values\n",
    "\n",
    "ax[0].imshow(img1)\n",
    "ax[0].set_title('Correct: '+correctLabels[0]+' -> Predict: '+predictLabels[0])\n",
    "\n",
    "ax[1].imshow(img2)\n",
    "ax[1].set_title('Correct: '+correctLabels[1]+' -> Predict: '+predictLabels[1])\n",
    "\n",
    "ax[2].imshow(img3)\n",
    "ax[2].set_title('False: '+correctLabels[2]+' -> Predict: '+predictLabels[2])\n",
    "\n",
    "ax[3].imshow(img4)\n",
    "ax[3].set_title('Correct: '+correctLabels[3]+' -> Predict: '+predictLabels[3])\n",
    "\n",
    "ax[4].imshow(img5)\n",
    "ax[4].set_title('Correct: '+correctLabels[4]+' -> Predict: '+predictLabels[4])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Run the predictions here and use the model to output the prediction for each image.\n",
    "### Make sure to pre-process the images with the same pre-processing pipeline used earlier.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "#predict_logits =  LeNet(x, keep_prob)\n",
    "with tf.Session() as session:\n",
    "    session.run(tf. global_variables_initializer())\n",
    "    #loader = tf.train.import_meta_graph('./weights.meta')\n",
    "    #loader.restore(session, tf.train.latest_checkpoint('./'))\n",
    "    saver.restore(session, './weights')\n",
    "    #session = tf.get_default_session()\n",
    "    output = session.run(logits, feed_dict={x: newTestImages, keep_prob: DROPOUT})\n",
    "    #output = session.run(predict_logits, feed_dict={x: newTestImages, keep_prob: DROPOUT})\n",
    "print(output.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "predict_index = output.argmax(axis=1)\n",
    "predictLabels = labels.ix[predict_index]['SignName'].values\n",
    "\n",
    "sns.set_style(style='white')\n",
    "for idx, image in enumerate(newTestImages):\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(image)\n",
    "    plt.title('<Correct Label: {0}>, <Predict Label: {1}>'.format(newTestLabels[idx], predictLabels[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Analyze Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Calculate the accuracy for these 5 new images. \n",
    "### For example, if the model predicted 1 out of 5 signs correctly, it's 20% accurate on these new images.\n",
    "# Launch the graph\n",
    "\n",
    "def testEvaluate(X_data, y_data, sess):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    \n",
    "    assert len(X_data) == len(y_data)\n",
    "    \n",
    "    for idx in range(len(y_data)):\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: X_data, y: y_data, keep_prob: 1})\n",
    "        total_accuracy += accuracy\n",
    "    return total_accuracy / num_examples\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf. global_variables_initializer())\n",
    "    saver.restore(session, './weights')\n",
    "    \n",
    "    accuracyOnTest = testEvaluate(newTestImages, correct_index, session)\n",
    "    \n",
    "print(accuracyOnTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Top 5 Softmax Probabilities For Each Image Found on the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For each of the new images, print out the model's softmax probabilities to show the **certainty** of the model's predictions (limit the output to the top 5 probabilities for each image). [`tf.nn.top_k`](https://www.tensorflow.org/versions/r0.12/api_docs/python/nn.html#top_k) could prove helpful here. \n",
    "\n",
    "The example below demonstrates how tf.nn.top_k can be used to find the top k predictions for each image.\n",
    "\n",
    "`tf.nn.top_k` will return the values and indices (class ids) of the top k predictions. So if k=3, for each sign, it'll return the 3 largest probabilities (out of a possible 43) and the correspoding class ids.\n",
    "\n",
    "Take this numpy array as an example. The values in the array represent predictions. The array contains softmax probabilities for five candidate images with six possible classes. `tk.nn.top_k` is used to choose the three classes with the highest probability:\n",
    "\n",
    "```\n",
    "# (5, 6) array\n",
    "a = np.array([[ 0.24879643,  0.07032244,  0.12641572,  0.34763842,  0.07893497,\n",
    "         0.12789202],\n",
    "       [ 0.28086119,  0.27569815,  0.08594638,  0.0178669 ,  0.18063401,\n",
    "         0.15899337],\n",
    "       [ 0.26076848,  0.23664738,  0.08020603,  0.07001922,  0.1134371 ,\n",
    "         0.23892179],\n",
    "       [ 0.11943333,  0.29198961,  0.02605103,  0.26234032,  0.1351348 ,\n",
    "         0.16505091],\n",
    "       [ 0.09561176,  0.34396535,  0.0643941 ,  0.16240774,  0.24206137,\n",
    "         0.09155967]])\n",
    "```\n",
    "\n",
    "Running it through `sess.run(tf.nn.top_k(tf.constant(a), k=3))` produces:\n",
    "\n",
    "```\n",
    "TopKV2(values=array([[ 0.34763842,  0.24879643,  0.12789202],\n",
    "       [ 0.28086119,  0.27569815,  0.18063401],\n",
    "       [ 0.26076848,  0.23892179,  0.23664738],\n",
    "       [ 0.29198961,  0.26234032,  0.16505091],\n",
    "       [ 0.34396535,  0.24206137,  0.16240774]]), indices=array([[3, 0, 5],\n",
    "       [0, 1, 4],\n",
    "       [0, 5, 1],\n",
    "       [1, 3, 5],\n",
    "       [1, 4, 3]], dtype=int32))\n",
    "```\n",
    "\n",
    "Looking just at the first row we get `[ 0.34763842,  0.24879643,  0.12789202]`, you can confirm these are the 3 largest probabilities in `a`. You'll also notice `[3, 0, 5]` are the corresponding indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Print out the top five softmax probabilities for the predictions on the German traffic sign images found on the web. \n",
    "### Feel free to use as many code cells as needed.\n",
    "with tf.Session() as session:\n",
    "    FivesoftmaxProbabilities = session.run(tf.nn.top_k(tf.constant(output), sorted=True, k=5))\n",
    "\n",
    "print('5 Softmax Probabilities For Each Image:\\n', FivesoftmaxProbabilities[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "first = []\n",
    "for idx in FivesoftmaxProbabilities[1][0]:\n",
    "    first = np.append(first, firstTrainDataForEachClass.iloc[idx].values[0])\n",
    "first = first.astype(int)\n",
    "\n",
    "print('Label for the sampled image:', correctLabels[0])\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=[20, 20])\n",
    "\n",
    "sns.set_style(style='white')\n",
    "for idx, img_idx in enumerate(first):\n",
    "    ax[idx].imshow(sharpenImage(enhanceContrastOfImage(X_train[img_idx])))\n",
    "    label = labels.loc[FivesoftmaxProbabilities[1][0][idx]]\n",
    "    ax[idx].set_title(label[1], fontsize=16)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "first = []\n",
    "for idx in FivesoftmaxProbabilities[1][1]:\n",
    "    first = np.append(first, firstTrainDataForEachClass.iloc[idx].values[0])\n",
    "first = first.astype(int)\n",
    "\n",
    "print('Label for the sampled image:', correctLabels[1])\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=[20, 20])\n",
    "\n",
    "sns.set_style(style='white')\n",
    "for idx, img_idx in enumerate(first):\n",
    "    ax[idx].imshow(sharpenImage(enhanceContrastOfImage(X_train[img_idx])))\n",
    "    label = labels.loc[FivesoftmaxProbabilities[1][1][idx]]\n",
    "    ax[idx].set_title(label[1], fontsize=16)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "first = []\n",
    "for idx in FivesoftmaxProbabilities[1][2]:\n",
    "    first = np.append(first, firstTrainDataForEachClass.iloc[idx].values[0])\n",
    "first = first.astype(int)\n",
    "\n",
    "print('Label for the sampled image:', correctLabels[2])\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=[20, 20])\n",
    "\n",
    "sns.set_style(style='white')\n",
    "for idx, img_idx in enumerate(first):\n",
    "    ax[idx].imshow(sharpenImage(enhanceContrastOfImage(X_train[img_idx])))\n",
    "    label = labels.loc[FivesoftmaxProbabilities[1][2][idx]]\n",
    "    ax[idx].set_title(label[1], fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "first = []\n",
    "for idx in FivesoftmaxProbabilities[1][3]:\n",
    "    first = np.append(first, firstTrainDataForEachClass.iloc[idx].values[0])\n",
    "first = first.astype(int)\n",
    "\n",
    "print('Label for the sampled image:', correctLabels[3])\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=[20, 20])\n",
    "\n",
    "sns.set_style(style='white')\n",
    "for idx, img_idx in enumerate(first):\n",
    "    ax[idx].imshow(sharpenImage(enhanceContrastOfImage(X_train[img_idx])))\n",
    "    label = labels.loc[FivesoftmaxProbabilities[1][3][idx]]\n",
    "    ax[idx].set_title(label[1], fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "first = []\n",
    "for idx in FivesoftmaxProbabilities[1][4]:\n",
    "    first = np.append(first, firstTrainDataForEachClass.iloc[idx].values[0])\n",
    "first = first.astype(int)\n",
    "\n",
    "print('Label for the sampled image:', correctLabels[4])\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=[20, 20])\n",
    "\n",
    "sns.set_style(style='white')\n",
    "for idx, img_idx in enumerate(first):\n",
    "    ax[idx].imshow(sharpenImage(enhanceContrastOfImage(X_train[img_idx])))\n",
    "    label = labels.loc[FivesoftmaxProbabilities[1][4][idx]]\n",
    "    ax[idx].set_title(label[1], fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Project Writeup\n",
    "\n",
    "Once you have completed the code implementation, document your results in a project writeup using this [template](https://github.com/udacity/CarND-Traffic-Sign-Classifier-Project/blob/master/writeup_template.md) as a guide. The writeup can be in a markdown or pdf file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## Step 4 (Optional): Visualize the Neural Network's State with Test Images\n",
    "\n",
    " This Section is not required to complete but acts as an additional excersise for understaning the output of a neural network's weights. While neural networks can be a great learning device they are often referred to as a black box. We can understand what the weights of a neural network look like better by plotting their feature maps. After successfully training your neural network you can see what it's feature maps look like by plotting the output of the network's weight layers in response to a test stimuli image. From these plotted feature maps, it's possible to see what characteristics of an image the network finds interesting. For a sign, maybe the inner network feature maps react with high activation to the sign's boundary outline or to the contrast in the sign's painted symbol.\n",
    "\n",
    " Provided for you below is the function code that allows you to get the visualization output of any tensorflow weight layer you want. The inputs to the function should be a stimuli image, one used during training or a new one you provided, and then the tensorflow variable name that represents the layer's state during the training process, for instance if you wanted to see what the [LeNet lab's](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/601ae704-1035-4287-8b11-e2c2716217ad/concepts/d4aca031-508f-4e0b-b493-e7b706120f81) feature maps looked like for it's second convolutional layer you could enter conv2 as the tf_activation variable.\n",
    "\n",
    "For an example of what feature map outputs look like, check out NVIDIA's results in their paper [End-to-End Deep Learning for Self-Driving Cars](https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/) in the section Visualization of internal CNN State. NVIDIA was able to show that their network's inner weights had high activations to road boundary lines by comparing feature maps from an image with a clear path to one without. Try experimenting with a similar test to show that your trained network's weights are looking for interesting features, whether it's looking at differences in feature maps from images with or without a sign, or even what feature maps look like in a trained network vs a completely untrained one on the same sign image.\n",
    "\n",
    "<figure>\n",
    " <img src=\"visualize_cnn.png\" width=\"380\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Your output should look something like this (above)</p> \n",
    " </figcaption>\n",
    "</figure>\n",
    " <p></p> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Visualize your network's feature maps here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "# image_input: the test image being fed into the network to produce the feature maps\n",
    "# tf_activation: should be a tf variable name used during your training procedure that represents the calculated state of a specific weight layer\n",
    "# activation_min/max: can be used to view the activation contrast in more detail, by default matplot sets min and max to the actual min and max values of the output\n",
    "# plt_num: used to plot out multiple different weight feature map sets on the same block, just extend the plt number for each new feature map entry\n",
    "\n",
    "def outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1 ,plt_num=1):\n",
    "    # Here make sure to preprocess your image_input in a way your network expects\n",
    "    # with size, normalization, ect if needed\n",
    "    # image_input =\n",
    "    # Note: x should be the same name as your network's tensorflow data placeholder variable\n",
    "    # If you get an error tf_activation is not defined it may be having trouble accessing the variable from inside a function\n",
    "    activation = tf_activation.eval(session=sess,feed_dict={x : image_input})\n",
    "    featuremaps = activation.shape[3]\n",
    "    plt.figure(plt_num, figsize=(15,15))\n",
    "    for featuremap in range(featuremaps):\n",
    "        plt.subplot(6,8, featuremap+1) # sets the number of feature maps to show on each row and column\n",
    "        plt.title('FeatureMap ' + str(featuremap)) # displays the feature map number\n",
    "        if activation_min != -1 & activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin =activation_min, vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_min !=-1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin=activation_min, cmap=\"gray\")\n",
    "        else:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
