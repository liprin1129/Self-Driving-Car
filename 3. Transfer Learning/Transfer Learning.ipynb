{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Table of contents\n",
    "1. [Transfer Learning with TensorFlow](#Transfer Learning with TensorFlow)\n",
    "2. [ImageNet Dataset Inference](#ImageNet Inference)\n",
    "3. [AlexNet: Traffic Sign Inference](#Traffic Sign Inference)\n",
    "    1. [Feature Extraction](#Feature Extraction)\n",
    "    2. [Training the Feature Extractor](#Training the Feature Extractor)\n",
    "4. [Transfer Learning with VGG, Inception:GoogLeNet, and ResNet](#Transfer Learning with)\n",
    "    1. [Feature extraction with Cifar10 Aside](#Feature extraction with Cifar10 Aside)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1. Transfer Learning with TensorFlow <a name='Transfer Learning with TensorFlow'></a>\n",
    "\n",
    "__Transfer learning__ is the practice of starting with a network that has already been trained, and then applying that network to your own problem.\n",
    "\n",
    "Because neural networks can often take days or even weeks to train, transfer learning (i.e. starting with a network that somebody else has already trained) can greatly shorten training time.\n",
    "\n",
    "How do we apply transfer learning? Two popular methods are __feature extraction__ and __finetuning__.\n",
    "\n",
    "1. __Feature extraction__. Take a pretrained neural network and replace the final (classification) layer with a new classification layer, or perhaps even a small feedforward network that ends with a new classification layer. During training the weights in all the pre-trained layers are frozen, so only the weights for the new layer(s) are trained. In other words, the gradient doesn't flow backwards past the first new layer.\n",
    "2. __Finetuning__. This is similar to feature extraction except the pre-trained weights aren't frozen. The network is trained end-to-end.\n",
    "\n",
    "The labs in this lesson will focus on feature extraction since it's less computationally intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. ImageNet Dataset Inference <a name='ImageNet Inference'></a>\n",
    "\n",
    "<img src='Images/ImageNet Inference.png' width=200>\n",
    "$$ \\text{top: Poodle, bottom: Weasel} $$\n",
    "\n",
    "To start, run __imagenet_inference.py__, and verify that the network classifies the images correctly.\n",
    "\n",
    "```Python\n",
    "python imagenet_inference.py\n",
    "```\n",
    "\n",
    "The output should look similar to this:\n",
    "```Python\n",
    "Image 0\n",
    "miniature poodle: 0.389\n",
    "toy poodle: 0.223\n",
    "Bedlington terrier: 0.173\n",
    "standard poodle: 0.150\n",
    "komondor: 0.026\n",
    "\n",
    "Image 1\n",
    "weasel: 0.331\n",
    "polecat, fitch, foulmart, foumart, Mustela putorius: 0.280\n",
    "black-footed ferret, ferret, Mustela nigripes: 0.210\n",
    "mink: 0.081\n",
    "Arctic fox, white fox, Alopex lagopus: 0.027\n",
    "\n",
    "Time: 5.587 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3.  AlexNet: Traffic Sign Inference <a name='Traffic Sign Inference'></a>\n",
    "\n",
    "<img src='Images/Traffic Sign Inference.png' width=200>\n",
    "$$ \\text{top: construction sign, bottom: stop sign} $$\n",
    "\n",
    "Next, run python __traffic_sign_inference.py__, and see how well the classifier performs on the example construction and stop signs.\n",
    "\n",
    "OH NO!\n",
    "\n",
    "AlexNet expects a 227x227x3 pixel image, whereas the traffic sign images are 32x32x3 pixels.\n",
    "\n",
    "In order to feed the traffic sign images into AlexNet, you'll need to resize the images to the dimensions that AlexNet expects.\n",
    "\n",
    "You could resize the images outside of this program, but that approach doesn't scale well. Instead, use the [tf.image.resize_images](https://www.tensorflow.org/api_guides/python/image#Resizing) method to resize the images as they are fed into the model.\n",
    "\n",
    "Open up __traffic_sign_inference.py__ and complete the __TODO(s)__.\n",
    "\n",
    "The output should look similar to this:\n",
    "```Python\n",
    "Image 0\n",
    "screen, CRT screen: 0.051\n",
    "digital clock: 0.041\n",
    "laptop, laptop computer: 0.030\n",
    "balance beam, beam: 0.027\n",
    "parallel bars, bars: 0.023\n",
    "\n",
    "Image 1\n",
    "digital watch: 0.395\n",
    "digital clock: 0.275\n",
    "bottlecap: 0.115\n",
    "stopwatch, stop watch: 0.104\n",
    "combination lock: 0.086\n",
    "\n",
    "Time: 0.592 seconds\n",
    "```\n",
    "\n",
    "__Quiz:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The traffic signs are 32x32 so you\n",
    "have to resize them to be 227x227 before\n",
    "passing them to AlexNet.\n",
    "\"\"\"\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.misc import imread\n",
    "from caffe_classes import class_names\n",
    "from alexnet import AlexNet\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "# TODO: Resize the images so they can be fed into AlexNet.\n",
    "# HINT: Use `tf.image.resize_images` to resize the images\n",
    "resized = tf.image.resize_images(x, (227, 227))\n",
    "\n",
    "assert resized is not Ellipsis, \"resized needs to modify the placeholder image size to (227,227)\"\n",
    "probs = AlexNet(resized)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Read Images\n",
    "im1 = imread(\"construction.jpg\").astype(np.float32)\n",
    "im1 = im1 - np.mean(im1)\n",
    "\n",
    "im2 = imread(\"stop.jpg\").astype(np.float32)\n",
    "im2 = im2 - np.mean(im2)\n",
    "\n",
    "# Run Inference\n",
    "t = time.time()\n",
    "output = sess.run(probs, feed_dict={x: [im1, im2]})\n",
    "\n",
    "# Print Output\n",
    "for input_im_ind in range(output.shape[0]):\n",
    "    inds = np.argsort(output)[input_im_ind, :]\n",
    "    print(\"Image\", input_im_ind)\n",
    "    for i in range(5):\n",
    "        print(\"%s: %.3f\" % (class_names[inds[-1 - i]], output[input_im_ind, inds[-1 - i]]))\n",
    "    print()\n",
    "\n",
    "print(\"Time: %.3f seconds\" % (time.time() - t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Answer:__\n",
    "```Python\n",
    "\"\"\"\n",
    "The traffic signs are 32x32 so you\n",
    "have to resize them to be 227x227 before\n",
    "passing them to AlexNet.\n",
    "\"\"\"\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.misc import imread\n",
    "from caffe_classes import class_names\n",
    "from alexnet import AlexNet\n",
    "\n",
    "\n",
    "# placeholders\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "resized = tf.image.resize_images(x, (227, 227))\n",
    "\n",
    "probs = AlexNet(resized)\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Read Images\n",
    "im1 = imread(\"construction.jpg\").astype(np.float32)\n",
    "im1 = im1 - np.mean(im1)\n",
    "\n",
    "im2 = imread(\"stop.jpg\").astype(np.float32)\n",
    "im2 = im2 - np.mean(im2)\n",
    "\n",
    "# Run Inference\n",
    "t = time.time()\n",
    "output = sess.run(probs, feed_dict={x: [im1, im2]})\n",
    "\n",
    "# Print Output\n",
    "for input_im_ind in range(output.shape[0]):\n",
    "    inds = np.argsort(output)[input_im_ind, :]\n",
    "    print(\"Image\", input_im_ind)\n",
    "    for i in range(5):\n",
    "        print(\"%s: %.3f\" % (class_names[inds[-1 - i]], output[input_im_ind, inds[-1 - i]]))\n",
    "    print()\n",
    "\n",
    "print(\"Time: %.3f seconds\" % (time.time() - t))\n",
    "```\n",
    "\n",
    "The notable part being:\n",
    "```Python\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "resized = tf.image.resize_images(x, (227, 227))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.1. Feature Extraction <a name='Feature Extraction'></a>\n",
    "\n",
    "The problem is that AlexNet was trained on the [ImageNet](http://www.image-net.org/) database, which has 1000 classes of images. We can see the classes in the __caffe_classes.py__ file. None of those classes involves traffic signs.\n",
    "\n",
    "In order to successfully classify our traffic sign images, we need to remove the final, 1000-neuron classification layer and replace it with a new, 43-neuron classification layer.\n",
    "\n",
    "This is called _feature extraction_, because we're basically extracting the image features inferred by the penultimate layer, and passing these features to a new classification layer.\n",
    "\n",
    "Open __feature_extraction.py__ and complete the __TODO(s)__.\n",
    "\n",
    "The output will probably not precisely match the sample output below, since the output will depend on the (probably random) initialization of weights in the network. That being said, the output classes you see should be present in __signnames.csv__.\n",
    "\n",
    "```Python\n",
    "Image 0\n",
    "Double curve: 0.059\n",
    "Ahead only: 0.048\n",
    "Road work: 0.047\n",
    "Dangerous curve to the right: 0.047\n",
    "Road narrows on the right: 0.039\n",
    "\n",
    "Image 1\n",
    "General caution: 0.079\n",
    "No entry: 0.067\n",
    "Dangerous curve to the right: 0.054\n",
    "Speed limit (50km/h): 0.053\n",
    "Ahead only: 0.048\n",
    "\n",
    "Time: 0.500 seconds\n",
    "```\n",
    "\n",
    "__Quiz:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.misc import imread\n",
    "from alexnet import AlexNet\n",
    "\n",
    "sign_names = pd.read_csv('signnames.csv')\n",
    "nb_classes = 43\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "resized = tf.image.resize_images(x, (227, 227))\n",
    "\n",
    "# NOTE: By setting `feature_extract` to `True` we return\n",
    "# the second to last layer.\n",
    "fc7 = AlexNet(resized, feature_extract=True)\n",
    "# TODO: Define a new fully connected layer followed by a softmax activation to classify\n",
    "# the traffic signs. Assign the result of the softmax activation to `probs` below.\n",
    "# HINT: Look at the final layer definition in alexnet.py to get an idea of what this\n",
    "# should look like.\n",
    "shape = (fc7.get_shape().as_list()[-1], nb_classes)  # use this shape for the weight matrix\n",
    "\n",
    "fc8W = tf.Variable(tf.truncated_normal([4096, 43], stddev=1e-2))\n",
    "fc8b = tf.Variable(tf.zeros(nb_classes))\n",
    "logits = tf.nn.xw_plus_b(fc7, fc8W, fc8b)\n",
    "probs = tf.nn.softmax(logits)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Read Images\n",
    "im1 = imread(\"construction.jpg\").astype(np.float32)\n",
    "im1 = im1 - np.mean(im1)\n",
    "\n",
    "im2 = imread(\"stop.jpg\").astype(np.float32)\n",
    "im2 = im2 - np.mean(im2)\n",
    "\n",
    "# Run Inference\n",
    "t = time.time()\n",
    "output = sess.run(probs, feed_dict={x: [im1, im2]})\n",
    "\n",
    "# Print Output\n",
    "for input_im_ind in range(output.shape[0]):\n",
    "    inds = np.argsort(output)[input_im_ind, :]\n",
    "    print(\"Image\", input_im_ind)\n",
    "    for i in range(5):\n",
    "        print(\"%s: %.3f\" % (sign_names.ix[inds[-1 - i]][1], output[input_im_ind, inds[-1 - i]]))\n",
    "    print()\n",
    "\n",
    "print(\"Time: %.3f seconds\" % (time.time() - t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Answer:__\n",
    "```Python\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.misc import imread\n",
    "from alexnet import AlexNet\n",
    "\n",
    "sign_names = pd.read_csv('signnames.csv')\n",
    "nb_classes = 43\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "resized = tf.image.resize_images(x, (227, 227))\n",
    "\n",
    "# Returns the second final layer of the AlexNet model,\n",
    "# this allows us to redo the last layer specifically for \n",
    "# traffic signs model.\n",
    "fc7 = AlexNet(resized, feature_extract=True)\n",
    "shape = (fc7.get_shape().as_list()[-1], nb_classes)\n",
    "fc8W = tf.Variable(tf.truncated_normal(shape, stddev=1e-2))\n",
    "fc8b = tf.Variable(tf.zeros(nb_classes))\n",
    "logits = tf.nn.xw_plus_b(fc7, fc8W, fc8b)\n",
    "probs = tf.nn.softmax(logits)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Read Images\n",
    "im1 = imread(\"construction.jpg\").astype(np.float32)\n",
    "im1 = im1 - np.mean(im1)\n",
    "\n",
    "im2 = imread(\"stop.jpg\").astype(np.float32)\n",
    "im2 = im2 - np.mean(im2)\n",
    "\n",
    "# Run Inference\n",
    "t = time.time()\n",
    "output = sess.run(probs, feed_dict={x: [im1, im2]})\n",
    "\n",
    "# Print Output\n",
    "for input_im_ind in range(output.shape[0]):\n",
    "    inds = np.argsort(output)[input_im_ind, :]\n",
    "    print(\"Image\", input_im_ind)\n",
    "    for i in range(5):\n",
    "        print(\"%s: %.3f\" % (sign_names.ix[inds[-1 - i]][1], output[input_im_ind, inds[-1 - i]]))\n",
    "    print()\n",
    "\n",
    "print(\"Time: %.3f seconds\" % (time.time() - t))\n",
    "```\n",
    "\n",
    "The notable part being:\n",
    "\n",
    "```Python\n",
    "# Returns the second final layer of the AlexNet model,\n",
    "# this allows us to redo the last layer specifically for \n",
    "# traffic signs model.\n",
    "fc7 = AlexNet(resized, feature_extract=True)\n",
    "shape = (fc7.get_shape().as_list()[-1], nb_classes)\n",
    "fc8W = tf.Variable(tf.truncated_normal(shape, stddev=1e-2))\n",
    "fc8b = tf.Variable(tf.zeros(nb_classes))\n",
    "logits = tf.nn.xw_plus_b(fc7, fc8W, fc8b)\n",
    "probs = tf.nn.softmax(logits)\n",
    "```\n",
    "\n",
    "First, I figure out the shape of the final fully connected layer, in my opinion this is the trickiest part. To do that I have to figure out the size of the output from __fc7__. Since it's a fully connected layer I know it's shape will be 2D so the second (or last) element of the list will be the size of the output. __fc7.get_shape().as_list()[-1]__ does the trick. I then combine this with the number of classes for the Traffic Sign dataset to get the shape of the final fully connected layer, __shape = (fc7.get_shape().as_list()[-1], nb_classes)__. The rest of the code is just the standard way to define a fully connected in TensorFlow. Finally, I calculate the probabilities via softmax, __probs = tf.nn.softmax(logits)__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.2. Training the Feature Extractor <a name='Training the Feature Extractor'></a>\n",
    "\n",
    "The feature extractor we just created works, in the sense that data will flow through the network and result in predictions.\n",
    "\n",
    "But the predictions aren't accurate, because we haven't yet trained the new classification layer.\n",
    "\n",
    "In order to do that, we'll need to read in the training dataset and train the network.\n",
    "\n",
    "Training AlexNet (even just the final layer!) can take a little while, so if we don't have a GPU, running on a subset of the data is a good alternative. As a point of reference one epoch over the training set takes roughly 53-55 seconds with a GTX 970.\n",
    "\n",
    "Open up __train_feature_extraction.py__ and complete the __TODO(s)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from alexnet import AlexNet\n",
    "\n",
    "nb_classes = 43\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# TODO: Load traffic signs data.\n",
    "file_name = 'Data/train.p'\n",
    "with open(file_name, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# TODO: Split data into training and validation sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['features'], data['labels'], test_size=0.33, random_state=42)\n",
    "\n",
    "# TODO: Define placeholders and resize operation.\n",
    "features = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "labels = tf.placeholder(tf.int64, None)\n",
    "resized = tf.image.resize_images(features, (227, 227))\n",
    "\n",
    "# TODO: pass placeholder as first argument to `AlexNet`.\n",
    "fc7 = AlexNet(resized, feature_extract=True)\n",
    "# NOTE: `tf.stop_gradient` prevents the gradient from flowing backwards\n",
    "# past this point, keeping the weights before and up to `fc7` frozen.\n",
    "# This also makes training faster, less work to do!\n",
    "fc7 = tf.stop_gradient(fc7)\n",
    "\n",
    "# TODO: Add the final layer for traffic sign classification.\n",
    "shape = (fc7.get_shape().as_list()[-1], nb_classes)\n",
    "print(shape)\n",
    "fc8W = tf.Variable(tf.truncated_normal(shape, stddev=1e-2))\n",
    "fc8b = tf.Variable(tf.zeros(nb_classes))\n",
    "logits = tf.nn.xw_plus_b(fc7, fc8W, fc8b)\n",
    "probs = tf.nn.softmax(logits)\n",
    "\n",
    "# TODO: Define loss, training, accuracy operations.\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "loss_op = tf.reduce_mean(cross_entropy)\n",
    "opt = tf.train.AdamOptimizer()\n",
    "train_op = opt.minimize(loss_op, var_list=[fc8W, fc8b])\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# HINT: Look back at your traffic signs project solution, you may\n",
    "# be able to reuse some the code.\n",
    "preds = tf.arg_max(logits, 1)\n",
    "accuracy_op = tf.reduce_mean(tf.cast(tf.equal(preds, labels), tf.float32))\n",
    "\n",
    "# TODO: Train and evaluate the feature extraction model.\n",
    "def eval_on_data(X, y, sess):\n",
    "    total_acc = 0\n",
    "    total_loss = 0\n",
    "    for offset in range(0, X.shape[0], batch_size):\n",
    "        end = offset + batch_size\n",
    "        X_batch = X[offset:end]\n",
    "        y_batch = y[offset:end]\n",
    "\n",
    "        loss, acc = sess.run([loss_op, accuracy_op], feed_dict={features: X_batch, labels: y_batch})\n",
    "        total_loss += (loss * X_batch.shape[0])\n",
    "        total_acc += (acc * X_batch.shape[0])\n",
    "\n",
    "    return total_loss/X.shape[0], total_acc/X.shape[0]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "\n",
    "    for i in tqdm(range(epochs)):\n",
    "        # training\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        t0 = time.time()\n",
    "        for offset in range(0, X_train.shape[0], batch_size):\n",
    "            end = offset + batch_size\n",
    "            sess.run(train_op, feed_dict={features: X_train[offset:end], labels: y_train[offset:end]})\n",
    "\n",
    "        val_loss, val_acc = eval_on_data(X_val, y_val, sess)\n",
    "        print(\"Epoch\", i+1)\n",
    "        print(\"Time: %.3f seconds\" % (time.time() - t0))\n",
    "        print(\"Validation Loss =\", val_loss)\n",
    "        print(\"Validation Accuracy =\", val_acc)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Most of the code should look familiar.\n",
    "```Python\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels)\n",
    "loss_op = tf.reduce_mean(cross_entropy)\n",
    "opt = tf.train.AdamOptimizer()\n",
    "train_op = opt.minimize(loss_op, var_list=[fc8W, fc8b])\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "preds = tf.arg_max(logits, 1)\n",
    "accuracy_op = tf.reduce_mean(tf.cast(tf.equal(preds, labels), tf.float32))\n",
    "```\n",
    "\n",
    "Here are all the operations are defined (training, loss, accuracy, etc); eval_on_data is a utility function to calculate the loss and accuracy over a dataset to evaluate all at once.\n",
    "```Python\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # training\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        t0 = time.time()\n",
    "        for offset in range(0, X_train.shape[0], batch_size):\n",
    "            end = offset + batch_size\n",
    "            sess.run(train_op, feed_dict={features: X_train[offset:end], labels: y_train[offset:end]})\n",
    "\n",
    "        val_loss, val_acc = eval_on_data(X_val, y_val, sess)\n",
    "        print(\"Epoch\", i+1)\n",
    "        print(\"Time: %.3f seconds\" % (time.time() - t0))\n",
    "        print(\"Validation Loss =\", val_loss)\n",
    "        print(\"Validation Accuracy =\", val_acc)\n",
    "        print(\"\")\n",
    "```\n",
    "\n",
    "This is the main training procedure. As we can see we run __train_op__ on each batch. Additionally, before each epoch the training set is shuffled using __shuffle__. At the end of each epoch the validation loss and accuracy are recorded and printed out.\n",
    "\n",
    "Running the above code results in the following results after 10 epochs:\n",
    "```Python\n",
    "Epoch 10\n",
    "Time: 53.402 seconds\n",
    "Validation Loss = 0.126141663276\n",
    "Validation Accuracy = 0.966069240196\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 4. Transfer Learning with VGG, Inception (GoogLeNet) and ResNet <a name='Transfer Learning with'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this lab, we will continue exploring transfer learning. We've already explored feature extraction with AlexNet and TensorFlow. Next, we will use Keras to explore feature extraction with the VGG, Inception and ResNet architectures. The models we will use were trained for days or weeks on the [ImageNet dataset](http://www.image-net.org/). Thus, the weights encapsulate higher-level features learned from training on thousands of classes.\n",
    "\n",
    "There are some notable differences from AlexNet lab.\n",
    "1. We're using two datasets. First, the German Traffic Sign dataset, and second, the [Cifar10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "2. Bottleneck Features. Unless you have a very powerful GPU, running feature extraction on these models will take a significant amount of time, as you might have observed in the AlexNet lab. To make things easier we've precomputed bottleneck features for each (network, dataset) pair. This will allow us to experiment with feature extraction even on a modest CPU. We can think of bottleneck features as feature extraction but with caching. Because the base network weights are frozen during feature extraction, the output for an image will always be the same. Thus, once the image has already been passed through the network, we can cache and reuse the output.\n",
    "3. Furthermore, we've limited each class in both training datasets to 100 examples. The idea here is to push feature extraction a bit further. It also greatly reduces the download size and speeds up training. The validation files remain the same.\n",
    "\n",
    "The files are encoded as such:\n",
    "- {network}_{dataset}_100_bottleneck_features_train.p\n",
    "- {network}_{dataset}_bottleneck_features_validation.p\n",
    "\n",
    "\"network\", in the above filenames, can be one of 'vgg', 'inception', or 'resnet'.\n",
    "\n",
    "\"dataset\" can be either 'cifar10' or 'traffic'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4.1. Feature extraction with Cifar10 Aside <a name = \"Feature extraction with Cifar10 Aside\"></a>\n",
    "\n",
    "Cifar10 images are also (32, 32, 3) so the main thing we'll need to change is __the number of classes from 43 to 10__. Cifar10 also doesn't come with a validation set, so we can randomly split training data into a training and validation.\n",
    "\n",
    "We can easily download and load the Cifar10 dataset like this:\n",
    "```Python\n",
    "from keras.datasets import cifar10\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "# y_train.shape is 2d, (50000, 1). While Keras is smart enough to handle this\n",
    "# it's a good idea to flatten the array.\n",
    "y_train = y_train.reshape(-1)\n",
    "y_test = y_test.reshape(-1)\n",
    "```\n",
    "\n",
    "We can then use sklearn to split off part of the data into a validation set:\n",
    "\n",
    "```Python\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.3, random_state=42, stratify = y_train)\n",
    "```\n",
    "\n",
    "The Cifar10 dataset contains 10 classes:\n",
    "<img src='Images/Cifar10 Aside.png' width=600>\n",
    "$$ \\text{Overview of the Cifar10 dataset. Source: Alex Krizhevsky.} $$\n",
    "\n",
    "While the German Traffic Sign dataset has more classes, the Cifar10 dataset is harder to classify due to the complexity of the classes. A ship is drastically different from a frog, and a frog is nothing like a deer, etc. These are the kind of datasets where the advantage of using a pre-trained model will become much more apparent.\n",
    "\n",
    "Train the model on the Cifar10 dataset and record your results, keep these in mind when we train from the bottleneck features. Don't be discouraged if we get results significantly worse than the Traffic Sign dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__The code explanation:__\n",
    "\n",
    "```Python\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "# TODO: import Keras layers you need here\n",
    "```\n",
    "\n",
    "Nothing fancy here, just some imports we need to run the code, __pickle__ is used to load the bottleneck features.\n",
    "\n",
    "```Python\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# command line flags\n",
    "flags.DEFINE_string('training_file', '', \"Bottleneck features training file (.p)\")\n",
    "flags.DEFINE_string('validation_file', '', \"Bottleneck features validation file (.p)\")\n",
    "```\n",
    "\n",
    "Here we define some command line flags, this avoids having to manually open and edit the file if we want to change the files we train and validate our model with.\n",
    "\n",
    "Here's how we would run the file from the command line:\n",
    "```Python\n",
    "python feature_extraction.py --training_file vgg_cifar10_100_bottleneck_features_train.p --validation_file vgg_cifar10_bottleneck_features_validation.p\n",
    "```\n",
    "\n",
    "Running this program will train feature extraction with the VGG network/Cifar10 dataset bottleneck features. The 100 in __vgg_cifar10_100__ indicates this file has 100 examples per class.\n",
    "\n",
    "You could define additional flags if you wish. Possible candidates could be the batch size or the number of epochs.\n",
    "\n",
    "```Python\n",
    "def load_bottleneck_data(training_file, validation_file):\n",
    "    \"\"\"\n",
    "    Utility function to load bottleneck features.\n",
    "\n",
    "    Arguments:\n",
    "        training_file - String\n",
    "        validation_file - String\n",
    "    \"\"\"\n",
    "    print(\"Training file\", training_file)\n",
    "    print(\"Validation file\", validation_file)\n",
    "\n",
    "    with open(training_file, 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    with open(validation_file, 'rb') as f:\n",
    "        validation_data = pickle.load(f)\n",
    "\n",
    "    X_train = train_data['features']\n",
    "    y_train = train_data['labels']\n",
    "    X_val = validation_data['features']\n",
    "    y_val = validation_data['labels']\n",
    "\n",
    "    return X_train, y_train, X_val, y_va\n",
    "```\n",
    "\n",
    "A utility function that loads the bottleneck features from the pickled training and validation files.\n",
    "\n",
    "```Python\n",
    "def main(_):\n",
    "    # load bottleneck data\n",
    "    X_train, y_train, X_val, y_val = load_bottleneck_data(FLAGS.training_file, FLAGS.validation_file)\n",
    "\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    print(X_val.shape, y_val.shape)\n",
    "\n",
    "    # TODO: define your model and hyperparams here\n",
    "    # make sure to adjust the number of classes based on\n",
    "    # the dataset\n",
    "    # 10 for cifar10\n",
    "    # 43 for traffic\n",
    "\n",
    "    # TODO: train your model here\n",
    "\n",
    "\n",
    "# parses flags and calls the `main` function above\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n",
    "```\n",
    "\n",
    "This is where we'll define and train the model. Notice __FLAGS.training_file__ and __FLAGS.validation_file__ are passed into load_bottleneck_data. These refer to the command line flags defined earlier.\n",
    "\n",
    "Once we've trained the model, record the results. How do they compare to the results from the previous exercise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file Cifar10 Aside/vgg-100/vgg_cifar10_100_bottleneck_features_train.p\n",
      "Validation file Cifar10 Aside/vgg-100/vgg_cifar10_bottleneck_features_validation.p\n",
      "(1000, 1, 1, 512) (1000, 1)\n",
      "(10000, 1, 1, 512) (10000, 1)\n",
      "Train on 1000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 0s - loss: 5.2252 - acc: 0.1060 - val_loss: 4.2199 - val_acc: 0.1202\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 3.9449 - acc: 0.1340 - val_loss: 3.6155 - val_acc: 0.1517\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 3.4210 - acc: 0.1680 - val_loss: 3.2391 - val_acc: 0.1893\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.9897 - acc: 0.2300 - val_loss: 2.8189 - val_acc: 0.2441\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.5516 - acc: 0.2900 - val_loss: 2.4647 - val_acc: 0.3023\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.2075 - acc: 0.3490 - val_loss: 2.1931 - val_acc: 0.3540\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 1.9313 - acc: 0.4200 - val_loss: 1.9804 - val_acc: 0.3967\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 1.7110 - acc: 0.4750 - val_loss: 1.8028 - val_acc: 0.4346\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 1.5258 - acc: 0.5180 - val_loss: 1.6663 - val_acc: 0.4694\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 1.3803 - acc: 0.5550 - val_loss: 1.5585 - val_acc: 0.5007\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 1.2583 - acc: 0.5930 - val_loss: 1.4675 - val_acc: 0.5283\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 1.1545 - acc: 0.6280 - val_loss: 1.3948 - val_acc: 0.5497\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 1.0720 - acc: 0.6570 - val_loss: 1.3328 - val_acc: 0.5663\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.9979 - acc: 0.6760 - val_loss: 1.2818 - val_acc: 0.5828\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.9351 - acc: 0.7020 - val_loss: 1.2358 - val_acc: 0.5973\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.8794 - acc: 0.7160 - val_loss: 1.1990 - val_acc: 0.6075\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.8311 - acc: 0.7300 - val_loss: 1.1689 - val_acc: 0.6165\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.7878 - acc: 0.7500 - val_loss: 1.1420 - val_acc: 0.6274\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.7479 - acc: 0.7660 - val_loss: 1.1178 - val_acc: 0.6353\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.7129 - acc: 0.7750 - val_loss: 1.0968 - val_acc: 0.6429\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.6787 - acc: 0.7830 - val_loss: 1.0746 - val_acc: 0.6485\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.6478 - acc: 0.7950 - val_loss: 1.0563 - val_acc: 0.6547\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.6204 - acc: 0.8040 - val_loss: 1.0410 - val_acc: 0.6594\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.5935 - acc: 0.8130 - val_loss: 1.0276 - val_acc: 0.6624\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.5691 - acc: 0.8190 - val_loss: 1.0146 - val_acc: 0.6667\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.5460 - acc: 0.8250 - val_loss: 1.0022 - val_acc: 0.6720\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.5242 - acc: 0.8310 - val_loss: 0.9904 - val_acc: 0.6766\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.5039 - acc: 0.8410 - val_loss: 0.9801 - val_acc: 0.6799\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.4850 - acc: 0.8430 - val_loss: 0.9704 - val_acc: 0.6827\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.4671 - acc: 0.8510 - val_loss: 0.9620 - val_acc: 0.6845\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.4496 - acc: 0.8560 - val_loss: 0.9541 - val_acc: 0.6880\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.4341 - acc: 0.8610 - val_loss: 0.9474 - val_acc: 0.6916\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.4188 - acc: 0.8730 - val_loss: 0.9391 - val_acc: 0.6976\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.4041 - acc: 0.8800 - val_loss: 0.9327 - val_acc: 0.6986\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3915 - acc: 0.8840 - val_loss: 0.9278 - val_acc: 0.6975\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3780 - acc: 0.8930 - val_loss: 0.9208 - val_acc: 0.7023\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3649 - acc: 0.8980 - val_loss: 0.9154 - val_acc: 0.7031\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3535 - acc: 0.9000 - val_loss: 0.9100 - val_acc: 0.7063\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3418 - acc: 0.9070 - val_loss: 0.9051 - val_acc: 0.7062\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3311 - acc: 0.9110 - val_loss: 0.9009 - val_acc: 0.7072\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3210 - acc: 0.9130 - val_loss: 0.8964 - val_acc: 0.7087\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3115 - acc: 0.9200 - val_loss: 0.8925 - val_acc: 0.7100\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.3017 - acc: 0.9240 - val_loss: 0.8888 - val_acc: 0.7124\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.2932 - acc: 0.9260 - val_loss: 0.8852 - val_acc: 0.7130\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.2850 - acc: 0.9300 - val_loss: 0.8816 - val_acc: 0.7149\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.2768 - acc: 0.9340 - val_loss: 0.8779 - val_acc: 0.7167\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.2695 - acc: 0.9390 - val_loss: 0.8751 - val_acc: 0.7173\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.2617 - acc: 0.9420 - val_loss: 0.8729 - val_acc: 0.7173\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.2547 - acc: 0.9470 - val_loss: 0.8704 - val_acc: 0.7196\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 0.2484 - acc: 0.9490 - val_loss: 0.8670 - val_acc: 0.7210\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "# TODO: import Keras layers you need here\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "#flags = tf.app.flags\n",
    "#FLAGS = flags.FLAGS\n",
    "\n",
    "# command line flags\n",
    "#flags.DEFINE_string('training_file', '', \"Bottleneck features training file (.p)\")\n",
    "#flags.DEFINE_string('validation_file', '', \"Bottleneck features validation file (.p)\")\n",
    "#flags.DEFINE_integer('epochs', 50, \"The number of epochs.\")\n",
    "#flags.DEFINE_integer('batch_size', 256, \"The batch size.\")\n",
    "\n",
    "training_file = 'Cifar10 Aside/vgg-100/vgg_cifar10_100_bottleneck_features_train.p'\n",
    "validation_file = 'Cifar10 Aside/vgg-100/vgg_cifar10_bottleneck_features_validation.p'\n",
    "epochs = 50\n",
    "batch_size = 256\n",
    "\n",
    "def load_bottleneck_data(training_file, validation_file):\n",
    "    \"\"\"\n",
    "    Utility function to load bottleneck features.\n",
    "\n",
    "    Arguments:\n",
    "        training_file - String\n",
    "        validation_file - String\n",
    "    \"\"\"\n",
    "    print(\"Training file\", training_file)\n",
    "    print(\"Validation file\", validation_file)\n",
    "\n",
    "    with open(training_file, 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    with open(validation_file, 'rb') as f:\n",
    "        validation_data = pickle.load(f)\n",
    "\n",
    "    X_train = train_data['features']\n",
    "    y_train = train_data['labels']\n",
    "    X_val = validation_data['features']\n",
    "    y_val = validation_data['labels']\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    # load bottleneck data\n",
    "    #X_train, y_train, X_val, y_val = load_bottleneck_data(FLAGS.training_file, FLAGS.validation_file)\n",
    "    X_train, y_train, X_val, y_val = load_bottleneck_data(training_file, validation_file)\n",
    "    \n",
    "    print(X_train.shape, y_train.shape)\n",
    "    print(X_val.shape, y_val.shape)\n",
    "\n",
    "    nb_classes = len(np.unique(y_train))\n",
    "    # TODO: define your model and hyperparams here\n",
    "    # make sure to adjust the number of classes based on\n",
    "    # the dataset\n",
    "    # 10 for cifar10\n",
    "    # 43 for traffic\n",
    "    input_shape = X_train.shape[1:]\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = Flatten()(inp)\n",
    "    x = Dense(nb_classes, activation='softmax')(x)\n",
    "    model = Model(inp, x)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # TODO: train your model here\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val), shuffle=True)\n",
    "\n",
    "\n",
    "# parses flags and calls the `main` function above\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Answer :__\n",
    "\n",
    "```Python\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# command line flags\n",
    "flags.DEFINE_string('training_file', '', \"Bottleneck features training file (.p)\")\n",
    "flags.DEFINE_string('validation_file', '', \"Bottleneck features validation file (.p)\")\n",
    "flags.DEFINE_integer('epochs', 50, \"The number of epochs.\")\n",
    "flags.DEFINE_integer('batch_size', 256, \"The batch size.\")\n",
    "\n",
    "\n",
    "def load_bottleneck_data(training_file, validation_file):\n",
    "    \"\"\"\n",
    "    Utility function to load bottleneck features.\n",
    "\n",
    "    Arguments:\n",
    "        training_file - String\n",
    "        validation_file - String\n",
    "    \"\"\"\n",
    "    print(\"Training file\", training_file)\n",
    "    print(\"Validation file\", validation_file)\n",
    "\n",
    "    with open(training_file, 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    with open(validation_file, 'rb') as f:\n",
    "        validation_data = pickle.load(f)\n",
    "\n",
    "    X_train = train_data['features']\n",
    "    y_train = train_data['labels']\n",
    "    X_val = validation_data['features']\n",
    "    y_val = validation_data['labels']\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    # load bottleneck data\n",
    "    X_train, y_train, X_val, y_val = load_bottleneck_data(FLAGS.training_file, FLAGS.validation_file)\n",
    "\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    print(X_val.shape, y_val.shape)\n",
    "\n",
    "    nb_classes = len(np.unique(y_train))\n",
    "\n",
    "    # define model\n",
    "    input_shape = X_train.shape[1:]\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = Flatten()(inp)\n",
    "    x = Dense(nb_classes, activation='softmax')(x)\n",
    "    model = Model(inp, x)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # train model\n",
    "    model.fit(X_train, y_train, epochs=FLAGS.epochs, batch_size=FLAGS.batch_size, validation_data=(X_val, y_val), shuffle=True)\n",
    "\n",
    "\n",
    "# parses flags and calls the `main` function above\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n",
    "Let's go over the changes.\n",
    "\n",
    "import numpy as np\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "from keras.models import Model\n",
    "Import the additional libraries required.\n",
    "\n",
    "flags.DEFINE_integer('epochs', 50, \"The number of epochs.\")\n",
    "flags.DEFINE_integer('batch_size', 256, \"The batch size.\")\n",
    "I add a couple of command-line flags to set the number of epochs and batch size. This is more for convenience than anything else.\n",
    "\n",
    "nb_classes = len(np.unique(y_train))\n",
    "```\n",
    "\n",
    "Import the additional libraries required.\n",
    "```Python\n",
    "import numpy as np\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "from keras.models import Model\n",
    "```\n",
    "\n",
    "add a couple of command-line flags to set the number of epochs and batch size. This is more for convenience than anything else.\n",
    "```Python\n",
    "flags.DEFINE_integer('epochs', 50, \"The number of epochs.\")\n",
    "flags.DEFINE_integer('batch_size', 256, \"The batch size.\")\n",
    "```\n",
    "\n",
    "Here, the number of classes for the dataset can be found. __np.unique__ returns all the unique elements of a numpy array. The elements of __y\\_train__ are integers, __0-9__ for Cifar10 and __0\\-42__ for Traffic Signs. So, when combined with __len__ we get back the number of classes.\n",
    "```Python\n",
    "nb_classes = len(np.unique(y_train))\n",
    "```\n",
    "\n",
    "Very simple model is determined, a linear layer (Dense in Keras terms) followed by a softmax activation. The Adam optimizer is used.\n",
    "```Python\n",
    "# define model\n",
    "input_shape = X_train.shape[1:]\n",
    "inp = Input(shape=input_shape)\n",
    "x = Flatten()(inp)\n",
    "x = Dense(nb_classes, activation='softmax')(x)\n",
    "model = Model(inp, x)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "Finally, the model is trained. Notice here __FLAGS.epochs__ and __FLAGS.batch\\_size__ are used.\n",
    "\n",
    "After 50 epochs these are the results for each model:\n",
    "\n",
    "VGG\n",
    "```Python\n",
    "Epoch 50/50\n",
    "1000/1000 [==============================] - 0s - loss: 0.2418 - acc: 0.9540 - val_loss: 0.8759 - val_acc: 0.7235\n",
    "```\n",
    "\n",
    "Inception\n",
    "```Python\n",
    "Epoch 50/50\n",
    "1000/1000 [==============================] - 0s - loss: 0.0887 - acc: 1.0000 - val_loss: 1.0428 - val_acc: 0.6556\n",
    "```\n",
    "\n",
    "ResNet\n",
    "```Python\n",
    "Epoch 50/50\n",
    "1000/1000 [==============================] - 0s - loss: 0.0790 - acc: 1.0000 - val_loss: 0.8005 - val_acc: 0.7347\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Feature extraction with Traffic signs <a name = \"Feature extraction with Traffic signs\"></a>\n",
    "\n",
    "The ImageNet dataset with 1000 classes had no traffic sign images. Will the high-level features learned still be transferable to such a different dataset?\n",
    "\n",
    "Staying with the VGG example:\n",
    "```Python\n",
    "python feature_extraction.py --training_file bottlenecks/vgg_traffic_100_bottleneck_features_train.p --validation_file bottlenecks/vgg_traffic_bottleneck_features_validation.p\n",
    "```\n",
    "\n",
    "The only difference in the filename is we're swapping \"cifar10\" with \"traffic\".\n",
    "\n",
    "Depending on how we wrote our solution, we may have to manually change the number of classes back to 43 as well.\n",
    "\n",
    "How do the feature extraction results compare with the results from the Traffic Sign project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file Cifar10 Aside/vgg-100/vgg_traffic_100_bottleneck_features_train.p\n",
      "Validation file Cifar10 Aside/vgg-100/vgg_traffic_bottleneck_features_validation.p\n",
      "(4300, 1, 1, 512) (4300,)\n",
      "(12939, 1, 1, 512) (12939,)\n",
      "Train on 4300 samples, validate on 12939 samples\n",
      "Epoch 1/50\n",
      "4300/4300 [==============================] - 0s - loss: 5.4431 - acc: 0.0609 - val_loss: 4.0114 - val_acc: 0.1150\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/50\n",
      "4300/4300 [==============================] - 0s - loss: 3.2020 - acc: 0.2056 - val_loss: 2.6750 - val_acc: 0.2588\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/50\n",
      "4300/4300 [==============================] - 0s - loss: 2.1598 - acc: 0.3835 - val_loss: 2.0001 - val_acc: 0.4144\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/50\n",
      "4300/4300 [==============================] - 0s - loss: 1.6234 - acc: 0.5247 - val_loss: 1.6609 - val_acc: 0.4992\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/50\n",
      "4300/4300 [==============================] - 0s - loss: 1.2920 - acc: 0.6237 - val_loss: 1.4139 - val_acc: 0.5754\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 6/50\n",
      "4300/4300 [==============================] - 0s - loss: 1.0668 - acc: 0.6974 - val_loss: 1.2594 - val_acc: 0.6247\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 7/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.9110 - acc: 0.7519 - val_loss: 1.1523 - val_acc: 0.6539\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 8/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.7942 - acc: 0.7867 - val_loss: 1.0457 - val_acc: 0.6851\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 9/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.6983 - acc: 0.8263 - val_loss: 0.9925 - val_acc: 0.7020\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 10/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.6258 - acc: 0.8514 - val_loss: 0.9464 - val_acc: 0.7213\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 11/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.5649 - acc: 0.8714 - val_loss: 0.8576 - val_acc: 0.7475\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 12/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.5119 - acc: 0.8916 - val_loss: 0.8300 - val_acc: 0.7523\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 13/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.4670 - acc: 0.9028 - val_loss: 0.7843 - val_acc: 0.7655\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 14/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.4320 - acc: 0.9140 - val_loss: 0.7591 - val_acc: 0.7768\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 15/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.3996 - acc: 0.9219 - val_loss: 0.7171 - val_acc: 0.7874\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 16/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.3698 - acc: 0.9314 - val_loss: 0.6963 - val_acc: 0.7947\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 17/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.3455 - acc: 0.9407 - val_loss: 0.6764 - val_acc: 0.8009\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 18/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.3242 - acc: 0.9447 - val_loss: 0.6605 - val_acc: 0.8055\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 19/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.3049 - acc: 0.9488 - val_loss: 0.6390 - val_acc: 0.8107\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 20/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.2881 - acc: 0.9533 - val_loss: 0.6195 - val_acc: 0.8170\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 21/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.2700 - acc: 0.9567 - val_loss: 0.6076 - val_acc: 0.8209\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 22/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.2569 - acc: 0.9595 - val_loss: 0.5869 - val_acc: 0.8253\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 23/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.2422 - acc: 0.9614 - val_loss: 0.5798 - val_acc: 0.8260\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 24/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.2300 - acc: 0.9677 - val_loss: 0.5632 - val_acc: 0.8330\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 25/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.2181 - acc: 0.9712 - val_loss: 0.5603 - val_acc: 0.8328\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 26/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.2084 - acc: 0.9707 - val_loss: 0.5491 - val_acc: 0.8347\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 27/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.1994 - acc: 0.9730 - val_loss: 0.5416 - val_acc: 0.8392\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 28/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.1908 - acc: 0.9751 - val_loss: 0.5219 - val_acc: 0.8456\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 29/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.1819 - acc: 0.9756 - val_loss: 0.5164 - val_acc: 0.8471\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 30/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.1749 - acc: 0.9793 - val_loss: 0.5118 - val_acc: 0.8479\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 31/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.1677 - acc: 0.9816 - val_loss: 0.5061 - val_acc: 0.8505\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 32/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.1603 - acc: 0.9812 - val_loss: 0.4974 - val_acc: 0.8526\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 33/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.1536 - acc: 0.9840 - val_loss: 0.4938 - val_acc: 0.8547\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 34/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.1476 - acc: 0.9853 - val_loss: 0.4899 - val_acc: 0.8535\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 35/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.1429 - acc: 0.9865 - val_loss: 0.4921 - val_acc: 0.8522\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 36/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.1385 - acc: 0.9874 - val_loss: 0.4731 - val_acc: 0.8610\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 37/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.1318 - acc: 0.9881 - val_loss: 0.4719 - val_acc: 0.8573\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 38/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.1270 - acc: 0.9879 - val_loss: 0.4666 - val_acc: 0.8621\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 39/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.1227 - acc: 0.9886 - val_loss: 0.4640 - val_acc: 0.8601\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 40/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.1186 - acc: 0.9902 - val_loss: 0.4595 - val_acc: 0.8639\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 41/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.1151 - acc: 0.9916 - val_loss: 0.4533 - val_acc: 0.8626\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 42/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.1108 - acc: 0.9912 - val_loss: 0.4563 - val_acc: 0.8635\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 43/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.1074 - acc: 0.9930 - val_loss: 0.4472 - val_acc: 0.8647\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 44/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.1041 - acc: 0.9928 - val_loss: 0.4502 - val_acc: 0.8651\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 45/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.1012 - acc: 0.9935 - val_loss: 0.4409 - val_acc: 0.8683\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 46/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.0981 - acc: 0.9937 - val_loss: 0.4401 - val_acc: 0.8678\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 47/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.0946 - acc: 0.9947 - val_loss: 0.4387 - val_acc: 0.8676\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 48/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.0923 - acc: 0.9949 - val_loss: 0.4386 - val_acc: 0.8670\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 49/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.0894 - acc: 0.9951 - val_loss: 0.4347 - val_acc: 0.8694\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 50/50\n",
      "4300/4300 [==============================] - 0s - loss: 0.0876 - acc: 0.9953 - val_loss: 0.4333 - val_acc: 0.8678\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "# TODO: import Keras layers you need here\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "#flags = tf.app.flags\n",
    "#FLAGS = flags.FLAGS\n",
    "\n",
    "# command line flags\n",
    "#flags.DEFINE_string('training_file', '', \"Bottleneck features training file (.p)\")\n",
    "#flags.DEFINE_string('validation_file', '', \"Bottleneck features validation file (.p)\")\n",
    "#flags.DEFINE_integer('epochs', 50, \"The number of epochs.\")\n",
    "#flags.DEFINE_integer('batch_size', 256, \"The batch size.\")\n",
    "\n",
    "training_file = 'Cifar10 Aside/vgg-100/vgg_traffic_100_bottleneck_features_train.p'\n",
    "validation_file = 'Cifar10 Aside/vgg-100/vgg_traffic_bottleneck_features_validation.p'\n",
    "epochs = 50\n",
    "batch_size = 256\n",
    "\n",
    "def load_bottleneck_data(training_file, validation_file):\n",
    "    \"\"\"\n",
    "    Utility function to load bottleneck features.\n",
    "\n",
    "    Arguments:\n",
    "        training_file - String\n",
    "        validation_file - String\n",
    "    \"\"\"\n",
    "    print(\"Training file\", training_file)\n",
    "    print(\"Validation file\", validation_file)\n",
    "\n",
    "    with open(training_file, 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    with open(validation_file, 'rb') as f:\n",
    "        validation_data = pickle.load(f)\n",
    "\n",
    "    X_train = train_data['features']\n",
    "    y_train = train_data['labels']\n",
    "    X_val = validation_data['features']\n",
    "    y_val = validation_data['labels']\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    # load bottleneck data\n",
    "    #X_train, y_train, X_val, y_val = load_bottleneck_data(FLAGS.training_file, FLAGS.validation_file)\n",
    "    X_train, y_train, X_val, y_val = load_bottleneck_data(training_file, validation_file)\n",
    "    \n",
    "    print(X_train.shape, y_train.shape)\n",
    "    print(X_val.shape, y_val.shape)\n",
    "\n",
    "    nb_classes = len(np.unique(y_train))\n",
    "    # TODO: define your model and hyperparams here\n",
    "    # make sure to adjust the number of classes based on\n",
    "    # the dataset\n",
    "    # 10 for cifar10\n",
    "    # 43 for traffic\n",
    "    input_shape = X_train.shape[1:]\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = Flatten()(inp)\n",
    "    x = Dense(nb_classes, activation='softmax')(x)\n",
    "    model = Model(inp, x)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # TODO: train your model here\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val), shuffle=True)\n",
    "\n",
    "\n",
    "# parses flags and calls the `main` function above\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer :__\n",
    "I used the same code from the Cifar10 solution.\n",
    "\n",
    "After 50 epochs these are the results for each model:\n",
    "\n",
    "VGG\n",
    "```Python\n",
    "Epoch 50/50\n",
    "4300/4300 [==============================] - 0s - loss: 0.0873 - acc: 0.9958 - val_loss: 0.4368 - val_acc: 0.8666\n",
    "```\n",
    "\n",
    "Inception\n",
    "```Python\n",
    "Epoch 50/50\n",
    "4300/4300 [==============================] - 0s - loss: 0.0276 - acc: 1.0000 - val_loss: 0.8378 - val_acc: 0.7519\n",
    "```\n",
    "\n",
    "ResNet\n",
    "```Python\n",
    "Epoch 50/50\n",
    "4300/4300 [==============================] - 0s - loss: 0.0332 - acc: 1.0000 - val_loss: 0.6146 - val_acc: 0.8108\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Summation\n",
    "\n",
    "We've trained AlexNet, VGG, GoogLeNet, and ResNet as feature extractors!\n",
    "\n",
    "To end this lab, let's summarize when we should consider:\n",
    "\n",
    "1. Feature extraction (train only the top-level of the network, the rest of the network remains fixed)\n",
    "2. Finetuning (train the entire network end-to-end, start with pre-trained weights)\n",
    "3. Training from scratch (train the entire network end-to-end, start from random weights)\n",
    "\n",
    "__Consider feature extraction when ...__\n",
    "\n",
    "... the new dataset is small and similar to the original dataset. The higher-level features learned from the original dataset should transfer well to the new dataset.\n",
    "\n",
    "__Consider finetuning when ...__\n",
    "\n",
    "... the new dataset is large and similar to the original dataset. Altering the original weights should be safe because the network is unlikely to overfit the new, large dataset.\n",
    "\n",
    "... the new dataset is small and very different from the original dataset. You could also make the case for training from scratch. If you choose to finetune, it might be a good idea to only use features from the first few layers of the pre-trained network; features from the final layers of the pre-trained network might be too specific to the original dataset.\n",
    "\n",
    "__Consider training from scratch when ...__\n",
    "\n",
    "... the dataset is large and very different from the original dataset. In this case we have enough data to confidently train from scratch. However, even in this case it might be beneficial to initialize the entire network with pretrained weights and finetune it on the new dataset.\n",
    "\n",
    "Finally, keep in mind that for a lot of problems we won't need an architecture as complicated and powerful as VGG, Inception, or ResNet. These architectures were made for the task of classifying thousands of complex classes. A smaller network might be a better fit for a smaller problem, especially if you can comfortably train it on moderate hardware."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
