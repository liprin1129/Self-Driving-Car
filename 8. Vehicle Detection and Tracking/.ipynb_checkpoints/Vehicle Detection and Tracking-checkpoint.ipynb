{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Manual Vehicle Detection](#Manual Vehicle Detection)\n",
    "2. [Feature](#Feature)\n",
    "    1. [Teplate Matching](#Teplate Matching)\n",
    "    2. [Histograms of Colour](#Histograms of Colour)\n",
    "    3. [Histogram Comparison](#Histogram Comparison)\n",
    "    4. [Explore Colour Sapces](#Explore Colour Sapces)\n",
    "    5. [Spatial Binning of Colour](#Spatial Binning of Colour)\n",
    "3. [Gradient Features](#Gradient Features)\n",
    "    1. [HOG Features](#HOG Features)\n",
    "    2. [scikit-image HOG](#scikit-image HOG)\n",
    "4. [Combining Features (Combine and Normalize Features)](#Combining Features)\n",
    "5. [Build A Classifier](#Build A Classifier)\n",
    "    1. [Labeled Data](#Labeled Data)\n",
    "    2. [Paremeter Tuning](#Paremeter Tuning)\n",
    "    3. [Color Classify](#Color Classify)\n",
    "    4. [HOG Classify](#HOG Classify)\n",
    "    5. [How many windows?](#How many windows?)\n",
    "    6. [Sliding Window Implementation](#Sliding Window Implementation)\n",
    "    7. [Multiple Detections & False Positives](#Multiple Detections & False Positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Manual Vehicle Detection<a name='Manual Vehicle Detection'></a>\n",
    "\n",
    "<img src='Figures/1. bbox-example-image.jpg' width=500>\n",
    "\n",
    "Here's our chance to be a human vehicle detector! In this lesson, we will be drawing a lot of bounding boxes on vehicle positions in images. Eventually, we'll have an algorithm that's spewing out bounding box positions and we'll want an easy way to plot them up over our images. So, now is a good time to get familiar with the __cv2.rectangle()__ function ([documentation](http://docs.opencv.org/2.4/modules/core/doc/drawing_functions.html)) that makes it easy to draw boxes of different size, shape and color.\n",
    "\n",
    "In this exercise, our goal is to write a function that takes as its arguments, an image, and a list of bounding box coordinates for each car. Our function should then draw bounding boxes on a copy of the image and return that as its output as the image below.\n",
    "\n",
    "<img src='Figures/1. manual-bbox-quiz-output.jpg' width=500>\n",
    "\n",
    "```Python\n",
    "# Define a function that takes an image, a list of bounding boxes, \n",
    "# and optional color tuple and line thickness as inputs\n",
    "# then draws boxes in that color on the output\n",
    "\n",
    "def draw_boxes(img, bboxes, color=(0, 0, 255), thick=6):\n",
    "    # make a copy of the image\n",
    "    # draw each bounding box on your image copy using cv2.rectangle()\n",
    "    # return the image copy with boxes drawn\n",
    "```\n",
    "\n",
    "We'll draw bounding boxes with __cv2.rectangle()__ like this:\n",
    "```Python\n",
    "cv2.rectangle(image_to_draw_on, (x1, y1), (x2, y2), color, thick)\n",
    "```\n",
    "\n",
    "In this call to __cv2.rectangle()__ our __image\\_to\\_draw\\_on__ should be the copy of our image, then __(x1, y1)__ and __(x2, y2)__ are the $x$ and $y$ coordinates of any two opposing corners of the bounding box we want to draw. __color__ is a 3-tuple, for example, __(0, 0, 255)__ for blue, and __thick__ is an optional integer parameter to define the box thickness.\n",
    "\n",
    "Have a look at the image above with labeled axes, where some bounding boxes and \"guesstimate\" where some of the box corners are were drawn. We should pass our bounding box positions to our __draw_boxes()__ function as a list of tuple pairs, like this:\n",
    "```Python\n",
    "bboxes = [((x1, y1), (x2, y2)), ((,),(,)), ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "image = mpimg.imread('Figures/1. bbox-example-image.jpg')\n",
    "\n",
    "# Define a function that takes an image, a list of bounding boxes, \n",
    "# and optional color tuple and line thickness as inputs\n",
    "# then draws boxes in that color on the output\n",
    "\n",
    "def draw_boxes(img, bboxes, color=(0, 0, 255), thick=6):\n",
    "    # make a copy of the image\n",
    "    draw_img = np.copy(img)\n",
    "    # draw each bounding box on your image copy using cv2.rectangle()\n",
    "    # return the image copy with boxes drawn\n",
    "    for bbox in bboxes:\n",
    "        # Draw a rectangle given bbox coordinates\n",
    "        cv2.rectangle(draw_img, bbox[0], bbox[1], color, thick)\n",
    "    return draw_img # Change this line to return image copy with boxes\n",
    "# Add bounding boxes in this format, these are just example coordinates.\n",
    "bboxes = [((275, 572), (380, 510)), ((488, 563), (549, 518)), ((554, 543), (582, 522)), \n",
    "          ((601, 555), (646, 522)), ((657, 545), (685, 517)), ((849, 678), (1135, 512))]\n",
    "\n",
    "result = draw_boxes(image, bboxes)\n",
    "plt.imshow(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Feature <a name='Feature'></a>\n",
    "\n",
    "__QUIZ: Feature Intuition__\n",
    "\n",
    "Features describe the characteristics of an object, and with images, it really all comes down to intensity and gradients of intensity, and how these features capture the color and shape of an object.\n",
    "\n",
    "<img src='Figures/2. Feature Intuition.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Teplate Matching <a name='Teplate Matching'></a>\n",
    "\n",
    "<img src='Figures/3. bbox-example-image.jpg' width=500>\n",
    "\n",
    "To figure out when template matching works and when it doesn't, let's play around with the OpenCV __cv2.matchTemplate()__ function! In the bounding boxes exercise, we found six cars in the image above. This time, we're going to play the opposite game. Assuming we know these six cars are what we're looking for, we can use them as templates and search the image for matches.\n",
    "\n",
    "__Let's suppose we want to find the templates shown below in the image shown above:__\n",
    "\n",
    "<img src='Figures/3. cutout1.jpg' width=150>\n",
    "<img src='Figures/3. cutout2.jpg' width=150>\n",
    "<img src='Figures/3. cutout3.jpg' width=150>\n",
    "<img src='Figures/3. cutout4.jpg' width=150>\n",
    "<img src='Figures/3. cutout5.jpg' width=150>\n",
    "<img src='Figures/3. cutout6.jpg' width=150>\n",
    "\n",
    "Our goal in this exercise is to write a function that takes in an image and a list of templates, and returns a list of the best fit location (bounding box) for each of the templates within the image. OpenCV provides us with the handy function __cv2.matchTemplate()__ ([documentation](http://docs.opencv.org/2.4/modules/imgproc/doc/object_detection.html)) to search the image, and __cv2.minMaxLoc()__ ([documentation](http://docs.opencv.org/2.4/modules/core/doc/operations_on_arrays.html?highlight=minmaxloc#cv2.minMaxLoc)) to extract the location of the best match.\n",
    "\n",
    "We can choose between \"squared difference\" or \"correlation\" methods in using __cv2.matchTemplate()__, but keep in mind with squared differences we need to locate the global minimum difference to find a match, while for correlation, we're looking for a global maximum.\n",
    "\n",
    "Follow along with [this tutorial](http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_template_matching/py_template_matching.html) provided by OpenCV to try some different template matching techniques. The function we write should work like this:\n",
    "\n",
    "```Python\n",
    "def find_matches(img, template_list):\n",
    "    # Iterate over the list of templates\n",
    "    # Use cv2.matchTemplate() to search the image for each template\n",
    "    # NOTE: You can use any of the cv2.matchTemplate() search methods\n",
    "    # Use cv2.minMaxLoc() to extract the location of the best match in each case\n",
    "    # Compile a list of bounding box corners as output\n",
    "    # Return the list of bounding boxes\n",
    "```\n",
    "\n",
    "__However__, the point of this exercise is not to discover why template matching works for vehicle detection, but rather, why it doesn't! So, after we have a working implementation of the __find\\_matches()__ function, try it on the second image, __temp-matching-example-2.jpg__, which is currently commented out.\n",
    "\n",
    "In the second image, all of the same six cars are visible (just a few seconds later in the video), but we'll find that none of the templates find the correct match! This is because with template matching we can only find very close matches, and changes in size or orientation of a car make it impossible to match with a template.\n",
    "\n",
    "So, just to be clear, our goal here is to:\n",
    "\n",
    "- Write a function that takes in an image and list of templates and returns a list of bounding boxes.\n",
    "- Find that our function works well to locate the six example templates taken from the first image.\n",
    "- Try our code on the second image, and find that template matching breaks easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "image = mpimg.imread('Figures/3. bbox-example-image.jpg')\n",
    "#image = mpimg.imread('temp-matching-example-2.jpg')\n",
    "templist = ['Figures/3. cutout1.jpg', 'Figures/3. cutout2.jpg', 'Figures/3. cutout3.jpg',\n",
    "            'Figures/3. cutout4.jpg', 'Figures/3. cutout5.jpg', 'Figures/3. cutout6.jpg']\n",
    "\n",
    "# Here is your draw_boxes function from the previous exercise\n",
    "def draw_boxes(img, bboxes, color=(0, 0, 255), thick=6):\n",
    "    # Make a copy of the image\n",
    "    imcopy = np.copy(img)\n",
    "    # Iterate through the bounding boxes\n",
    "    for bbox in bboxes:\n",
    "        # Draw a rectangle given bbox coordinates\n",
    "        cv2.rectangle(imcopy, bbox[0], bbox[1], color, thick)\n",
    "    # Return the image copy with boxes drawn\n",
    "    return imcopy\n",
    "    \n",
    "    \n",
    "# Define a function that takes an image and a list of templates as inputs\n",
    "# then searches the image and returns the a list of bounding boxes \n",
    "# for matched templates\n",
    "def find_matches(img, template_list):\n",
    "    # Make a copy of the image to draw on\n",
    "    # Define an empty list to take bbox coords\n",
    "    bbox_list = []\n",
    "    # Iterate through template list\n",
    "    # Read in templates one by one\n",
    "    # Use cv2.matchTemplate() to search the image\n",
    "    #     using whichever of the OpenCV search methods you prefer\n",
    "    # Use cv2.minMaxLoc() to extract the location of the best match\n",
    "    # Determine bounding box corners for the match\n",
    "    # Return the list of bounding boxes\n",
    "    return bbox_list\n",
    "\n",
    "bboxes = find_matches(image, templist)\n",
    "result = draw_boxes(image, bboxes)\n",
    "plt.imshow(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__\n",
    "```Python\n",
    "# Define a function to search for template matches\n",
    "# and return a list of bounding boxes\n",
    "def find_matches(img, template_list):\n",
    "    # Define an empty list to take bbox coords\n",
    "    bbox_list = []\n",
    "    # Define matching method\n",
    "    # Other options include: cv2.TM_CCORR_NORMED', 'cv2.TM_CCOEFF', 'cv2.TM_CCORR',\n",
    "    #         'cv2.TM_SQDIFF', 'cv2.TM_SQDIFF_NORMED'\n",
    "    method = cv2.TM_CCOEFF_NORMED\n",
    "    # Iterate through template list\n",
    "    for temp in template_list:\n",
    "        # Read in templates one by one\n",
    "        tmp = mpimg.imread(temp)\n",
    "        # Use cv2.matchTemplate() to search the image\n",
    "        result = cv2.matchTemplate(img, tmp, method)\n",
    "        # Use cv2.minMaxLoc() to extract the location of the best match\n",
    "        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "        # Determine a bounding box for the match\n",
    "        w, h = (tmp.shape[1], tmp.shape[0])\n",
    "        if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:\n",
    "            top_left = min_loc\n",
    "        else:\n",
    "            top_left = max_loc\n",
    "        bottom_right = (top_left[0] + w, top_left[1] + h)\n",
    "        # Append bbox position to list\n",
    "        bbox_list.append((top_left, bottom_right))\n",
    "        # Return the list of bounding boxes\n",
    "        \n",
    "    return bbox_list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "So the exercise in finding cars has so far been mostly for the purpose of getting you familiarized with searching for objects in an image and drawing bounding boxes where we find them. But before we move on, one more quiz for intuition on the method of template matching!\n",
    "\n",
    "__ Quiz: Template Matching__\n",
    "\n",
    "<img src='Figures/4. 10-q-quiz-slide-v2.png' width=500>\n",
    "\n",
    "Assume our template is the car image on the left marked \"template\". If the other cars (A-D) were located somewhere in an image, which ones would we find to be a match using template matching?\n",
    "\n",
    "1. A\n",
    "2. B\n",
    "3. C\n",
    "4. D\n",
    "\n",
    "__Answer: B__\n",
    "\n",
    "Only the image where the car is very close to the same orientation, size, and color will be a match to this template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Histograms of Colour <a name='Histograms of Colour'></a>\n",
    "\n",
    "We've looked at using raw pixel intensities as features and now we'll look at histograms of pixel intensity (color histograms) as features.\n",
    "\n",
    "We'll use the template shown below from the last exercise as an example. \n",
    "\n",
    "We can construct histograms of the R, G, and B channels like this:\n",
    "```Python\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "\n",
    "# Read in the image\n",
    "image = mpimg.imread('cutout1.jpg')\n",
    "\n",
    "# Take histograms in R, G, and B\n",
    "rhist = np.histogram(image[:,:,0], bins=32, range=(0, 256))\n",
    "ghist = np.histogram(image[:,:,1], bins=32, range=(0, 256))\n",
    "bhist = np.histogram(image[:,:,2], bins=32, range=(0, 256))\n",
    "```\n",
    "\n",
    "With __np.histogram()__, we don't actually have to specify the number of bins or the range, but here we've arbitrarily chosen __32 bins__ and specified __range=(0, 256)__ in order to get orderly bin sizes. __np.histogram()__ returns a tuple of two arrays. In this case, for example, __rhist[0]__ contains the counts in each of the bins and __rhist[1]__ contains the bin edges (so it is one element longer than __rhist[0]__).\n",
    "\n",
    "To look at a plot of these results, we can compute the bin centers from the bin edges. Each of the histograms in this case have the same bins, so we'll just use the __rhist__ bin edges:\n",
    "```Python\n",
    "# Generating bin centers\n",
    "bin_edges = rhist[1]\n",
    "bin_centers = (bin_edges[1:]  + bin_edges[0:len(bin_edges)-1])/2\n",
    "```\n",
    "\n",
    "And then plotting up the results in a bar chart:\n",
    "```Python\n",
    "# Plot a figure with all three bar charts\n",
    "fig = plt.figure(figsize=(12,3))\n",
    "plt.subplot(131)\n",
    "plt.bar(bin_centers, rhist[0])\n",
    "plt.xlim(0, 256)\n",
    "plt.title('R Histogram')\n",
    "plt.subplot(132)\n",
    "plt.bar(bin_centers, ghist[0])\n",
    "plt.xlim(0, 256)\n",
    "plt.title('G Histogram')\n",
    "plt.subplot(133)\n",
    "plt.bar(bin_centers, bhist[0])\n",
    "plt.xlim(0, 256)\n",
    "plt.title('B Histogram')\n",
    "```\n",
    "\n",
    "Which gives us this result:\n",
    "<img src='Figures/4. rgb-histogram-plot.jpg' width=500>\n",
    "\n",
    "These, collectively, are now our feature vector for this particular cutout image. We can concatenate them in the following way:\n",
    "```Python\n",
    "hist_features = np.concatenate((rhist[0], ghist[0], bhist[0]))\n",
    "```\n",
    "\n",
    "__Quiz:__\n",
    "Having a function that does all these steps might be useful for the project so for this next exercise, our goal is to write a function that takes an image and computes the RGB color histogram of features given a particular number of bins and pixels intensity range, and returns the concatenated RGB feature vector, like this:\n",
    "```Python\n",
    "# Define a function to compute color histogram features  \n",
    "def color_hist(img, nbins=32, bins_range=(0, 256)):\n",
    "    # Compute the histogram of the RGB channels separately\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    # Return the feature vector\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "image = mpimg.imread('Figures/3. cutout1.jpg')\n",
    "\n",
    "# Define a function to compute color histogram features  \n",
    "def color_hist(img, nbins=32, bins_range=(0, 256)):\n",
    "    # Compute the histogram of the RGB channels separately\n",
    "    rhist = None\n",
    "    ghist = None\n",
    "    bhist = None\n",
    "    # Generating bin centers\n",
    "    bin_centers = None\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = None\n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return rhist, ghist, bhist, bin_centers, hist_features\n",
    "    \n",
    "rh, gh, bh, bincen, feature_vec = color_hist(image, nbins=32, bins_range=(0, 256))\n",
    "\n",
    "# Plot a figure with all three bar charts\n",
    "if rh is not None:\n",
    "    fig = plt.figure(figsize=(12,3))\n",
    "    plt.subplot(131)\n",
    "    plt.bar(bincen, rh[0])\n",
    "    plt.xlim(0, 256)\n",
    "    plt.title('R Histogram')\n",
    "    plt.subplot(132)\n",
    "    plt.bar(bincen, gh[0])\n",
    "    plt.xlim(0, 256)\n",
    "    plt.title('G Histogram')\n",
    "    plt.subplot(133)\n",
    "    plt.bar(bincen, bh[0])\n",
    "    plt.xlim(0, 256)\n",
    "    plt.title('B Histogram')\n",
    "    fig.tight_layout()\n",
    "else:\n",
    "    print('Function is returning None for at least one variable...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__\n",
    "```Python\n",
    "# Define a function to compute color histogram features  \n",
    "def color_hist(img, nbins=32, bins_range=(0, 256)):\n",
    "    # Compute the histogram of the RGB channels separately\n",
    "    rhist = np.histogram(img[:,:,0], bins=nbins, range=bins_range)\n",
    "    ghist = np.histogram(img[:,:,1], bins=nbins, range=bins_range)\n",
    "    bhist = np.histogram(img[:,:,2], bins=nbins, range=bins_range)\n",
    "    # Generating bin centers\n",
    "    bin_edges = rhist[1]\n",
    "    bin_centers = (bin_edges[1:]  + bin_edges[0:len(bin_edges)-1])/2\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate((rhist[0], ghist[0], bhist[0]))\n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return rhist, ghist, bhist, bin_centers, hist_features\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Histogram Comparison <a name='Histogram Comparison'></a>\n",
    "\n",
    "Let's look at the color histogram features for two totally different images. The first image is of a red car and the second a blue car. The red car's color histograms are displayed on the first row and the blue car's are displayed on the second row below. Here we are just looking at 8 bins per RGB channel.\n",
    "\n",
    "If we had to, we could differentiate the two images based on the differences in histograms alone. As expected the image of the red car has a greater intensity of total bin values in the __R Histogram 1__ (Red Channel) compared to the blue car's __R Histogram 2__. In contrast the blue car has a greater intensity of total bin values in __B Histogram 2__ (Blue Channel) than the red car's __B Histogram 1__ features.\n",
    "\n",
    "<img src='Figures/4. hist-compare.jpg' width=500>\n",
    "\n",
    "Differentiating images by the intensity and range of color they contain can be helpful for looking at car vs non-car images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Explore Colour Sapces <a name='Explore Colour Sapces'></a>\n",
    "\n",
    "We can study the distribution of color values in an image by plotting each pixel in some color space. Here's a code snippet that we can use to generate 3D plots:\n",
    "```Python\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot3d(pixels, colors_rgb,\n",
    "        axis_labels=list(\"RGB\"), axis_limits=[(0, 255), (0, 255), (0, 255)]):\n",
    "    \"\"\"Plot pixels in 3D.\"\"\"\n",
    "\n",
    "    # Create figure and 3D axes\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = Axes3D(fig)\n",
    "\n",
    "    # Set axis limits\n",
    "    ax.set_xlim(*axis_limits[0])\n",
    "    ax.set_ylim(*axis_limits[1])\n",
    "    ax.set_zlim(*axis_limits[2])\n",
    "\n",
    "    # Set axis labels and sizes\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14, pad=8)\n",
    "    ax.set_xlabel(axis_labels[0], fontsize=16, labelpad=16)\n",
    "    ax.set_ylabel(axis_labels[1], fontsize=16, labelpad=16)\n",
    "    ax.set_zlabel(axis_labels[2], fontsize=16, labelpad=16)\n",
    "\n",
    "    # Plot pixel values with colors given in colors_rgb\n",
    "    ax.scatter(\n",
    "        pixels[:, :, 0].ravel(),\n",
    "        pixels[:, :, 1].ravel(),\n",
    "        pixels[:, :, 2].ravel(),\n",
    "        c=colors_rgb.reshape((-1, 3)), edgecolors='none')\n",
    "\n",
    "    return ax  # return Axes3D object for further manipulation\n",
    "\n",
    "\n",
    "# Read a color image\n",
    "img = cv2.imread(\"000275.png\")\n",
    "\n",
    "# Select a small fraction of pixels to plot by subsampling it\n",
    "scale = max(img.shape[0], img.shape[1], 64) / 64  # at most 64 rows and columns\n",
    "img_small = cv2.resize(img, (np.int(img.shape[1] / scale), np.int(img.shape[0] / scale)), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "# Convert subsampled image to desired color space(s)\n",
    "img_small_RGB = cv2.cvtColor(img_small, cv2.COLOR_BGR2RGB)  # OpenCV uses BGR, matplotlib likes RGB\n",
    "img_small_HSV = cv2.cvtColor(img_small, cv2.COLOR_BGR2HSV)\n",
    "img_small_rgb = img_small_RGB / 255.  # scaled to [0, 1], only for plotting\n",
    "\n",
    "# Plot and show\n",
    "plot3d(img_small_RGB, img_small_rgb)\n",
    "plt.show()\n",
    "\n",
    "plot3d(img_small_HSV, img_small_rgb, axis_labels=list(\"HSV\"))\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Use this to first explore some video frames, and see if we can locate clusters of colors that correspond to the sky, trees, specific cars, etc. Here are some sample images to use (these are taken from the [KITTI Vision Benchmark Suite](http://www.cvlibs.net/datasets/kitti/)):\n",
    "\n",
    "<img src='Figures/5. 000275.png' width=500>\n",
    "<img src='Figures/5. 001240.png' width=500>\n",
    "<img src='Figures/5. 000528.png' width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot3d(pixels, colors_rgb,\n",
    "        axis_labels=list(\"RGB\"), axis_limits=[(0, 255), (0, 255), (0, 255)]):\n",
    "    \"\"\"Plot pixels in 3D.\"\"\"\n",
    "\n",
    "    # Create figure and 3D axes\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = Axes3D(fig)\n",
    "\n",
    "    # Set axis limits\n",
    "    ax.set_xlim(*axis_limits[0])\n",
    "    ax.set_ylim(*axis_limits[1])\n",
    "    ax.set_zlim(*axis_limits[2])\n",
    "\n",
    "    # Set axis labels and sizes\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14, pad=8)\n",
    "    ax.set_xlabel(axis_labels[0], fontsize=16, labelpad=16)\n",
    "    ax.set_ylabel(axis_labels[1], fontsize=16, labelpad=16)\n",
    "    ax.set_zlabel(axis_labels[2], fontsize=16, labelpad=16)\n",
    "\n",
    "    # Plot pixel values with colors given in colors_rgb\n",
    "    ax.scatter(\n",
    "        pixels[:, :, 0].ravel(),\n",
    "        pixels[:, :, 1].ravel(),\n",
    "        pixels[:, :, 2].ravel(),\n",
    "        c=colors_rgb.reshape((-1, 3)), edgecolors='none')\n",
    "\n",
    "    return ax  # return Axes3D object for further manipulation\n",
    "\n",
    "\n",
    "# Read a color image\n",
    "img = cv2.imread(\"Figures/5. 000275.png\")\n",
    "\n",
    "# Select a small fraction of pixels to plot by subsampling it\n",
    "scale = max(img.shape[0], img.shape[1], 64) / 64  # at most 64 rows and columns\n",
    "img_small = cv2.resize(img, (np.int(img.shape[1] / scale), np.int(img.shape[0] / scale)), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "# Convert subsampled image to desired color space(s)\n",
    "img_small_RGB = cv2.cvtColor(img_small, cv2.COLOR_BGR2RGB)  # OpenCV uses BGR, matplotlib likes RGB\n",
    "img_small_HSV = cv2.cvtColor(img_small, cv2.COLOR_BGR2HSV)\n",
    "img_small_rgb = img_small_RGB / 255.  # scaled to [0, 1], only for plotting\n",
    "\n",
    "# Plot and show\n",
    "plot3d(img_small_RGB, img_small_rgb)\n",
    "plt.show()\n",
    "\n",
    "plot3d(img_small_HSV, img_small_rgb, axis_labels=list(\"HSV\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to distinguish between the class of pixels we are interested in (vehicles, in this case) from the background. So it may be more beneficial to plot pixels from vehicle and non-vehicle images separately. See if we can identify any trends using these samples:\n",
    "\n",
    "<img src='Figures/5. 25.png' width=100>\n",
    "<img src='Figures/5. 31.png' width=100>\n",
    "<img src='Figures/5. 53.png' width=100>\n",
    "<img src='Figures/5. 8.png' width=100>\n",
    "<img src='Figures/5. 2.png' width=100>\n",
    "<img src='Figures/5. 3.png' width=100>\n",
    "\n",
    "Try experimenting with different color spaces such as LUV or HLS to see if we can find a way to consistently separate vehicle images from non-vehicles. It doesn't have to be perfect, but it will help when combined with other kinds of features fed into a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Spatial Binning of Colour <a name='Spatial Binning of Colour'></a>\n",
    "\n",
    "<img src='Figures/6. spatial-binning.jpg' width=500>\n",
    "\n",
    "We saw earlier in the lesson that template matching is not a particularly robust method for finding vehicles unless we know exactly what our target object looks like. However, raw pixel values are still quite useful to include in our feature vector in searching for cars.\n",
    "\n",
    "While it could be cumbersome to include three color channels of a full resolution image, we can perform spatial binning on an image and still retain enough information to help in finding vehicles.\n",
    "\n",
    "As we can see in the example above, even going all the way down to 32 x 32 pixel resolution, the car itself is still clearly identifiable by eye, and this means that the relevant features are still preserved at this resolution.\n",
    "\n",
    "A convenient function for scaling down the resolution of an image is OpenCV's __cv2.resize()__. We can use it to scale a colour image or a single colour channel like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "image = mpimg.imread('Figures/6. test_img.jpg')\n",
    "small_img = cv2.resize(image, (32, 32))\n",
    "print(small_img.shape)\n",
    "(32, 32, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we then wanted to convert this to a one dimensional [feature vector](https://en.wikipedia.org/wiki/Feature_vector), we could simply say something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_vec = small_img.ravel()\n",
    "print(feature_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, but 3072 elements is still quite a few features! Could we get away with even lower resolution? \n",
    "\n",
    "Now that we've played with colour spaces a bit, it's probably a good time to write a function that allows us to convert any test image into a feature vector that we can feed our classifier. So, our goal in this exercise is to write a function that takes in an image, a color space conversion, and the resolution we would like to convert it to, and returns a feature vector. Something like this:\n",
    "```Python\n",
    "# Define a function that takes an image, a color space, \n",
    "# and a new image size\n",
    "# and returns a feature vector\n",
    "def bin_spatial(img, color_space='RGB', size=(32, 32)):\n",
    "    # Convert image to new color space (if specified)\n",
    "    # Use cv2.resize().ravel() to create the feature vector\n",
    "    # Return the feature vector\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "# Read in an image\n",
    "# You can also read cutout2, 3, 4 etc. to see other examples\n",
    "image = mpimg.imread('Figures/3. cutout1.jpg')\n",
    "\n",
    "# Define a function to compute color histogram features  \n",
    "# Pass the color_space flag as 3-letter all caps string\n",
    "# like 'HSV' or 'LUV' etc.\n",
    "# KEEP IN MIND IF YOU DECIDE TO USE THIS FUNCTION LATER\n",
    "# IN YOUR PROJECT THAT IF YOU READ THE IMAGE WITH \n",
    "# cv2.imread() INSTEAD YOU START WITH BGR COLOR!\n",
    "def bin_spatial(img, color_space='RGB', size=(32, 32)):\n",
    "    # Convert image to new color space (if specified)\n",
    "    # Use cv2.resize().ravel() to create the feature vector\n",
    "    features = img.ravel() # Remove this line!\n",
    "    # Return the feature vector\n",
    "    return features\n",
    "    \n",
    "feature_vec = bin_spatial(image, color_space='RGB', size=(32, 32))\n",
    "\n",
    "# Plot features\n",
    "plt.plot(feature_vec)\n",
    "plt.title('Spatially Binned Features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__\n",
    "```Python\n",
    "# Define a function to compute color histogram features  \n",
    "# Pass the color_space flag as 3-letter all caps string\n",
    "# like 'HSV' or 'LUV' etc.\n",
    "def bin_spatial(img, color_space='RGB', size=(32, 32)):\n",
    "    # Convert image to new color space (if specified)\n",
    "    if color_space != 'RGB':\n",
    "        if color_space == 'HSV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "        elif color_space == 'LUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2LUV)\n",
    "        elif color_space == 'HLS':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "        elif color_space == 'YUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "        elif color_space == 'YCrCb':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n",
    "    else: feature_image = np.copy(img)             \n",
    "    # Use cv2.resize().ravel() to create the feature vector\n",
    "    features = cv2.resize(feature_image, size).ravel() \n",
    "    # Return the feature vector\n",
    "    return features\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Features <a name='Gradient Features'></a>\n",
    "\n",
    "### 3.1. HOG Features <a name='HOG Features'></a>\n",
    "\n",
    "Here is a [presentation](https://www.youtube.com/watch?v=7S5qXET179I) on using Histogram of Oriented Gradient (HOG) features for pedestrian detection by Navneet Dalal, the original developer of HOG for object detection. We can find his original paper on the subject [here](http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf).\n",
    "\n",
    "### 3.2. Data Exploration\n",
    "\n",
    "For the exercises throughout the rest of this lesson, we'll use a relatively small labeled dataset to try out feature extraction and training a classifier. Before we get on to extracting HOG features and training a classifier, let's explore the dataset a bit. This dataset is a subset of the data we'll be starting with for the project.\n",
    "\n",
    "There's no need to download anything at this point, but if want to, we can download this subset of images for [vehicles](https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/vehicles_smallset.zip) and [non-vehicles](https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/non-vehicles_smallset.zip).\n",
    "\n",
    "These datasets are comprised of images taken from the [GTI vehicle image database](http://www.gti.ssr.upm.es/data/Vehicle_database.html), the [KITTI vision benchmark suite](http://www.cvlibs.net/datasets/kitti/), and examples extracted from the project video itself. In this exercise, we can explore the data to see what we're working with.\n",
    "\n",
    "We are also welcome and encouraged to explore the recently released [Udacity labeled dataset](https://github.com/udacity/self-driving-car/tree/master/annotations). Each of the Udacity datasets comes with a __labels.csv__ file that gives bounding box corners for each object labeled.\n",
    "\n",
    "<img src='Figures/7. car-not-car-examples.jpg' width=500>\n",
    "\n",
    "__Quiz:__\n",
    "Here is the code to extract the car/not-car image filenames into two lists. Write a function that takes in these two lists and returns a dictionary with the keys \"n_cars\", \"n_notcars\", \"image_shape\", and \"data_type\", like this:\n",
    "```Python\n",
    "# Define a function to return some characteristics of the dataset \n",
    "def data_look(car_list, notcar_list):\n",
    "    data_dict = {}\n",
    "    # Define a key in data_dict \"n_cars\" and store the number of car images\n",
    "    # Define a key \"n_notcars\" and store the number of notcar images\n",
    "    # Read in a test image, either car or notcar\n",
    "    # Define a key \"image_shape\" and store the test image shape 3-tuple\n",
    "    # Define a key \"data_type\" and store the data type of the test image.\n",
    "    # Return data_dict\n",
    "    return data_dict\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "#from skimage.feature import hog\n",
    "#from skimage import color, exposure\n",
    "# images are divided up into vehicles and non-vehicles\n",
    "\n",
    "cars = []\n",
    "notcars = []\n",
    "\n",
    "car_images = glob.glob('Figures/vehicles_smallset/cars1/*.jpeg')\n",
    "for image in car_images:\n",
    "    if 'image' in image or 'extra' in image:\n",
    "        notcars.append(image)\n",
    "    else:\n",
    "        cars.append(image)\n",
    "        \n",
    "non_car_images = glob.glob('Figures/non-vehicles_smallset/notcars1/*.jpeg')\n",
    "for image in non_car_images:\n",
    "    if 'image' in image or 'extra' in image:\n",
    "        notcars.append(image)\n",
    "    else:\n",
    "        cars.append(image)\n",
    "        \n",
    "# Define a function to return some characteristics of the dataset \n",
    "def data_look(car_list, notcar_list):\n",
    "    data_dict = {}\n",
    "    # Define a key in data_dict \"n_cars\" and store the number of car images\n",
    "    data_dict[\"n_cars\"] = 0\n",
    "    # Define a key \"n_notcars\" and store the number of notcar images\n",
    "    data_dict[\"n_notcars\"] = 0\n",
    "    # Read in a test image, either car or notcar\n",
    "    # Define a key \"image_shape\" and store the test image shape 3-tuple\n",
    "    data_dict[\"image_shape\"] = (0, 0, 0)\n",
    "    # Define a key \"data_type\" and store the data type of the test image.\n",
    "    data_dict[\"data_type\"] = None\n",
    "    # Return data_dict\n",
    "    return data_dict\n",
    "    \n",
    "data_info = data_look(cars, notcars)\n",
    "\n",
    "print('Your function returned a count of', \n",
    "      data_info[\"n_cars\"], ' cars and', \n",
    "      data_info[\"n_notcars\"], ' non-cars')\n",
    "print('of size: ',data_info[\"image_shape\"], ' and data type:', \n",
    "      data_info[\"data_type\"])\n",
    "# Just for fun choose random car / not-car indices and plot example images   \n",
    "car_ind = np.random.randint(0, len(cars))\n",
    "notcar_ind = np.random.randint(0, len(notcars))\n",
    "    \n",
    "# Read in car / not-car images\n",
    "car_image = mpimg.imread(cars[car_ind])\n",
    "notcar_image = mpimg.imread(notcars[notcar_ind])\n",
    "\n",
    "\n",
    "# Plot the examples\n",
    "fig = plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.imshow(car_image)\n",
    "plt.title('Example Car Image')\n",
    "plt.subplot(122)\n",
    "plt.imshow(notcar_image)\n",
    "plt.title('Example Not-car Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__\n",
    "```Python\n",
    "# Define a function to return some characteristics of the dataset \n",
    "def data_look(car_list, notcar_list):\n",
    "    data_dict = {}\n",
    "    # Define a key in data_dict \"n_cars\" and store the number of car images\n",
    "    data_dict[\"n_cars\"] = len(car_list)\n",
    "    # Define a key \"n_notcars\" and store the number of notcar images\n",
    "    data_dict[\"n_notcars\"] = len(notcar_list)\n",
    "    # Read in a test image, either car or notcar\n",
    "    example_img = mpimg.imread(car_list[0])\n",
    "    # Define a key \"image_shape\" and store the test image shape 3-tuple\n",
    "    data_dict[\"image_shape\"] = example_img.shape\n",
    "    # Define a key \"data_type\" and store the data type of the test image.\n",
    "    data_dict[\"data_type\"] = example_img.dtype\n",
    "    # Return data_dict\n",
    "    return data_dict\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. scikit-image HOG <a name='scikit-image HOG'></a>\n",
    "\n",
    "The [scikit-image](http://scikit-image.org/) package has a built in function to extract Histogram of Oriented Gradient features. The documentation for this function can be found [here](http://scikit-image.org/docs/dev/api/skimage.feature.html?highlight=feature%20hog#skimage.feature.hog) and a brief explanation of the algorithm and tutorial can be found [here](http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html).\n",
    "\n",
    "The scikit-image __hog()__ function takes in a single color channel or grayscaled image as input, as well as various parameters. These parameters include __orientations__, __pixels_per_cell__ and __cells_per_block__.\n",
    "\n",
    "The number of __orientations__ is specified as an integer, and represents the number of orientation bins that the gradient information will be split up into in the histogram. Typical values are between 6 and 12 bins.\n",
    "\n",
    "The __pixels_per_cell__ parameter specifies the cell size over which each gradient histogram is computed. This paramater is passed as a 2-tuple so we could have different cell sizes in $x$ and $y$, but cells are commonly chosen to be square.\n",
    "\n",
    "The __cells_per_block__ parameter is also passed as a 2-tuple, and specifies the local area over which the histogram counts in a given cell will be normalized. Block normalization is not necessarily required, but generally leads to a more robust feature set.\n",
    "\n",
    "There is another optional power law or \"gamma\" normalization scheme set by the flag __transform_sqrt__. This type of normalization may help reduce the effects of shadows or other illumination variation, but will cause an error if our image contains negative values (because it's taking the square root of image values).\n",
    "\n",
    "<img src='Figures/8. hog-visualization.jpg' width=500>\n",
    "\n",
    "This is where things get a little confusing though. Let's say we are computing HOG features for an image like the one shown above that is $64×64$ pixels. If we set __pixels_per_cell=(8, 8)__ and __cells_per_block=(2, 2)__ and __orientations=9__. How many elements will we have in our HOG feature vector for the entire image?\n",
    "\n",
    "Let's guess $the number of orientations times the number of cells$, or $9×8×8=576$, but that's not the case if we're using block normalization! In fact, the HOG features for all cells in each block are computed at each block position and the block steps across and down through the image cell by cell.\n",
    "\n",
    "So, the actual number of features in our final feature vector will be _the total number of block positions multiplied by the number of cells per block, times the number of orientations_, or in the case shown above: $7×7×2×2×9=1764$.\n",
    "\n",
    "For the example above, you would call the __hog()__ function on a single color channel __img__ like this:\n",
    "```Python\n",
    "from skimage.feature import hog\n",
    "pix_per_cell = 8\n",
    "cell_per_block = 2\n",
    "orient = 9\n",
    "\n",
    "features, hog_image = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell), cells_per_block=(cell_per_block, cell_per_block), visualise=True, feature_vector=False)\n",
    "```\n",
    "\n",
    "The __visualise=True__ flag tells the function to output a visualization of the HOG feature computation as well, which we're calling __hog_image__ in this case. If we take a look at a single color channel for a random car image, and its corresponding HOG visulization, they look like this:\n",
    "\n",
    "<img src='Figures/8. car-and-hog.jpg' width=500>\n",
    "\n",
    "The HOG visualization is not actually the feature vector, but rather, a representation that shows the dominant gradient direction within each cell with brightness corresponding to the strength of gradients in that cell, much like the \"star\" representation in the last video.\n",
    "\n",
    "If we look at the __features__ output, we'll find it's an array of shape $7×7×2×2×9$. This corresponds to the fact that a grid of $7×7$ blocks were sampled, with $2×2$ cells in each block and $9$ orientations per cell. We can unroll this array into a feature vector using __features.ravel()__, which yields, in this case, a one dimensional array of length __1764__.\n",
    "\n",
    "__Quiz:__\n",
    "Alternatively, we can set the __feature_vector=True__ flag when calling the __hog()__ function to automatically unroll the features. In the project, it could be useful to have a function defined that we could pass an image to with specifications for __orientations__, __pixels_per_cell__, and __cells_per_block__, as well as flags set for whether or not we want the feature vector unrolled and/or a visualization image, so let's write it!\n",
    "\n",
    "```Python\n",
    "# Define a function to return HOG features and visualization\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, vis=False, feature_vec=True):\n",
    "    if vis == True:\n",
    "        # Use skimage.hog() to get both features and a visualization\n",
    "        return features, hog_image\n",
    "    else:      \n",
    "        # Use skimage.hog() to get features only\n",
    "        return features\n",
    "```\n",
    "\n",
    "__Note__: we could also include a keyword to set the __tranform_sqrt__ flag but for this exercise we can just leave this at the default value of __transform_sqrt=False__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "from skimage.feature import hog\n",
    "%matplotlib inline\n",
    "\n",
    "cars = []\n",
    "notcars = []\n",
    "\n",
    "car_images = glob.glob('Figures/vehicles_smallset/cars1/*.jpeg')\n",
    "for image in car_images:\n",
    "    if 'image' in image or 'extra' in image:\n",
    "        notcars.append(image)\n",
    "    else:\n",
    "        cars.append(image)\n",
    "        \n",
    "non_car_images = glob.glob('Figures/non-vehicles_smallset/notcars1/*.jpeg')\n",
    "for image in non_car_images:\n",
    "    if 'image' in image or 'extra' in image:\n",
    "        notcars.append(image)\n",
    "    else:\n",
    "        cars.append(image)\n",
    "        \n",
    "# Define a function to return HOG features and visualization\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, vis=False, feature_vec=True):\n",
    "    if vis == True:\n",
    "        # Use skimage.hog() to get both features and a visualization\n",
    "        features = [] # Remove this line\n",
    "        hog_image = img # Remove this line\n",
    "        return features, hog_image\n",
    "    else:      \n",
    "        # Use skimage.hog() to get features only\n",
    "        features = [] # Remove this line\n",
    "        return features\n",
    "\n",
    "# Generate a random index to look at a car image\n",
    "ind = np.random.randint(0, len(cars))\n",
    "# Read in the image\n",
    "image = mpimg.imread(cars[ind])\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "# Define HOG parameters\n",
    "orient = 9\n",
    "pix_per_cell = 8\n",
    "cell_per_block = 2\n",
    "# Call our function with vis=True to see an image output\n",
    "features, hog_image = get_hog_features(gray, orient, \n",
    "                        pix_per_cell, cell_per_block, \n",
    "                        vis=True, feature_vec=False)\n",
    "\n",
    "\n",
    "# Plot the examples\n",
    "fig = plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title('Example Car Image')\n",
    "plt.subplot(122)\n",
    "plt.imshow(hog_image, cmap='gray')\n",
    "plt.title('HOG Visualization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__\n",
    "```Python\n",
    "# Define a function to return HOG features and visualization\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, vis=False, feature_vec=True):\n",
    "    if vis == True:\n",
    "        features, hog_image = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block), transform_sqrt=False, \n",
    "                                  visualise=True, feature_vector=False)\n",
    "        return features, hog_image\n",
    "    else:      \n",
    "        features = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block), transform_sqrt=False, \n",
    "                       visualise=False, feature_vector=feature_vec)\n",
    "        return features\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Combining Features (Combine and Normalize Features) <a name='Combining Features'></a>\n",
    "\n",
    "<img src='Figures/9. scaled-features-vis.jpg' width=500>\n",
    "\n",
    "Now that we've got several feature extraction methods in our toolkit, we're almost ready to train a classifier, but first, as in any machine learning application, we need to normalize our data. Python's __sklearn__ package provides us with the __StandardScaler()__ method to accomplish this task. To read more about how we can choose different normalizations with the __StandardScaler()__ method, check out the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n",
    "\n",
    "To apply __StandardScaler()__ we need to first have our data in the right format, as a numpy array where each row is a single feature vector. We will often create a list of feature vectors, and then convert them like this:\n",
    "```Python\n",
    "import numpy as np\n",
    "feature_list = [feature_vec1, feature_vec2, ...]\n",
    "# Create an array stack, NOTE: StandardScaler() expects np.float64\n",
    "X = np.vstack(feature_list).astype(np.float64)\n",
    "```\n",
    "\n",
    "You can then fit a scaler to $X$, and scale it like this:\n",
    "```Python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Fit a per-column scaler\n",
    "X_scaler = StandardScaler().fit(X)\n",
    "# Apply the scaler to X\n",
    "scaled_X = X_scaler.transform(X)\n",
    "```\n",
    "\n",
    "Now, __scaled_X__ contains the normalized feature vectors. In this next exercise, we've provided the feature scaling step, but we need to provide the feature vectors. We've also provided versions of the __bin_spatial()__ and __color_hist()__ functions we wrote in previous exercises.\n",
    "\n",
    "Our goal in this exercise is to write a function that takes in a list of image filenames, reads them one by one, then applies a color conversion (if necessary) and uses __bin_spatial()__ and __color_hist()__ to generate feature vectors. The function should then concatenate those two feature vectors and append the result to a list. After cycling through all the images, the function should return the list of feature vectors. Something like this:\n",
    "```Python\n",
    "# Define a function to extract features from a list of images\n",
    "# Have this function call bin_spatial() and color_hist()\n",
    "def extract_features(imgs, cspace='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, hist_range=(0, 256)):\n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    # Iterate through the list of images\n",
    "        # Read in each one by one\n",
    "        # apply color conversion if other than 'RGB'\n",
    "        # Apply bin_spatial() to get spatial color features\n",
    "        # Apply color_hist() to get color histogram features\n",
    "        # Append the new feature vector to the features list\n",
    "    # Return list of feature vectors\n",
    "    return features\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glob\n",
    "\n",
    "# Define a function to compute binned color features  \n",
    "def bin_spatial(img, size=(32, 32)):\n",
    "    # Use cv2.resize().ravel() to create the feature vector\n",
    "    features = cv2.resize(img, size).ravel() \n",
    "    # Return the feature vector\n",
    "    return features\n",
    "\n",
    "# Define a function to compute color histogram features  \n",
    "def color_hist(img, nbins=32, bins_range=(0, 256)):\n",
    "    # Compute the histogram of the color channels separately\n",
    "    channel1_hist = np.histogram(img[:,:,0], bins=nbins, range=bins_range)\n",
    "    channel2_hist = np.histogram(img[:,:,1], bins=nbins, range=bins_range)\n",
    "    channel3_hist = np.histogram(img[:,:,2], bins=nbins, range=bins_range)\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return hist_features\n",
    "\n",
    "###### TODO ###########\n",
    "# Define a function to extract features from a list of images\n",
    "# Have this function call bin_spatial() and color_hist()\n",
    "def extract_features(imgs, cspace='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, hist_range=(0, 256)):\n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    # Iterate through the list of images\n",
    "        # Read in each one by one\n",
    "        # apply color conversion if other than 'RGB'\n",
    "        # Apply bin_spatial() to get spatial color features\n",
    "        # Apply color_hist() to get color histogram features\n",
    "        # Append the new feature vector to the features list\n",
    "    # Return list of feature vectors\n",
    "    return features\n",
    "\n",
    "cars = []\n",
    "notcars = []\n",
    "\n",
    "car_images = glob.glob('Figures/vehicles_smallset/cars1/*.jpeg')\n",
    "for image in car_images:\n",
    "    if 'image' in image or 'extra' in image:\n",
    "        notcars.append(image)\n",
    "    else:\n",
    "        cars.append(image)\n",
    "        \n",
    "non_car_images = glob.glob('Figures/non-vehicles_smallset/notcars1/*.jpeg')\n",
    "for image in non_car_images:\n",
    "    if 'image' in image or 'extra' in image:\n",
    "        notcars.append(image)\n",
    "    else:\n",
    "        cars.append(image)\n",
    "        \n",
    "car_features = extract_features(cars, cspace='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, hist_range=(0, 256))\n",
    "notcar_features = extract_features(notcars, cspace='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, hist_range=(0, 256))\n",
    "\n",
    "if len(car_features) > 0:\n",
    "    # Create an array stack of feature vectors\n",
    "    X = np.vstack((car_features, notcar_features)).astype(np.float64)                        \n",
    "    # Fit a per-column scaler\n",
    "    X_scaler = StandardScaler().fit(X)\n",
    "    # Apply the scaler to X\n",
    "    scaled_X = X_scaler.transform(X)\n",
    "    car_ind = np.random.randint(0, len(cars))\n",
    "    # Plot an example of raw and scaled features\n",
    "    fig = plt.figure(figsize=(12,4))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(mpimg.imread(cars[car_ind]))\n",
    "    plt.title('Original Image')\n",
    "    plt.subplot(132)\n",
    "    plt.plot(X[car_ind])\n",
    "    plt.title('Raw Features')\n",
    "    plt.subplot(133)\n",
    "    plt.plot(scaled_X[car_ind])\n",
    "    plt.title('Normalized Features')\n",
    "    fig.tight_layout()\n",
    "else: \n",
    "    print('Your function only returns empty feature vectors...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__\n",
    "```Python\n",
    "# Define a function to extract features from a list of images\n",
    "# Have this function call bin_spatial() and color_hist()\n",
    "def extract_features(imgs, cspace='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, hist_range=(0, 256)):\n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    # Iterate through the list of images\n",
    "    for file in imgs:\n",
    "        # Read in each one by one\n",
    "        image = mpimg.imread(file)\n",
    "        # apply color conversion if other than 'RGB'\n",
    "        if cspace != 'RGB':\n",
    "            if cspace == 'HSV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "            elif cspace == 'LUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\n",
    "            elif cspace == 'HLS':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "            elif cspace == 'YUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "        else: feature_image = np.copy(image)      \n",
    "        # Apply bin_spatial() to get spatial color features\n",
    "        spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "        # Apply color_hist() also with a color space option now\n",
    "        hist_features = color_hist(feature_image, nbins=hist_bins, bins_range=hist_range)\n",
    "        # Append the new feature vector to the features list\n",
    "        features.append(np.concatenate((spatial_features, hist_features)))\n",
    "    # Return list of feature vectors\n",
    "    return features\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glob\n",
    "\n",
    "# Define a function to compute binned color features  \n",
    "def bin_spatial(img, size=(32, 32)):\n",
    "    # Use cv2.resize().ravel() to create the feature vector\n",
    "    features = cv2.resize(img, size).ravel() \n",
    "    # Return the feature vector\n",
    "    return features\n",
    "\n",
    "# Define a function to compute color histogram features  \n",
    "def color_hist(img, nbins=32, bins_range=(0, 256)):\n",
    "    # Compute the histogram of the color channels separately\n",
    "    channel1_hist = np.histogram(img[:,:,0], bins=nbins, range=bins_range)\n",
    "    channel2_hist = np.histogram(img[:,:,1], bins=nbins, range=bins_range)\n",
    "    channel3_hist = np.histogram(img[:,:,2], bins=nbins, range=bins_range)\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return hist_features\n",
    "\n",
    "###### TODO ###########\n",
    "# Define a function to extract features from a list of images\n",
    "# Have this function call bin_spatial() and color_hist()\n",
    "def extract_features(imgs, cspace='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, hist_range=(0, 256)):\n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    # Iterate through the list of images\n",
    "    for file in imgs:\n",
    "        # Read in each one by one\n",
    "        image = mpimg.imread(file)\n",
    "        # apply color conversion if other than 'RGB'\n",
    "        if cspace != 'RGB':\n",
    "            if cspace == 'HSV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "            elif cspace == 'LUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\n",
    "            elif cspace == 'HLS':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "            elif cspace == 'YUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "        else: feature_image = np.copy(image)      \n",
    "        # Apply bin_spatial() to get spatial color features\n",
    "        spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "        # Apply color_hist() also with a color space option now\n",
    "        hist_features = color_hist(feature_image, nbins=hist_bins, bins_range=hist_range)\n",
    "        # Append the new feature vector to the features list\n",
    "        features.append(np.concatenate((spatial_features, hist_features)))\n",
    "    # Return list of feature vectors\n",
    "    return features\n",
    "\n",
    "cars = []\n",
    "notcars = []\n",
    "\n",
    "car_images = glob.glob('Figures/vehicles_smallset/cars1/*.jpeg')\n",
    "for image in car_images:\n",
    "    if 'image' in image or 'extra' in image:\n",
    "        notcars.append(image)\n",
    "    else:\n",
    "        cars.append(image)\n",
    "        \n",
    "non_car_images = glob.glob('Figures/non-vehicles_smallset/notcars1/*.jpeg')\n",
    "for image in non_car_images:\n",
    "    if 'image' in image or 'extra' in image:\n",
    "        notcars.append(image)\n",
    "    else:\n",
    "        cars.append(image)\n",
    "        \n",
    "car_features = extract_features(cars, cspace='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, hist_range=(0, 256))\n",
    "notcar_features = extract_features(notcars, cspace='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, hist_range=(0, 256))\n",
    "\n",
    "if len(car_features) > 0:\n",
    "    # Create an array stack of feature vectors\n",
    "    X = np.vstack((car_features, notcar_features)).astype(np.float64)                        \n",
    "    # Fit a per-column scaler\n",
    "    X_scaler = StandardScaler().fit(X)\n",
    "    # Apply the scaler to X\n",
    "    scaled_X = X_scaler.transform(X)\n",
    "    car_ind = np.random.randint(0, len(cars))\n",
    "    # Plot an example of raw and scaled features\n",
    "    fig = plt.figure(figsize=(12,4))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(mpimg.imread(cars[car_ind]))\n",
    "    plt.title('Original Image')\n",
    "    plt.subplot(132)\n",
    "    plt.plot(X[car_ind])\n",
    "    plt.title('Raw Features')\n",
    "    plt.subplot(133)\n",
    "    plt.plot(scaled_X[car_ind])\n",
    "    plt.title('Normalized Features')\n",
    "    fig.tight_layout()\n",
    "else: \n",
    "    print('Your function only returns empty feature vectors...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Build A Classifier <a name='Build A Classifier'></a>\n",
    "\n",
    "### 5.1. Labeled Data <a name='Labeled Data'></a>\n",
    "\n",
    "<img src='Figures/10. Data Preparation.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Paremeter Tuning <a name='Paremeter Tuning'></a>\n",
    "\n",
    "__SVM Hyperparameters:__ optimize the Gamma and C parameters.\n",
    "\n",
    "Successfully tuning our algorithm involves searching for a kernel, a gamma value and a C value that minimize prediction error. To tune our SVM vehicle detection model, we can use one of scikit-learn's parameter tuning algorithms.\n",
    "\n",
    "When tuning SVM, remember that we can only tune the C parameter with a linear kernel. For a non-linear kernel, we can tune C and gamma.\n",
    "\n",
    "__Parameter Tuning in Scikit-learn:__\n",
    "\n",
    "Scikit-learn includes two algorithms for carrying out an automatic parameter search:\n",
    "- [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)\n",
    "- [RandomizedSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV)\n",
    "\n",
    "_GridSearchCV_ exhaustively works through multiple parameter combinations, cross-validating as it goes. The beauty is that it can work through many combinations in only a couple extra lines of code.\n",
    "\n",
    "For example, if we input the values C:[0.1, 1, 10] and gamma:[0.1, 1, 10], gridSearchCV will train and cross-validate every possible combination of (C, gamma): (0.1, 0.1), (0.1, 1), (0.1, 10), (1, .1), (1, 1), etc.\n",
    "\n",
    "_RandomizedSearchCV_ works similarly to GridSearchCV except RandomizedSearchCV takes a random sample of parameter combinations. RandomizedSearchCV is faster than GridSearchCV since RandomizedSearchCV uses a subset of the parameter combinations.\n",
    "\n",
    "__Cross-validation with GridSearchCV:__\n",
    "\n",
    "GridSearchCV uses 3-fold cross validation to determine the best performing parameter set. GridSearchCV will take in a training set and divide the training set into three equal partitions. The algorithm will train on two partitions and then validate using the third partition. Then GridSearchCV chooses a different partition for validation and trains with the other two partitions. Finally, GridSearchCV uses the last remaining partition for cross-validation and trains with the other two partitions.\n",
    "\n",
    "By default, GridSearchCV uses accuracy as an error metric by averaging the accuracy for each partition. So for every possible parameter combination, GridSearchCV calculates an accuracy score. Then GridSearchCV will choose the parameter combination that performed the best.\n",
    "\n",
    "__Scikit-learn Cross Validation Example:__\n",
    "\n",
    "Here's an example from the sklearn [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html) for implementing GridSearchCV:\n",
    "\n",
    "```Python\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svr = svm.SVC()\n",
    "clf = grid_search.GridSearchCV(svr, parameters)\n",
    "clf.fit(iris.data, iris.target)\n",
    "```\n",
    "\n",
    "Let's break this down line by line.\n",
    "\n",
    "```Python\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]} \n",
    "```\n",
    "\n",
    "A dictionary of the parameters, and the possible values they may take. In this case, they're playing around with the kernel (possible choices are 'linear' and 'rbf'), and C (possible choices are 1 and 10).\n",
    "\n",
    "Then a 'grid' of all the following combinations of values for (kernel, C) are automatically generated:\n",
    "\n",
    "||||\n",
    "|-|-|\n",
    "|('rbf', 1)|('rbf', 10)|\n",
    "|('linear', 1)|('linear', 10)|\n",
    "\n",
    "Each is used to train an SVM, and the performance is then assessed using cross-validation.\n",
    "\n",
    "```Python\n",
    "svr = svm.SVC() \n",
    "```\n",
    "This looks kind of like creating a classifier, just like we've been doing since the first lesson. But note that the \"clf\" isn't made until the next line--this is just saying what kind of algorithm to use. Another way to think about this is that the \"classifier\" isn't just the algorithm in this case, it's algorithm plus parameter values. Note that there's no monkeying around with the kernel or C; all that is handled in the next line.\n",
    "\n",
    "```Python\n",
    "clf = grid_search.GridSearchCV(svr, parameters) \n",
    "```\n",
    "This is where the first bit of magic happens; the classifier is being created. We pass the algorithm (svr) and the dictionary of parameters to try (parameters) and it generates a grid of parameter combinations to try.\n",
    "\n",
    "```Python\n",
    "clf.fit(iris.data, iris.target) \n",
    "```\n",
    "And the second bit of magic. The fit function now tries all the parameter combinations, and returns a fitted classifier that's automatically tuned to the optimal parameter combination. We can now access the parameter values via **clf.best\\_params\\_**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Color Classify <a name='Color Classify'></a>\n",
    "\n",
    "<img src='Figures/11. car-color-and-hist.jpg' width=500>\n",
    "\n",
    "Now we'll try training a classifier on our dataset. First, we'll see how well it does just using spatially binned color and color histograms.\n",
    "\n",
    "To do this, we'll use the functions we defined in previous exercises, namely, __bin_spatial()__, __color_hist()__, and __extract_features()__. We'll then read in our car and non-car images, extract the color features for each, and scale the feature vectors to zero mean and unit variance.\n",
    "\n",
    "All that remains is to define a labels vector, shuffle and split the data into training and testing sets, and finally, define a classifier and train it!\n",
    "\n",
    "Our labels vector __y__ in this case will just be a binary vector indicating whether each feature vector in our dataset corresponds to a car or non-car (1's for cars, 0's for non-cars). Given lists of car and non-car features (the output of __extract_features()__) we can define a labels vector like this:\n",
    "```Python\n",
    "import numpy as np\n",
    "# Define a labels vector based on features lists\n",
    "y = np.hstack((np.ones(len(car_features)), \n",
    "              np.zeros(len(notcar_features))))\n",
    "```\n",
    "\n",
    "Next, we'll stack and scale our feature vectors like before:\n",
    "```Python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Create an array stack of feature vectors\n",
    "X = np.vstack((car_features, notcar_features)).astype(np.float64)\n",
    "# Fit a per-column scaler\n",
    "X_scaler = StandardScaler().fit(X)\n",
    "# Apply the scaler to X\n",
    "scaled_X = X_scaler.transform(X)\n",
    "```\n",
    "\n",
    "And now we're ready to shuffle and split the data into training and testing sets. To do this we'll use the Scikit-Learn __train_test_split()__ function, but it's worth noting that recently, this function moved from the __sklearn.cross_validation__ package (in __sklearn__ version <=0.17) to the __sklearn.model_selection__ package (in __sklearn__ version >=0.18).\n",
    "\n",
    "In the quiz editor we're still running __sklearn v0.17__, so we'll import it like this:\n",
    "\n",
    "```Python\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# But, if you are using scikit-learn >= 0.18 then use this:\n",
    "# from sklearn.model_selection import train_test_split\n",
    "```\n",
    "\n",
    "__train_test_split()__ performs both the shuffle and split of the data and we'll call it like this (here choosing to initialize the shuffle with a different random state each time):\n",
    "\n",
    "```Python\n",
    "# Split up data into randomized training and test sets\n",
    "rand_state = np.random.randint(0, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    scaled_X, y, test_size=0.2, random_state=rand_state)\n",
    "```\n",
    "\n",
    "__Warning:__ when dealing with image data that was extracted from video, we may be dealing with sequences of images where our target object (vehicles in this case) appear almost identical in a whole series of images. In such a case, even a randomized train-test split will be subject to overfitting because images in the training set may be nearly identical to images in the test set. For the subset of images used in the next several quizzes, this is not a problem, but to optimize our classifier for the project, we may need to worry about time-series of images!\n",
    "\n",
    "---\n",
    "Now, we're ready to define and train a classifier! Here we'll try a Linear Support Vector Machine. To define and train the classifier it takes just a few lines of code:\n",
    "\n",
    "```Python\n",
    "from sklearn.svm import LinearSVC\n",
    "# Use a linear SVC (support vector classifier)\n",
    "svc = LinearSVC()\n",
    "# Train the SVC\n",
    "svc.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "Then we can check the accuracy of our classifier on the test dataset like this:\n",
    "```Python\n",
    "print('Test Accuracy of SVC = ', svc.score(X_test, y_test))\n",
    "```\n",
    "\n",
    "Or we can make predictions on a subset of the test data and compare directly with ground truth:\n",
    "```Python\n",
    "print('My SVC predicts: ', svc.predict(X_test[0:10].reshape(1, -1)))\n",
    "print('For labels: ', y_test[0:10])\n",
    "```\n",
    "\n",
    "Play with the parameter values __spatial__ and __histbin__ in the exercise below to see how the classifier accuracy and training time vary with the feature vector input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# NOTE: the next import is only valid \n",
    "# for scikit-learn version <= 0.17\n",
    "# if you are using scikit-learn >= 0.18 then use this:\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Define a function to compute binned color features  \n",
    "def bin_spatial(img, size=(32, 32)):\n",
    "    # Use cv2.resize().ravel() to create the feature vector\n",
    "    features = cv2.resize(img, size).ravel() \n",
    "    # Return the feature vector\n",
    "    return features\n",
    "\n",
    "# Define a function to compute color histogram features  \n",
    "def color_hist(img, nbins=32, bins_range=(0, 256)):\n",
    "    # Compute the histogram of the color channels separately\n",
    "    channel1_hist = np.histogram(img[:,:,0], bins=nbins, range=bins_range)\n",
    "    channel2_hist = np.histogram(img[:,:,1], bins=nbins, range=bins_range)\n",
    "    channel3_hist = np.histogram(img[:,:,2], bins=nbins, range=bins_range)\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return hist_features\n",
    "\n",
    "# Define a function to extract features from a list of images\n",
    "# Have this function call bin_spatial() and color_hist()\n",
    "def extract_features(imgs, cspace='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, hist_range=(0, 256)):\n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    # Iterate through the list of images\n",
    "    for file in imgs:\n",
    "        # Read in each one by one\n",
    "        image = mpimg.imread(file)\n",
    "        # apply color conversion if other than 'RGB'\n",
    "        if cspace != 'RGB':\n",
    "            if cspace == 'HSV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "            elif cspace == 'LUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\n",
    "            elif cspace == 'HLS':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "            elif cspace == 'YUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "        else: feature_image = np.copy(image)      \n",
    "        # Apply bin_spatial() to get spatial color features\n",
    "        spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "        # Apply color_hist() also with a color space option now\n",
    "        hist_features = color_hist(feature_image, nbins=hist_bins, bins_range=hist_range)\n",
    "        # Append the new feature vector to the features list\n",
    "        features.append(np.concatenate((spatial_features, hist_features)))\n",
    "    # Return list of feature vectors\n",
    "    return features\n",
    "\n",
    "# Divide up into cars and notcars\n",
    "cars = []\n",
    "notcars = []\n",
    "\n",
    "car_images = glob.glob('Figures/vehicles_smallset/cars1/*.jpeg')\n",
    "for image in car_images:\n",
    "    if 'image' in image or 'extra' in image:\n",
    "        notcars.append(image)\n",
    "    else:\n",
    "        cars.append(image)\n",
    "        \n",
    "non_car_images = glob.glob('Figures/non-vehicles_smallset/notcars1/*.jpeg')\n",
    "for image in non_car_images:\n",
    "    if 'image' in image or 'extra' in image:\n",
    "        notcars.append(image)\n",
    "    else:\n",
    "        cars.append(image)\n",
    "\n",
    "# TODO play with these values to see how your classifier\n",
    "# performs under different binning scenarios\n",
    "spatial = 32\n",
    "histbin = 32\n",
    "\n",
    "car_features = extract_features(cars, cspace='RGB', spatial_size=(spatial, spatial),\n",
    "                        hist_bins=histbin, hist_range=(0, 256))\n",
    "notcar_features = extract_features(notcars, cspace='RGB', spatial_size=(spatial, spatial),\n",
    "                        hist_bins=histbin, hist_range=(0, 256))\n",
    "\n",
    "# Create an array stack of feature vectors\n",
    "X = np.vstack((car_features, notcar_features)).astype(np.float64)                        \n",
    "# Fit a per-column scaler\n",
    "X_scaler = StandardScaler().fit(X)\n",
    "# Apply the scaler to X\n",
    "scaled_X = X_scaler.transform(X)\n",
    "\n",
    "# Define the labels vector\n",
    "y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\n",
    "\n",
    "\n",
    "# Split up data into randomized training and test sets\n",
    "rand_state = np.random.randint(0, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    scaled_X, y, test_size=0.2, random_state=rand_state)\n",
    "\n",
    "print('Using spatial binning of:',spatial,\n",
    "    'and', histbin,'histogram bins')\n",
    "print('Feature vector length:', len(X_train[0]))\n",
    "# Use a linear SVC \n",
    "svc = LinearSVC()\n",
    "# Check the training time for the SVC\n",
    "t=time.time()\n",
    "svc.fit(X_train, y_train)\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "# Check the score of the SVC\n",
    "print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))\n",
    "# Check the prediction time for a single sample\n",
    "t=time.time()\n",
    "n_predict = 10\n",
    "print('My SVC predicts: ', svc.predict(X_test[0:n_predict]))\n",
    "print('For these',n_predict, 'labels: ', y_test[0:n_predict])\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 5), 'Seconds to predict', n_predict,'labels with SVC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. HOG Classify <a name='HOG Classify'></a>\n",
    "\n",
    "classification by color features alone is pretty effective! Now let's try classifying with HOG features and see how well we can do.\n",
    "\n",
    "<img src='Figures/12. car-and-hog.jpg' width=500>\n",
    "\n",
    "---\n",
    "__NOTE:__ if we copy the code from the exercise below onto our local machine, but are running sklearn version >= 0.18 we will need to change from calling:\n",
    "\n",
    "```Python\n",
    "from sklearn.cross_validation import train_test_split\n",
    "```\n",
    "\n",
    "```Python\n",
    "from sklearn.model_selection import train_test_split\n",
    "```\n",
    "\n",
    "---\n",
    "In the exercise below, we're given all the code to extract HOG features and train a linear SVM. There is no right or wrong answer, but our mission, should we choose to accept it, is to play with the parameters __colorspace__, __orient__, __pix_per_cell__, __cell_per_block__, and __hog_channel__ to get a feel for what combination of parameters give the best results.\n",
    "\n",
    "__Note:__ __hog_channel__ can take values of 0, 1, 2, or \"ALL\", meaning that we extract HOG features from the first, second, third, or all color channels respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.feature import hog\n",
    "# NOTE: the next import is only valid for scikit-learn version <= 0.17\n",
    "# for scikit-learn >= 0.18 use:\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Define a function to return HOG features and visualization\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, \n",
    "                        vis=False, feature_vec=True):\n",
    "    # Call with two outputs if vis==True\n",
    "    if vis == True:\n",
    "        features, hog_image = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block), transform_sqrt=True, \n",
    "                                  visualise=vis, feature_vector=feature_vec)\n",
    "        return features, hog_image\n",
    "    # Otherwise call with one output\n",
    "    else:      \n",
    "        features = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block), transform_sqrt=True, \n",
    "                       visualise=vis, feature_vector=feature_vec)\n",
    "        return features\n",
    "\n",
    "# Define a function to extract features from a list of images\n",
    "# Have this function call bin_spatial() and color_hist()\n",
    "def extract_features(imgs, cspace='RGB', orient=9, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0):\n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    # Iterate through the list of images\n",
    "    for file in imgs:\n",
    "        # Read in each one by one\n",
    "        image = mpimg.imread(file)\n",
    "        # apply color conversion if other than 'RGB'\n",
    "        if cspace != 'RGB':\n",
    "            if cspace == 'HSV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "            elif cspace == 'LUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\n",
    "            elif cspace == 'HLS':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "            elif cspace == 'YUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "            elif cspace == 'YCrCb':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n",
    "        else: feature_image = np.copy(image)      \n",
    "\n",
    "        # Call get_hog_features() with vis=False, feature_vec=True\n",
    "        if hog_channel == 'ALL':\n",
    "            hog_features = []\n",
    "            for channel in range(feature_image.shape[2]):\n",
    "                hog_features.append(get_hog_features(feature_image[:,:,channel], \n",
    "                                    orient, pix_per_cell, cell_per_block, \n",
    "                                    vis=False, feature_vec=True))\n",
    "            hog_features = np.ravel(hog_features)        \n",
    "        else:\n",
    "            hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                        pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "        # Append the new feature vector to the features list\n",
    "        features.append(hog_features)\n",
    "    # Return list of feature vectors\n",
    "    return features\n",
    "\n",
    "\n",
    "# Divide up into cars and notcars\n",
    "cars = []\n",
    "notcars = []\n",
    "\n",
    "car_images = glob.glob('Figures/vehicles_smallset/cars1/*.jpeg')\n",
    "for image in car_images:\n",
    "    if 'image' in image or 'extra' in image:\n",
    "        notcars.append(image)\n",
    "    else:\n",
    "        cars.append(image)\n",
    "        \n",
    "non_car_images = glob.glob('Figures/non-vehicles_smallset/notcars1/*.jpeg')\n",
    "for image in non_car_images:\n",
    "    if 'image' in image or 'extra' in image:\n",
    "        notcars.append(image)\n",
    "    else:\n",
    "        cars.append(image)\n",
    "\n",
    "# Reduce the sample size because HOG features are slow to compute\n",
    "# The quiz evaluator times out after 13s of CPU time\n",
    "sample_size = 500\n",
    "cars = cars[0:sample_size]\n",
    "notcars = notcars[0:sample_size]\n",
    "\n",
    "### TODO: Tweak these parameters and see how the results change.\n",
    "colorspace = 'RGB' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "orient = 9\n",
    "pix_per_cell = 8\n",
    "cell_per_block = 2\n",
    "hog_channel = 0 # Can be 0, 1, 2, or \"ALL\"\n",
    "\n",
    "t=time.time()\n",
    "car_features = extract_features(cars, cspace=colorspace, orient=orient, \n",
    "                        pix_per_cell=pix_per_cell, cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel)\n",
    "notcar_features = extract_features(notcars, cspace=colorspace, orient=orient, \n",
    "                        pix_per_cell=pix_per_cell, cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel)\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 2), 'Seconds to extract HOG features...')\n",
    "# Create an array stack of feature vectors\n",
    "X = np.vstack((car_features, notcar_features)).astype(np.float64)                        \n",
    "# Fit a per-column scaler\n",
    "X_scaler = StandardScaler().fit(X)\n",
    "# Apply the scaler to X\n",
    "scaled_X = X_scaler.transform(X)\n",
    "\n",
    "# Define the labels vector\n",
    "y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\n",
    "\n",
    "\n",
    "# Split up data into randomized training and test sets\n",
    "rand_state = np.random.randint(0, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    scaled_X, y, test_size=0.2, random_state=rand_state)\n",
    "\n",
    "print('Using:',orient,'orientations',pix_per_cell,\n",
    "    'pixels per cell and', cell_per_block,'cells per block')\n",
    "print('Feature vector length:', len(X_train[0]))\n",
    "# Use a linear SVC \n",
    "svc = LinearSVC()\n",
    "# Check the training time for the SVC\n",
    "t=time.time()\n",
    "svc.fit(X_train, y_train)\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "# Check the score of the SVC\n",
    "print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))\n",
    "# Check the prediction time for a single sample\n",
    "t=time.time()\n",
    "n_predict = 10\n",
    "print('My SVC predicts: ', svc.predict(X_test[0:n_predict]))\n",
    "print('For these',n_predict, 'labels: ', y_test[0:n_predict])\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 5), 'Seconds to predict', n_predict,'labels with SVC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. How many windows? <a name='How many windows?'></a>\n",
    "\n",
    "<img src='Figures/13. car-identified.jpg' width=500>\n",
    "\n",
    "To implement a sliding window search, we need to decide what size window we want to search, where in the image we want to start and stop our search, and how much we want windows to overlap. So, let's try an example to see how many windows we would be searching given a particular image size, window size, and overlap.\n",
    "\n",
    "Suppose we have an image that is $256 x 256$ pixels and we want to search windows of a size $128 x 128$ pixels each with an overlap of 50% between adjacent windows in both the vertical and horizontal dimensions. Our sliding window search would then look like this:\n",
    "<img src='Figures/13. sliding-window.jpg' width=500>\n",
    "\n",
    "So, we searched 9 windows total in this case. \n",
    "\n",
    "__Quiz:__\n",
    "Suppose we have an image that is $1280 x 960$ pixels and we want to search the entire image using $64 x 64$ pixel windows with 50% overlap between windows in both the vertical and horizontal directions. How many windows would you be searching total?\n",
    "\n",
    "__Answer:__ 1131"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6. Sliding Window Implementation <a name='Sliding Window Implementation'></a>\n",
    "\n",
    "<img src='Figures/14. sliding-window.jpg' width=500>\n",
    "\n",
    "In the last exercise, we saw how the number of windows scales with image size, window size, and overlap. In the project it will be useful to have a function to perform a sliding window search on an image, so let's write one! This will just be the first iteration, where we don't actually need to do anything besides plot a rectangle at each window position.\n",
    "\n",
    "So, our goal here is to write a function that takes in an image, start and stop positions in both $x$ and $y$ (imagine a bounding box for the entire search region), window size ($x$ and $y$ dimensions), and overlap fraction (also for both $x$ and $y$). Our function should return a list of bounding boxes for the search windows, which will then be passed to draw draw_boxes() function.\n",
    "\n",
    "__Quiz:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "image = mpimg.imread('Figures/3. bbox-example-image.jpg')\n",
    "\n",
    "# Here is your draw_boxes function from the previous exercise\n",
    "def draw_boxes(img, bboxes, color=(0, 0, 255), thick=6):\n",
    "    # Make a copy of the image\n",
    "    imcopy = np.copy(img)\n",
    "    # Iterate through the bounding boxes\n",
    "    for bbox in bboxes:\n",
    "        # Draw a rectangle given bbox coordinates\n",
    "        cv2.rectangle(imcopy, bbox[0], bbox[1], color, thick)\n",
    "    # Return the image copy with boxes drawn\n",
    "    return imcopy\n",
    "    \n",
    "    \n",
    "# Define a function that takes an image,\n",
    "# start and stop positions in both x and y, \n",
    "# window size (x and y dimensions),  \n",
    "# and overlap fraction (for both x and y)\n",
    "def slide_window(img, x_start_stop=[None, None], y_start_stop=[None, None], \n",
    "                    xy_window=(64, 64), xy_overlap=(0.5, 0.5)):\n",
    "    # If x and/or y start/stop positions not defined, set to image size\n",
    "    # Compute the span of the region to be searched    \n",
    "    # Compute the number of pixels per step in x/y\n",
    "    # Compute the number of windows in x/y\n",
    "    # Initialize a list to append window positions to\n",
    "    window_list = []\n",
    "    # Loop through finding x and y window positions\n",
    "    #     Note: you could vectorize this step, but in practice\n",
    "    #     you'll be considering windows one by one with your\n",
    "    #     classifier, so looping makes sense\n",
    "        # Calculate each window position\n",
    "        # Append window position to list\n",
    "    # Return the list of windows\n",
    "    return window_list\n",
    "\n",
    "windows = slide_window(image, x_start_stop=[None, None], y_start_stop=[None, None], \n",
    "                    xy_window=(128, 128), xy_overlap=(0.5, 0.5))\n",
    "                       \n",
    "window_img = draw_boxes(image, windows, color=(0, 0, 255), thick=6)                    \n",
    "plt.imshow(window_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__\n",
    "```Python\n",
    "# Define a function that takes an image,\n",
    "# start and stop positions in both x and y, \n",
    "# window size (x and y dimensions),  \n",
    "# and overlap fraction (for both x and y)\n",
    "def slide_window(img, x_start_stop=[None, None], y_start_stop=[None, None], \n",
    "                    xy_window=(64, 64), xy_overlap=(0.5, 0.5)):\n",
    "    # If x and/or y start/stop positions not defined, set to image size\n",
    "    if x_start_stop[0] == None:\n",
    "        x_start_stop[0] = 0\n",
    "    if x_start_stop[1] == None:\n",
    "        x_start_stop[1] = img.shape[1]\n",
    "    if y_start_stop[0] == None:\n",
    "        y_start_stop[0] = 0\n",
    "    if y_start_stop[1] == None:\n",
    "        y_start_stop[1] = img.shape[0]\n",
    "    # Compute the span of the region to be searched    \n",
    "    xspan = x_start_stop[1] - x_start_stop[0]\n",
    "    yspan = y_start_stop[1] - y_start_stop[0]\n",
    "    # Compute the number of pixels per step in x/y\n",
    "    nx_pix_per_step = np.int(xy_window[0]*(1 - xy_overlap[0]))\n",
    "    ny_pix_per_step = np.int(xy_window[1]*(1 - xy_overlap[1]))\n",
    "    # Compute the number of windows in x/y\n",
    "    nx_buffer = np.int(xy_window[0]*(xy_overlap[0]))\n",
    "    ny_buffer = np.int(xy_window[1]*(xy_overlap[1]))\n",
    "    nx_windows = np.int((xspan-nx_buffer)/nx_pix_per_step) \n",
    "    ny_windows = np.int((yspan-ny_buffer)/ny_pix_per_step) \n",
    "    # Initialize a list to append window positions to\n",
    "    window_list = []\n",
    "    # Loop through finding x and y window positions\n",
    "    # Note: you could vectorize this step, but in practice\n",
    "    # you'll be considering windows one by one with your\n",
    "    # classifier, so looping makes sense\n",
    "    for ys in range(ny_windows):\n",
    "        for xs in range(nx_windows):\n",
    "            # Calculate window position\n",
    "            startx = xs*nx_pix_per_step + x_start_stop[0]\n",
    "            endx = startx + xy_window[0]\n",
    "            starty = ys*ny_pix_per_step + y_start_stop[0]\n",
    "            endy = starty + xy_window[1]\n",
    "            # Append window position to list\n",
    "            window_list.append(((startx, starty), (endx, endy)))\n",
    "    # Return the list of windows\n",
    "    return window_list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6. Search and Classify\n",
    "\n",
    "<img src='Figures/15. bbox-example-image.jpg' width=500>\n",
    "\n",
    "Now we're able to run a sliding window search on an image and we've trained a classifier... time to combine both steps and search for cars!\n",
    "\n",
    "We already have all the tools we need to do this from the previous exercises. Just train our classifier, then run our sliding window search, extract features, and predict whether each window contains a car or not. We'll probably find some false positives, but soon we'll deal with removing them.\n",
    "\n",
    "In the __lesson_functions.py__ tab on the quiz editor, we'll find all the functions we've defined so far in the lesson, including __get_hog_features()__, __bin_spatial()__, __color_hist()__, __extract_features()__, __slide_window()__, and __draw_boxes()__. These are now all imported for use in the quiz with this command:\n",
    "```Python\n",
    "from lesson_functions import *\n",
    "```\n",
    "\n",
    "Two new functions are defined: __single_img_features()__ and __search_windows()__. We can use these to search over all the windows defined by our __slide_windows()__, extract features at each window position, and predict with our classifier on each set of features.\n",
    "\n",
    "We have limited the sample size to 500 each of car and not-car training images for the classifier to avoid quiz evaluator timeout, but if we search a very large number of windows or create huge feature vectors, we still may run into timeout issues. If we want to explore more, download the data and try the code on a local machine.\n",
    "\n",
    "The subset of data can be downloaded used in this lesson for [vehicles](https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/vehicles_smallset.zip) and [non-vehicles](https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/non-vehicles_smallset.zip), or if prefer, we can directly grab the larger project dataset for [vehicles](https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/vehicles.zip) and [non-vehicles](https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/non-vehicles_smallset.zip).\n",
    "\n",
    "In this exercise, experiment with different color and gradient feature sets, different search window sizes and overlap to get an idea of how our classifier performs with different training features. Getting false positives in the skies and treetops? Try restricting our search area on the image with y_start_stop in the slide_window() function. What combination of features works best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__lesson_functions.py__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.feature import hog\n",
    "# Define a function to return HOG features and visualization\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, \n",
    "                        vis=False, feature_vec=True):\n",
    "    # Call with two outputs if vis==True\n",
    "    if vis == True:\n",
    "        features, hog_image = hog(img, orientations=orient, \n",
    "                                  pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block), \n",
    "                                  transform_sqrt=True, \n",
    "                                  visualise=vis, feature_vector=feature_vec)\n",
    "        return features, hog_image\n",
    "    # Otherwise call with one output\n",
    "    else:      \n",
    "        features = hog(img, orientations=orient, \n",
    "                       pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block), \n",
    "                       transform_sqrt=True, \n",
    "                       visualise=vis, feature_vector=feature_vec)\n",
    "        return features\n",
    "\n",
    "# Define a function to compute binned color features  \n",
    "def bin_spatial(img, size=(32, 32)):\n",
    "    # Use cv2.resize().ravel() to create the feature vector\n",
    "    features = cv2.resize(img, size).ravel() \n",
    "    # Return the feature vector\n",
    "    return features\n",
    "\n",
    "# Define a function to compute color histogram features \n",
    "# NEED TO CHANGE bins_range if reading .png files with mpimg!\n",
    "def color_hist(img, nbins=32, bins_range=(0, 256)):\n",
    "    # Compute the histogram of the color channels separately\n",
    "    channel1_hist = np.histogram(img[:,:,0], bins=nbins, range=bins_range)\n",
    "    channel2_hist = np.histogram(img[:,:,1], bins=nbins, range=bins_range)\n",
    "    channel3_hist = np.histogram(img[:,:,2], bins=nbins, range=bins_range)\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return hist_features\n",
    "\n",
    "# Define a function to extract features from a list of images\n",
    "# Have this function call bin_spatial() and color_hist()\n",
    "def extract_features(imgs, color_space='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=9, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0,\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True):\n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    # Iterate through the list of images\n",
    "    for file in imgs:\n",
    "        file_features = []\n",
    "        # Read in each one by one\n",
    "        image = mpimg.imread(file)\n",
    "        # apply color conversion if other than 'RGB'\n",
    "        if color_space != 'RGB':\n",
    "            if color_space == 'HSV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "            elif color_space == 'LUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\n",
    "            elif color_space == 'HLS':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "            elif color_space == 'YUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "            elif color_space == 'YCrCb':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n",
    "        else: feature_image = np.copy(image)      \n",
    "\n",
    "        if spatial_feat == True:\n",
    "            spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "            file_features.append(spatial_features)\n",
    "        if hist_feat == True:\n",
    "            # Apply color_hist()\n",
    "            hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "            file_features.append(hist_features)\n",
    "        if hog_feat == True:\n",
    "        # Call get_hog_features() with vis=False, feature_vec=True\n",
    "            if hog_channel == 'ALL':\n",
    "                hog_features = []\n",
    "                for channel in range(feature_image.shape[2]):\n",
    "                    hog_features.append(get_hog_features(feature_image[:,:,channel], \n",
    "                                        orient, pix_per_cell, cell_per_block, \n",
    "                                        vis=False, feature_vec=True))\n",
    "                hog_features = np.ravel(hog_features)        \n",
    "            else:\n",
    "                hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                            pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "            # Append the new feature vector to the features list\n",
    "            file_features.append(hog_features)\n",
    "        features.append(np.concatenate(file_features))\n",
    "    # Return list of feature vectors\n",
    "    return features\n",
    "    \n",
    "# Define a function that takes an image,\n",
    "# start and stop positions in both x and y, \n",
    "# window size (x and y dimensions),  \n",
    "# and overlap fraction (for both x and y)\n",
    "def slide_window(img, x_start_stop=[None, None], y_start_stop=[None, None], \n",
    "                    xy_window=(64, 64), xy_overlap=(0.5, 0.5)):\n",
    "    # If x and/or y start/stop positions not defined, set to image size\n",
    "    if x_start_stop[0] == None:\n",
    "        x_start_stop[0] = 0\n",
    "    if x_start_stop[1] == None:\n",
    "        x_start_stop[1] = img.shape[1]\n",
    "    if y_start_stop[0] == None:\n",
    "        y_start_stop[0] = 0\n",
    "    if y_start_stop[1] == None:\n",
    "        y_start_stop[1] = img.shape[0]\n",
    "    # Compute the span of the region to be searched    \n",
    "    xspan = x_start_stop[1] - x_start_stop[0]\n",
    "    yspan = y_start_stop[1] - y_start_stop[0]\n",
    "    # Compute the number of pixels per step in x/y\n",
    "    nx_pix_per_step = np.int(xy_window[0]*(1 - xy_overlap[0]))\n",
    "    ny_pix_per_step = np.int(xy_window[1]*(1 - xy_overlap[1]))\n",
    "    # Compute the number of windows in x/y\n",
    "    nx_buffer = np.int(xy_window[0]*(xy_overlap[0]))\n",
    "    ny_buffer = np.int(xy_window[1]*(xy_overlap[1]))\n",
    "    nx_windows = np.int((xspan-nx_buffer)/nx_pix_per_step) \n",
    "    ny_windows = np.int((yspan-ny_buffer)/ny_pix_per_step) \n",
    "    # Initialize a list to append window positions to\n",
    "    window_list = []\n",
    "    # Loop through finding x and y window positions\n",
    "    # Note: you could vectorize this step, but in practice\n",
    "    # you'll be considering windows one by one with your\n",
    "    # classifier, so looping makes sense\n",
    "    for ys in range(ny_windows):\n",
    "        for xs in range(nx_windows):\n",
    "            # Calculate window position\n",
    "            startx = xs*nx_pix_per_step + x_start_stop[0]\n",
    "            endx = startx + xy_window[0]\n",
    "            starty = ys*ny_pix_per_step + y_start_stop[0]\n",
    "            endy = starty + xy_window[1]\n",
    "            \n",
    "            # Append window position to list\n",
    "            window_list.append(((startx, starty), (endx, endy)))\n",
    "    # Return the list of windows\n",
    "    return window_list\n",
    "\n",
    "# Define a function to draw bounding boxes\n",
    "def draw_boxes(img, bboxes, color=(0, 0, 255), thick=6):\n",
    "    # Make a copy of the image\n",
    "    imcopy = np.copy(img)\n",
    "    # Iterate through the bounding boxes\n",
    "    for bbox in bboxes:\n",
    "        # Draw a rectangle given bbox coordinates\n",
    "        cv2.rectangle(imcopy, bbox[0], bbox[1], color, thick)\n",
    "    # Return the image copy with boxes drawn\n",
    "    return imcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__search_classify.py__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.feature import hog\n",
    "#from lesson_functions import *\n",
    "# NOTE: the next import is only valid for scikit-learn version <= 0.17\n",
    "# for scikit-learn >= 0.18 use:\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Define a function to extract features from a single image window\n",
    "# This function is very similar to extract_features()\n",
    "# just for a single image rather than list of images\n",
    "def single_img_features(img, color_space='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=9, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0,\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True):    \n",
    "    #1) Define an empty list to receive features\n",
    "    img_features = []\n",
    "    #2) Apply color conversion if other than 'RGB'\n",
    "    if color_space != 'RGB':\n",
    "        if color_space == 'HSV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "        elif color_space == 'LUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2LUV)\n",
    "        elif color_space == 'HLS':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "        elif color_space == 'YUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "        elif color_space == 'YCrCb':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n",
    "    else: feature_image = np.copy(img)      \n",
    "    #3) Compute spatial features if flag is set\n",
    "    if spatial_feat == True:\n",
    "        spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "        #4) Append features to list\n",
    "        img_features.append(spatial_features)\n",
    "    #5) Compute histogram features if flag is set\n",
    "    if hist_feat == True:\n",
    "        hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "        #6) Append features to list\n",
    "        img_features.append(hist_features)\n",
    "    #7) Compute HOG features if flag is set\n",
    "    if hog_feat == True:\n",
    "        if hog_channel == 'ALL':\n",
    "            hog_features = []\n",
    "            for channel in range(feature_image.shape[2]):\n",
    "                hog_features.extend(get_hog_features(feature_image[:,:,channel], \n",
    "                                    orient, pix_per_cell, cell_per_block, \n",
    "                                    vis=False, feature_vec=True))      \n",
    "        else:\n",
    "            hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                        pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "        #8) Append features to list\n",
    "        img_features.append(hog_features)\n",
    "\n",
    "    #9) Return concatenated array of features\n",
    "    return np.concatenate(img_features)\n",
    "\n",
    "# Define a function you will pass an image \n",
    "# and the list of windows to be searched (output of slide_windows())\n",
    "def search_windows(img, windows, clf, scaler, color_space='RGB', \n",
    "                    spatial_size=(32, 32), hist_bins=32, \n",
    "                    hist_range=(0, 256), orient=9, \n",
    "                    pix_per_cell=8, cell_per_block=2, \n",
    "                    hog_channel=0, spatial_feat=True, \n",
    "                    hist_feat=True, hog_feat=True):\n",
    "\n",
    "    #1) Create an empty list to receive positive detection windows\n",
    "    on_windows = []\n",
    "    #2) Iterate over all windows in the list\n",
    "    for window in windows:\n",
    "        #3) Extract the test window from original image\n",
    "        test_img = cv2.resize(img[window[0][1]:window[1][1], window[0][0]:window[1][0]], (64, 64))      \n",
    "        #4) Extract features for that window using single_img_features()\n",
    "        features = single_img_features(test_img, color_space=color_space, \n",
    "                            spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                            orient=orient, pix_per_cell=pix_per_cell, \n",
    "                            cell_per_block=cell_per_block, \n",
    "                            hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                            hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "        #5) Scale extracted features to be fed to classifier\n",
    "        test_features = scaler.transform(np.array(features).reshape(1, -1))\n",
    "        #6) Predict using your classifier\n",
    "        prediction = clf.predict(test_features)\n",
    "        #7) If positive (prediction == 1) then save the window\n",
    "        if prediction == 1:\n",
    "            on_windows.append(window)\n",
    "    #8) Return windows for positive detections\n",
    "    return on_windows\n",
    "\n",
    "\n",
    "# Read in cars and notcars\n",
    "# Divide up into cars and notcars\n",
    "cars = []\n",
    "notcars = []\n",
    "\n",
    "car_images = glob.glob('Figures/vehicles_smallset/cars1/*.jpeg')\n",
    "for image in car_images:\n",
    "    if 'image' in image or 'extra' in image:\n",
    "        notcars.append(image)\n",
    "    else:\n",
    "        cars.append(image)\n",
    "        \n",
    "non_car_images = glob.glob('Figures/non-vehicles_smallset/notcars1/*.jpeg')\n",
    "for image in non_car_images:\n",
    "    if 'image' in image or 'extra' in image:\n",
    "        notcars.append(image)\n",
    "    else:\n",
    "        cars.append(image)\n",
    "\n",
    "# Reduce the sample size because\n",
    "# The quiz evaluator times out after 13s of CPU time\n",
    "sample_size = 500\n",
    "cars = cars[0:sample_size]\n",
    "notcars = notcars[0:sample_size]\n",
    "\n",
    "### TODO: Tweak these parameters and see how the results change.\n",
    "color_space = 'RGB' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "orient = 9  # HOG orientations\n",
    "pix_per_cell = 8 # HOG pixels per cell\n",
    "cell_per_block = 2 # HOG cells per block\n",
    "hog_channel = 0 # Can be 0, 1, 2, or \"ALL\"\n",
    "spatial_size = (16, 16) # Spatial binning dimensions\n",
    "hist_bins = 16    # Number of histogram bins\n",
    "spatial_feat = True # Spatial features on or off\n",
    "hist_feat = True # Histogram features on or off\n",
    "hog_feat = True # HOG features on or off\n",
    "y_start_stop = [None, None] # Min and max in y to search in slide_window()\n",
    "\n",
    "car_features = extract_features(cars, color_space=color_space, \n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                        orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "notcar_features = extract_features(notcars, color_space=color_space, \n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                        orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "\n",
    "X = np.vstack((car_features, notcar_features)).astype(np.float64)                        \n",
    "# Fit a per-column scaler\n",
    "X_scaler = StandardScaler().fit(X)\n",
    "# Apply the scaler to X\n",
    "scaled_X = X_scaler.transform(X)\n",
    "\n",
    "# Define the labels vector\n",
    "y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\n",
    "\n",
    "\n",
    "# Split up data into randomized training and test sets\n",
    "rand_state = np.random.randint(0, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    scaled_X, y, test_size=0.2, random_state=rand_state)\n",
    "\n",
    "print('Using:',orient,'orientations',pix_per_cell,\n",
    "    'pixels per cell and', cell_per_block,'cells per block')\n",
    "print('Feature vector length:', len(X_train[0]))\n",
    "# Use a linear SVC \n",
    "svc = LinearSVC()\n",
    "# Check the training time for the SVC\n",
    "t=time.time()\n",
    "svc.fit(X_train, y_train)\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "# Check the score of the SVC\n",
    "print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))\n",
    "# Check the prediction time for a single sample\n",
    "t=time.time()\n",
    "\n",
    "image = mpimg.imread('Figures/1. bbox-example-image.jpg')\n",
    "draw_image = np.copy(image)\n",
    "\n",
    "# Uncomment the following line if you extracted training\n",
    "# data from .png images (scaled 0 to 1 by mpimg) and the\n",
    "# image you are searching is a .jpg (scaled 0 to 255)\n",
    "#image = image.astype(np.float32)/255\n",
    "\n",
    "windows = slide_window(image, x_start_stop=[None, None], y_start_stop=y_start_stop, \n",
    "                    xy_window=(96, 96), xy_overlap=(0.5, 0.5))\n",
    "\n",
    "hot_windows = search_windows(image, windows, svc, X_scaler, color_space=color_space, \n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                        orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)                       \n",
    "\n",
    "window_img = draw_boxes(draw_image, hot_windows, color=(0, 0, 255), thick=6)                    \n",
    "\n",
    "plt.imshow(window_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__\n",
    "```Python\n",
    "# Define a function to extract features from a single image window\n",
    "# This function is very similar to extract_features()\n",
    "# just for a single image rather than list of images\n",
    "def single_img_features(img, color_space='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=9, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0,\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True):    \n",
    "    #1) Define an empty list to receive features\n",
    "    img_features = []\n",
    "    #2) Apply color conversion if other than 'RGB'\n",
    "    if color_space != 'RGB':\n",
    "        if color_space == 'HSV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "        elif color_space == 'LUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2LUV)\n",
    "        elif color_space == 'HLS':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "        elif color_space == 'YUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "        elif color_space == 'YCrCb':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n",
    "    else: feature_image = np.copy(img)      \n",
    "    #3) Compute spatial features if flag is set\n",
    "    if spatial_feat == True:\n",
    "        spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "        #4) Append features to list\n",
    "        img_features.append(spatial_features)\n",
    "    #5) Compute histogram features if flag is set\n",
    "    if hist_feat == True:\n",
    "        hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "        #6) Append features to list\n",
    "        img_features.append(hist_features)\n",
    "    #7) Compute HOG features if flag is set\n",
    "    if hog_feat == True:\n",
    "        if hog_channel == 'ALL':\n",
    "            hog_features = []\n",
    "            for channel in range(feature_image.shape[2]):\n",
    "                hog_features.extend(get_hog_features(feature_image[:,:,channel], \n",
    "                                    orient, pix_per_cell, cell_per_block, \n",
    "                                    vis=False, feature_vec=True))      \n",
    "        else:\n",
    "            hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                        pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "        #8) Append features to list\n",
    "        img_features.append(hog_features)\n",
    "\n",
    "    #9) Return concatenated array of features\n",
    "    return np.concatenate(img_features)\n",
    "\n",
    "# Define a function you will pass an image \n",
    "# and the list of windows to be searched (output of slide_windows())\n",
    "def search_windows(img, windows, clf, scaler, color_space='RGB', \n",
    "                    spatial_size=(32, 32), hist_bins=32, \n",
    "                    hist_range=(0, 256), orient=9, \n",
    "                    pix_per_cell=8, cell_per_block=2, \n",
    "                    hog_channel=0, spatial_feat=True, \n",
    "                    hist_feat=True, hog_feat=True):\n",
    "\n",
    "    #1) Create an empty list to receive positive detection windows\n",
    "    on_windows = []\n",
    "    #2) Iterate over all windows in the list\n",
    "    for window in windows:\n",
    "        #3) Extract the test window from original image\n",
    "        test_img = cv2.resize(img[window[0][1]:window[1][1], window[0][0]:window[1][0]], (64, 64))      \n",
    "        #4) Extract features for that window using single_img_features()\n",
    "        features = single_img_features(test_img, color_space=color_space, \n",
    "                            spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                            orient=orient, pix_per_cell=pix_per_cell, \n",
    "                            cell_per_block=cell_per_block, \n",
    "                            hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                            hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "        #5) Scale extracted features to be fed to classifier\n",
    "        test_features = scaler.transform(np.array(features).reshape(1, -1))\n",
    "        #6) Predict using your classifier\n",
    "        prediction = clf.predict(test_features)\n",
    "        #7) If positive (prediction == 1) then save the window\n",
    "        if prediction == 1:\n",
    "            on_windows.append(window)\n",
    "    #8) Return windows for positive detections\n",
    "    return on_windows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.feature import hog\n",
    "#from lesson_functions import *\n",
    "# NOTE: the next import is only valid for scikit-learn version <= 0.17\n",
    "# for scikit-learn >= 0.18 use:\n",
    "# Define a function to extract features from a single image window\n",
    "# This function is very similar to extract_features()\n",
    "# just for a single image rather than list of images\n",
    "def single_img_features(img, color_space='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=9, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0,\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True):    \n",
    "    #1) Define an empty list to receive features\n",
    "    img_features = []\n",
    "    #2) Apply color conversion if other than 'RGB'\n",
    "    if color_space != 'RGB':\n",
    "        if color_space == 'HSV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "        elif color_space == 'LUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2LUV)\n",
    "        elif color_space == 'HLS':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "        elif color_space == 'YUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "        elif color_space == 'YCrCb':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n",
    "    else: feature_image = np.copy(img)      \n",
    "    #3) Compute spatial features if flag is set\n",
    "    if spatial_feat == True:\n",
    "        spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "        #4) Append features to list\n",
    "        img_features.append(spatial_features)\n",
    "    #5) Compute histogram features if flag is set\n",
    "    if hist_feat == True:\n",
    "        hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "        #6) Append features to list\n",
    "        img_features.append(hist_features)\n",
    "    #7) Compute HOG features if flag is set\n",
    "    if hog_feat == True:\n",
    "        if hog_channel == 'ALL':\n",
    "            hog_features = []\n",
    "            for channel in range(feature_image.shape[2]):\n",
    "                hog_features.extend(get_hog_features(feature_image[:,:,channel], \n",
    "                                    orient, pix_per_cell, cell_per_block, \n",
    "                                    vis=False, feature_vec=True))      \n",
    "        else:\n",
    "            hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                        pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "        #8) Append features to list\n",
    "        img_features.append(hog_features)\n",
    "\n",
    "    #9) Return concatenated array of features\n",
    "    return np.concatenate(img_features)\n",
    "\n",
    "# Define a function you will pass an image \n",
    "# and the list of windows to be searched (output of slide_windows())\n",
    "def search_windows(img, windows, clf, scaler, color_space='RGB', \n",
    "                    spatial_size=(32, 32), hist_bins=32, \n",
    "                    hist_range=(0, 256), orient=9, \n",
    "                    pix_per_cell=8, cell_per_block=2, \n",
    "                    hog_channel=0, spatial_feat=True, \n",
    "                    hist_feat=True, hog_feat=True):\n",
    "\n",
    "    #1) Create an empty list to receive positive detection windows\n",
    "    on_windows = []\n",
    "    #2) Iterate over all windows in the list\n",
    "    for window in windows:\n",
    "        #3) Extract the test window from original image\n",
    "        test_img = cv2.resize(img[window[0][1]:window[1][1], window[0][0]:window[1][0]], (64, 64))      \n",
    "        #4) Extract features for that window using single_img_features()\n",
    "        features = single_img_features(test_img, color_space=color_space, \n",
    "                            spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                            orient=orient, pix_per_cell=pix_per_cell, \n",
    "                            cell_per_block=cell_per_block, \n",
    "                            hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                            hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "        #5) Scale extracted features to be fed to classifier\n",
    "        test_features = scaler.transform(np.array(features).reshape(1, -1))\n",
    "        #6) Predict using your classifier\n",
    "        prediction = clf.predict(test_features)\n",
    "        #7) If positive (prediction == 1) then save the window\n",
    "        if prediction == 1:\n",
    "            on_windows.append(window)\n",
    "    #8) Return windows for positive detections\n",
    "    return on_windows\n",
    "    \n",
    "# Read in cars and notcars\n",
    "# Divide up into cars and notcars\n",
    "car_images = glob.glob('Figures/vehicles_smallset/cars1/*.jpeg')\n",
    "for image in car_images:\n",
    "    if 'image' in image or 'extra' in image:\n",
    "        notcars.append(image)\n",
    "    else:\n",
    "        cars.append(image)\n",
    "        \n",
    "non_car_images = glob.glob('Figures/non-vehicles_smallset/notcars1/*.jpeg')\n",
    "for image in non_car_images:\n",
    "    if 'image' in image or 'extra' in image:\n",
    "        notcars.append(image)\n",
    "    else:\n",
    "        cars.append(image)\n",
    "\n",
    "# Reduce the sample size because\n",
    "# The quiz evaluator times out after 13s of CPU time\n",
    "sample_size = 500\n",
    "cars = cars[0:sample_size]\n",
    "notcars = notcars[0:sample_size]\n",
    "\n",
    "### TODO: Tweak these parameters and see how the results change.\n",
    "color_space = 'RGB' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "orient = 9  # HOG orientations\n",
    "pix_per_cell = 8 # HOG pixels per cell\n",
    "cell_per_block = 2 # HOG cells per block\n",
    "hog_channel = 0 # Can be 0, 1, 2, or \"ALL\"\n",
    "spatial_size = (16, 16) # Spatial binning dimensions\n",
    "hist_bins = 16    # Number of histogram bins\n",
    "spatial_feat = True # Spatial features on or off\n",
    "hist_feat = True # Histogram features on or off\n",
    "hog_feat = True # HOG features on or off\n",
    "y_start_stop = [None, None] # Min and max in y to search in slide_window()\n",
    "\n",
    "car_features = extract_features(cars, color_space=color_space, \n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                        orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "notcar_features = extract_features(notcars, color_space=color_space, \n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                        orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "\n",
    "X = np.vstack((car_features, notcar_features)).astype(np.float64)                        \n",
    "# Fit a per-column scaler\n",
    "X_scaler = StandardScaler().fit(X)\n",
    "# Apply the scaler to X\n",
    "scaled_X = X_scaler.transform(X)\n",
    "\n",
    "# Define the labels vector\n",
    "y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\n",
    "\n",
    "\n",
    "# Split up data into randomized training and test sets\n",
    "rand_state = np.random.randint(0, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    scaled_X, y, test_size=0.2, random_state=rand_state)\n",
    "\n",
    "print('Using:',orient,'orientations',pix_per_cell,\n",
    "    'pixels per cell and', cell_per_block,'cells per block')\n",
    "print('Feature vector length:', len(X_train[0]))\n",
    "# Use a linear SVC \n",
    "svc = LinearSVC()\n",
    "# Check the training time for the SVC\n",
    "t=time.time()\n",
    "svc.fit(X_train, y_train)\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "# Check the score of the SVC\n",
    "print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))\n",
    "# Check the prediction time for a single sample\n",
    "t=time.time()\n",
    "\n",
    "image = mpimg.imread('Figures/3. bbox-example-image.jpg')\n",
    "draw_image = np.copy(image)\n",
    "\n",
    "# Uncomment the following line if you extracted training\n",
    "# data from .png images (scaled 0 to 1 by mpimg) and the\n",
    "# image you are searching is a .jpg (scaled 0 to 255)\n",
    "#image = image.astype(np.float32)/255\n",
    "\n",
    "windows = slide_window(image, x_start_stop=[None, None], y_start_stop=y_start_stop, \n",
    "                    xy_window=(96, 96), xy_overlap=(0.5, 0.5))\n",
    "\n",
    "hot_windows = search_windows(image, windows, svc, X_scaler, color_space=color_space, \n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                        orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)                       \n",
    "\n",
    "window_img = draw_boxes(draw_image, hot_windows, color=(0, 0, 255), thick=6)                    \n",
    "\n",
    "plt.imshow(window_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7. Hog Sub-sampling Window Search\n",
    "\n",
    "Now lets explore a more efficient method for doing the sliding window approach, one that allows us to only have to extract the Hog features once. The code below defines a single function __find_cars__ that's able to both extract features and make predictions.\n",
    "\n",
    "The __find_cars__ only has to extract hog features once and then can be sub-sampled to get all of its overlaying windows. Each window is defined by a scaling factor where a scale of 1 would result in a window that's 8 x 8 cells then the overlap of each window is in terms of the cell distance. This means that a __cells_per_step = 2__ would result in a search window overlap of 75%. Its possible to run this same function multiple times for different scale values to generate multiple-scaled search windows.\n",
    "\n",
    "<img src='Figures/16. hog-sub.jpg' width=500>\n",
    "\n",
    "__Quiz:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__lesson_functions.py__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.feature import hog\n",
    "\n",
    "def convert_color(img, conv='RGB2YCrCb'):\n",
    "    if conv == 'RGB2YCrCb':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n",
    "    if conv == 'BGR2YCrCb':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n",
    "    if conv == 'RGB2LUV':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2LUV)\n",
    "\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, \n",
    "                        vis=False, feature_vec=True):\n",
    "    # Call with two outputs if vis==True\n",
    "    if vis == True:\n",
    "        features, hog_image = hog(img, orientations=orient, \n",
    "                                  pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block), \n",
    "                                  transform_sqrt=False, \n",
    "                                  visualise=vis, feature_vector=feature_vec)\n",
    "        return features, hog_image\n",
    "    # Otherwise call with one output\n",
    "    else:      \n",
    "        features = hog(img, orientations=orient, \n",
    "                       pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block), \n",
    "                       transform_sqrt=False, \n",
    "                       visualise=vis, feature_vector=feature_vec)\n",
    "        return features\n",
    "\n",
    "    \n",
    "def bin_spatial(img, size=(32, 32)):\n",
    "    color1 = cv2.resize(img[:,:,0], size).ravel()\n",
    "    color2 = cv2.resize(img[:,:,1], size).ravel()\n",
    "    color3 = cv2.resize(img[:,:,2], size).ravel()\n",
    "    return np.hstack((color1, color2, color3))\n",
    "\n",
    "\n",
    "def color_hist(img, nbins=32):    #bins_range=(0, 256)\n",
    "    # Compute the histogram of the color channels separately\n",
    "    channel1_hist = np.histogram(img[:,:,0], bins=nbins)\n",
    "    channel2_hist = np.histogram(img[:,:,1], bins=nbins)\n",
    "    channel3_hist = np.histogram(img[:,:,2], bins=nbins)\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return hist_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__hog_subsample.py__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "from lesson_functions import *\n",
    "\n",
    "dist_pickle = pickle.load( open(\"svc_pickle.p\", \"rb\" ) )\n",
    "svc = dist_pickle[\"svc\"]\n",
    "X_scaler = dist_pickle[\"scaler\"]\n",
    "orient = dist_pickle[\"orient\"]\n",
    "pix_per_cell = dist_pickle[\"pix_per_cell\"]\n",
    "cell_per_block = dist_pickle[\"cell_per_block\"]\n",
    "spatial_size = dist_pickle[\"spatial_size\"]\n",
    "hist_bins = dist_pickle[\"hist_bins\"]\n",
    "\n",
    "img = mpimg.imread('test_image.jpg')\n",
    "\n",
    "# Define a single function that can extract features using hog sub-sampling and make predictions\n",
    "def find_cars(img, ystart, ystop, scale, svc, X_scaler, orient, pix_per_cell, cell_per_block, spatial_size, hist_bins):\n",
    "    \n",
    "    draw_img = np.copy(img)\n",
    "    img = img.astype(np.float32)/255\n",
    "    \n",
    "    img_tosearch = img[ystart:ystop,:,:]\n",
    "    ctrans_tosearch = convert_color(img_tosearch, conv='RGB2YCrCb')\n",
    "    if scale != 1:\n",
    "        imshape = ctrans_tosearch.shape\n",
    "        ctrans_tosearch = cv2.resize(ctrans_tosearch, (np.int(imshape[1]/scale), np.int(imshape[0]/scale)))\n",
    "        \n",
    "    ch1 = ctrans_tosearch[:,:,0]\n",
    "    ch2 = ctrans_tosearch[:,:,1]\n",
    "    ch3 = ctrans_tosearch[:,:,2]\n",
    "\n",
    "    # Define blocks and steps as above\n",
    "    nxblocks = (ch1.shape[1] // pix_per_cell) - cell_per_block + 1\n",
    "    nyblocks = (ch1.shape[0] // pix_per_cell) - cell_per_block + 1 \n",
    "    nfeat_per_block = orient*cell_per_block**2\n",
    "    \n",
    "    # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell\n",
    "    window = 64\n",
    "    nblocks_per_window = (window // pix_per_cell) - cell_per_block + 1\n",
    "    cells_per_step = 2  # Instead of overlap, define how many cells to step\n",
    "    nxsteps = (nxblocks - nblocks_per_window) // cells_per_step\n",
    "    nysteps = (nyblocks - nblocks_per_window) // cells_per_step\n",
    "    \n",
    "    # Compute individual channel HOG features for the entire image\n",
    "    hog1 = get_hog_features(ch1, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    hog2 = get_hog_features(ch2, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    hog3 = get_hog_features(ch3, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    \n",
    "    for xb in range(nxsteps):\n",
    "        for yb in range(nysteps):\n",
    "            ypos = yb*cells_per_step\n",
    "            xpos = xb*cells_per_step\n",
    "            # Extract HOG for this patch\n",
    "            hog_feat1 = hog1[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_feat2 = hog2[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_feat3 = hog3[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_features = np.hstack((hog_feat1, hog_feat2, hog_feat3))\n",
    "\n",
    "            xleft = xpos*pix_per_cell\n",
    "            ytop = ypos*pix_per_cell\n",
    "\n",
    "            # Extract the image patch\n",
    "            subimg = cv2.resize(ctrans_tosearch[ytop:ytop+window, xleft:xleft+window], (64,64))\n",
    "          \n",
    "            # Get color features\n",
    "            spatial_features = bin_spatial(subimg, size=spatial_size)\n",
    "            hist_features = color_hist(subimg, nbins=hist_bins)\n",
    "\n",
    "            # Scale features and make a prediction\n",
    "            test_features = X_scaler.transform(np.hstack((spatial_features, hist_features, hog_features)).reshape(1, -1))    \n",
    "            #test_features = X_scaler.transform(np.hstack((shape_feat, hist_feat)).reshape(1, -1))    \n",
    "            test_prediction = svc.predict(test_features)\n",
    "            \n",
    "            if test_prediction == 1:\n",
    "                xbox_left = np.int(xleft*scale)\n",
    "                ytop_draw = np.int(ytop*scale)\n",
    "                win_draw = np.int(window*scale)\n",
    "                cv2.rectangle(draw_img,(xbox_left, ytop_draw+ystart),(xbox_left+win_draw,ytop_draw+win_draw+ystart),(0,0,255),6) \n",
    "                \n",
    "    return draw_img\n",
    "    \n",
    "ystart = 400\n",
    "ystop = 656\n",
    "scale = 1.5\n",
    "    \n",
    "out_img = find_cars(img, ystart, ystop, scale, svc, X_scaler, orient, pix_per_cell, cell_per_block, spatial_size, hist_bins)\n",
    "\n",
    "plt.imshow(out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8. Multiple Detections & False Positives <a name='Multiple Detections & False Positives'></a>\n",
    "\n",
    "Here are six consecutive frames from the project video and showing all the bounding boxes for where our classifier reported positive detections. We can see that overlapping detections exist for each of the two vehicles, and in two of the frames, we find a false positive detection on the guardrail to the left. In this exercise, we'll build a heat-map from these detections in order to combine overlapping detections and remove false positives.\n",
    "\n",
    "<img src='Figures/17. Multiple Detections.png' widht=500>\n",
    "\n",
    "To make a heat-map, we're simply going to add \"heat\" (+=1) for all pixels within windows where a positive detection is reported by our classifier. The individual heat-maps for the above images look like this:\n",
    "\n",
    "<img src='Figures/18. Heat Map.png' width=500>\n",
    "\n",
    "In the exercise below, we are provided with a list of bounding boxes for the detections in the images shown above. Rather than consider heat-maps for each individual image, we'll write a function that adds \"heat\" to a map for a list of bounding boxes.\n",
    "\n",
    "```Python\n",
    "def add_heat(heatmap, bbox_list):\n",
    "    # Iterate through list of bboxes\n",
    "    for box in bbox_list:\n",
    "        # Add += 1 for all pixels inside each bbox\n",
    "        # Assuming each \"box\" takes the form ((x1, y1), (x2, y2))\n",
    "        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\n",
    "\n",
    "    # Return updated heatmap\n",
    "    return heatmap\n",
    "```\n",
    "\n",
    "If our classifier is working well, then the \"hot\" parts of the map are where the cars are, and by imposing a threshold, we can reject areas affected by false positives. So let's write a function to threshold the map as well.\n",
    "\n",
    "```Python\n",
    "def apply_threshold(heatmap, threshold):\n",
    "    # Zero out pixels below the threshold\n",
    "    heatmap[heatmap <= threshold] = 0\n",
    "    # Return thresholded map\n",
    "    return heatmap\n",
    "```\n",
    "\n",
    "In practice, we will want to integrate a heat map over several frames of video, such that areas of multiple detections get \"hot\", while transient false positives stay \"cool\". We can then simply threshold our heatmap to remove false positives.\n",
    "\n",
    "Once we have a thresholded heat-map, there are many ways we could go about trying to figure out how many cars we have in each frame and which pixels belong to which cars, but one of the most straightforward solutions is to use the __label()__ function from __scipy.ndimage.measurements__. We'll use it like this:\n",
    "\n",
    "```Python\n",
    "from scipy.ndimage.measurements import label\n",
    "labels = label(heatmap)\n",
    "```\n",
    "\n",
    "Now __labels__ is a 2-tuple, where the first item is an array the size of the heatmap input image and the second element is the number of labels (cars) found.\n",
    "\n",
    "In the above series of images the same false positive window appeared twice, so if we set a threshold of 2 (meaning set all values <= 2 to 0) and then run the labels() function. We get a result that 2 cars were identified (the two islands in the heat-map) and the labeled image where pixels are set to 0 for background, 1 for car number 1, and 2 for car number 2.\n",
    "\n",
    "```Python\n",
    "heatmap = threshold(heatmap, 2)\n",
    "labels = label(heatmap)\n",
    "print(labels[1], 'cars found')\n",
    "plt.imshow(labels[0], cmap='gray')\n",
    "```\n",
    "\n",
    "2 cars found\n",
    "<img src='Figures/18. Heat Map Cars.png' width=500>\n",
    "\n",
    "Next, we can take our labels image and put bounding boxes around the labeled regions. We could do this in the following manner:\n",
    "```Python\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def draw_labeled_bboxes(img, labels):\n",
    "    # Iterate through all detected cars\n",
    "    for car_number in range(1, labels[1]+1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        # Draw the box on the image\n",
    "        cv2.rectangle(img, bbox[0], bbox[1], (0,0,255), 6)\n",
    "    # Return the image\n",
    "    return img\n",
    "\n",
    "# Read in the last image above\n",
    "image = mpimg.imread('img105.jpg')\n",
    "# Draw bounding boxes on a copy of the image\n",
    "draw_img = draw_labeled_bboxes(np.copy(image), labels)\n",
    "# Display the image\n",
    "plt.imshow(draw_img)\n",
    "```\n",
    "\n",
    "And the output looks like this:\n",
    "\n",
    "<img src='Figures/18. Output.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Tips and Tricks for the Project\n",
    "\n",
    "### Extract HOG features just once for the entire region of interest in each full image / video frame\n",
    "\n",
    "In one of the previous exercises you extracted HOG features from each individual window as you searched across the image, but it turns out this is rather inefficient. To speed things up, extract HOG features just once for the entire region of interest (i.e. lower half of each frame of video) and subsample that array for each sliding window. To do this, apply __skimage.feature.hog()__ with the flag __feature_vec=False__, like this:\n",
    "\n",
    "```Python\n",
    "from skimage.feature import hog\n",
    "orient = 9\n",
    "pix_per_cell = 8\n",
    "cell_per_block = 2\n",
    "\n",
    "feature_array = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell), cells_per_block=(cell_per_block, cell_per_block), visualise=False, feature_vector=False)\n",
    "```\n",
    "\n",
    "The output __feature_array__ will have a shape of __(n_yblocks, n_xblocks, 2, 2, 9)__, where __n_yblocks__ and __n_xblocks__ are determined by the shape of your region of interest (i.e. how many blocks fit across and down your image in x and y).\n",
    "\n",
    "So, for example, if you used __cells_per_block=2__ in extracting features from the 64x64 pixel training images, then you would want to extract subarrays of shape __(7, 7, 2, 2, 9)__ from __feature_array__ and then use __np.ravel()__ to unroll the feature vector.\n",
    "\n",
    "### Make sure your images are scaled correctly\n",
    "The training dataset provided for this project ( [vehicle](https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/vehicles.zip) and [non-vehicle](https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/non-vehicles.zip) images) are in the __.png__ format. Somewhat confusingly, __matplotlib image__ will read these in on a scale of 0 to 1, but __cv2.imread()__ will scale them from 0 to 255. Be sure if you are switching between __cv2.imread()__ and __matplotlib image__ for reading images that you scale them appropriately! Otherwise your feature vectors can get screwed up.\n",
    "\n",
    "To add to the confusion, __matplotlib image__ will read __.jpg__ images in on a scale of 0 to 255 so if you are testing your pipeline on __.jpg__ images remember to scale them accordingly. And if you take an image that is scaled from 0 to 1 and change color spaces using __cv2.cvtColor()__ you'll get back an image scaled from 0 to 255. So just be sure to be consistent between your training data features and inference features!\n",
    "\n",
    "### Be sure to normalize your training data\n",
    "Use __sklearn.preprocessing.StandardScaler()__ to normalize your feature vectors for training your classifier as described in [this lesson](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/2b62a1c3-e151-4a0e-b6b6-e424fa46ceab/lessons/fd66c083-4ccb-4fe3-bda1-c29db76f50a0/concepts/cacf86d7-f8eb-46bd-9f09-34a2ff208ce8). Then apply the same scaling to each of the feature vectors you extract from windows in your test images.\n",
    "\n",
    "### Random shuffling of data\n",
    "When dealing with image data that was extracted from video, you may be dealing with sequences of images where your target object (vehicles in this case) appear almost identical in a whole series of images. In such a case, even a randomized train-test split will be subject to overfitting because images in the training set may be nearly identical to images in the test set.\n",
    "\n",
    "For the project [vehicles](https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/vehicles.zip) dataset, the __GTI\\*__ folders contain time-series data. In the KITTI folder, you may see the same vehicle appear more than once, but typically under significantly different lighting/angle from other instances.\n",
    "\n",
    "While it is possible to achieve a sufficiently good result on the project without worrying about time-series issues, if you really want to optimize your classifier, you should devise a train/test split that avoids having nearly identical images in both your training and test sets. This means extracting the time-series tracks from the GTI data and separating the images manually to make sure train and test images are sufficiently different from one another.\n",
    "\n",
    "---\n",
    "### Project Details\n",
    "For this project, we provide you with a labeled dataset and your job is to decide what features to extract, then train a classifier and ultimately track vehicles in a video stream. Here are links to the labeled data for [vehicle](https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/vehicles.zip) and [non-vehicle](https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/non-vehicles.zip) examples to train your classifier. These example images come from a combination of the [GTI vehicle image database](http://www.gti.ssr.upm.es/data/Vehicle_database.html), the [KITTI vision benchmark suite](http://www.cvlibs.net/datasets/kitti/), and examples extracted from the project video itself.\n",
    "\n",
    "Udacity recently released a labeled dataset of our own, which you are encouraged to take advantage of to augment your training data. You can find the Udacity data [here](https://github.com/udacity/self-driving-car/tree/master/annotations). In each of the folders containing images there's a csv file containing all the labels and bounding boxes. To add vehicle images to your training data, you'll need to use the csv files to extract the bounding box regions and scale them to the same size as the rest of the training images.\n",
    "\n",
    "The project video will be the same one as for the [Advanced Lane Finding Project](https://github.com/udacity/CarND-Advanced-Lane-Lines). The reason for this is that, assuming you already have a working implementation of lane finding for this video, once your vehicle detection pipeline works, you can add it to your lane finding pipeline and do both analyses simultaneously! You can use the test images from the lane finding project to start with, or extract other frames from the video to work on."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
